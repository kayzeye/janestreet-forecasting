{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc2fe3a7-575e-42d3-8681-05f54836d10c",
   "metadata": {},
   "source": [
    "# Load parquet files, process them into train/val tensors, then save them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caabf66e-b655-4941-b881-934560141037",
   "metadata": {},
   "source": [
    "### We need to do this because IPython + jupyter is complete ass at memory management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dff0a475-250f-4c96-913a-5cd52b36b027",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os, gc\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f14b8bba-d5ca-4fef-a694-9ec1db73fc80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1+cu121\n",
      "True\n",
      "NVIDIA GeForce RTX 3060\n"
     ]
    }
   ],
   "source": [
    "# check that torch is working and sees the GPU\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acec451c-592d-4a87-9cd8-e17281929cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['feature_00', 'feature_01', 'feature_02', 'feature_03', 'feature_04', 'feature_05', 'feature_06', 'feature_07', 'feature_08', 'feature_09', 'feature_10', 'feature_11', 'feature_12', 'feature_13', 'feature_14', 'feature_15', 'feature_16', 'feature_17', 'feature_18', 'feature_19', 'feature_20', 'feature_21', 'feature_22', 'feature_23', 'feature_24', 'feature_25', 'feature_26', 'feature_27', 'feature_28', 'feature_29', 'feature_30', 'feature_31', 'feature_32', 'feature_33', 'feature_34', 'feature_35', 'feature_36', 'feature_37', 'feature_38', 'feature_39', 'feature_40', 'feature_41', 'feature_42', 'feature_43', 'feature_44', 'feature_45', 'feature_46', 'feature_47', 'feature_48', 'feature_49', 'feature_50', 'feature_51', 'feature_52', 'feature_53', 'feature_54', 'feature_55', 'feature_56', 'feature_57', 'feature_58', 'feature_59', 'feature_60', 'feature_61', 'feature_62', 'feature_63', 'feature_64', 'feature_65', 'feature_66', 'feature_67', 'feature_68', 'feature_69', 'feature_70', 'feature_71', 'feature_72', 'feature_73', 'feature_74', 'feature_75', 'feature_76', 'feature_77', 'feature_78', 'responder_6_lag_1']\n"
     ]
    }
   ],
   "source": [
    "PATH = os.getcwd() + '/input/'\n",
    "\n",
    "METAS = ['date_id', 'time_id', 'symbol_id', 'weight']\n",
    "FEATURES = [f'feature_{i:02}' for i in range(79)]\n",
    "RESPONDERS = [f'responder_{i}' for i in range(9)]\n",
    "RESPONDERS_LAGS = [f'responder_{i}_lag_1' for i in range(9)]\n",
    "TARGET = 'responder_6'\n",
    "FEATURES_AND_LAGS=True\n",
    "ONLY_TARGET_LAGS=True\n",
    "if FEATURES_AND_LAGS and not ONLY_TARGET_LAGS:\n",
    "    FEATURES_WORKING = FEATURES + RESPONDERS + RESPONDERS_LAGS\n",
    "    FEATURES_WORKING.remove(TARGET)\n",
    "elif FEATURES_AND_LAGS and ONLY_TARGET_LAGS:\n",
    "    FEATURES_WORKING = FEATURES + ['responder_6_lag_1']\n",
    "else:\n",
    "    FEATURES_WORKING = FEATURES\n",
    "print(FEATURES_WORKING)\n",
    "\n",
    "USEFUL_COLS = FEATURES_WORKING + [TARGET] + ['weight']\n",
    "\n",
    "TEST=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "269b644e-d18f-4f60-8a64-89d7a47454c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling...\n",
      "Collecting dataframe...\n",
      "Generated data variables.\n",
      "Attempted garbage collection.\n",
      "Saved trainset.\n"
     ]
    }
   ],
   "source": [
    "# should test models with only features, features + responders, + lags, etc...\n",
    "if TEST:\n",
    "    train_df = pl.scan_parquet(PATH+'training_TEST.parquet') # this parquet has lags\n",
    "    valid_df = pl.scan_parquet(PATH+'validation_TEST.parquet') # this parquet has lags\n",
    "    train_save_path = f'./input/trainsets/working_trainset_TEST.pt'\n",
    "    valid_save_path = f'./input/validsets/working_validset_TEST.pt'\n",
    "else:\n",
    "    train_df = pl.scan_parquet(PATH+'training.parquet').select(USEFUL_COLS)\n",
    "    valid_df = pl.scan_parquet(PATH+'validation.parquet').select(USEFUL_COLS)\n",
    "    train_save_path = f'./input/trainsets/working_trainset.pt'\n",
    "    valid_save_path = f'./input/validsets/working_validset.pt'\n",
    "\n",
    "feature_tags = pl.read_csv(PATH+'features.csv') # no one seems to use this....\n",
    "gc.collect()\n",
    "\n",
    "print('Filling...')\n",
    "train_df = train_df.select([\n",
    "    pl.col(c).fill_null(pl.col(c).mean()).alias(c) for c in train_df.columns\n",
    "])\n",
    "\n",
    "fold_function = 'None'\n",
    "\n",
    "if fold_function == 'PurgedGroupTimeSeries':\n",
    "    X = train_df[FEATURES_WORKING].to_numpy()\n",
    "    y = train_df[TARGET].to_numpy()\n",
    "    group = train_df['date_id'].to_numpy()\n",
    "    w = train_df['weight'].to_numpy()\n",
    "    # Define the PurgedGroupTimeSeriesCV\n",
    "    cv = PurgedGroupTimeSeriesSplit(\n",
    "    n_splits=folds,\n",
    "    max_train_group_size=80,\n",
    "    group_gap=10,\n",
    "    max_test_group_size=20\n",
    "    )\n",
    "    folds = cv.split(X=X, y=y, groups=group)\n",
    "elif fold_function == 'KFold': # TODO\n",
    "    folds = _\n",
    "elif fold_function == 'None' :\n",
    "    folds = [([0,0],[0,0])]\n",
    "    print('Collecting dataframe...')\n",
    "    train_dfc = train_df.collect().to_pandas()\n",
    "    train_dfc = train_dfc.astype(np.float16)\n",
    "    X_tr, y_tr, w_tr = train_dfc[FEATURES_WORKING].values, train_dfc[TARGET].values, train_dfc[\"weight\"].values\n",
    "else:\n",
    "    folds = _\n",
    "print('Generated data variables.') \n",
    "gc.collect()\n",
    "train_dfc = None\n",
    "gc.collect()\n",
    "print('Attempted garbage collection.')\n",
    "\n",
    "trainset = TensorDataset(torch.tensor(X_tr), torch.tensor(y_tr), torch.tensor(w_tr))\n",
    "torch.save(trainset, save_path)\n",
    "print('Saved trainset.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97c46da6-dd99-4dcd-aa29-3a6fad0776ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dataframe...\n",
      "Generated data variables.\n",
      "Attempted garbage collection.\n",
      "Saved validset.\n"
     ]
    }
   ],
   "source": [
    "valid_df = valid_df.select([\n",
    "    pl.col(c).fill_null(pl.col(c).mean()).alias(c) for c in valid_df.columns\n",
    "])\n",
    "fold_function = 'None'\n",
    "\n",
    "if fold_function == 'PurgedGroupTimeSeries':\n",
    "    X = valid_df[FEATURES_WORKING].to_numpy()\n",
    "    y = valid_df[TARGET].to_numpy()\n",
    "    group = valid_df['date_id'].to_numpy()\n",
    "    w = valid_df['weight'].to_numpy()\n",
    "    # Define the PurgedGroupTimeSeriesCV\n",
    "    cv = PurgedGroupTimeSeriesSplit(\n",
    "    n_splits=folds,\n",
    "    max_valid_group_size=80,\n",
    "    group_gap=10,\n",
    "    max_test_group_size=20\n",
    "    )\n",
    "    folds = cv.split(X=X, y=y, groups=group)\n",
    "elif fold_function == 'KFold': # TODO\n",
    "    folds = _\n",
    "elif fold_function == 'None' :\n",
    "    folds = [([0,0],[0,0])]\n",
    "    print('Collecting dataframe...')\n",
    "    valid_dfc = valid_df.collect().to_pandas()\n",
    "    valid_dfc = valid_dfc.astype(np.float16)\n",
    "    X_vl, y_vl, w_vl = valid_dfc[FEATURES_WORKING].values, valid_dfc[TARGET].values, valid_dfc[\"weight\"].values\n",
    "else:\n",
    "    folds = _\n",
    "print('Generated data variables.') \n",
    "gc.collect()\n",
    "valid_dfc = None\n",
    "gc.collect()\n",
    "print('Attempted garbage collection.')\n",
    "\n",
    "validset = TensorDataset(torch.tensor(X_vl), torch.tensor(y_vl), torch.tensor(w_vl))\n",
    "torch.save(validset, save_path)\n",
    "print('Saved validset.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e7e079-ea76-42d8-a92c-e5b9fc9c890f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
