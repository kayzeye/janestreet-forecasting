{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os, gc\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import random\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "#import catboost as cbt # needs numpy <2.0\n",
    "from joblib import Parallel, delayed\n",
    "import numpy as np\n",
    "#from hyperopt import hp, fmin, tpe, Trials\n",
    "#from hyperopt.pyll.base import scope\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import matplotlib.pyplot as plt\n",
    "#from tqdm.notebook import tqdm\n",
    "#from joblib import dump, load\n",
    "#import datatable as dtable\n",
    "#from mlxtend.evaluate import GroupTimeSeriesSplit\n",
    "import kaggle_evaluation.jane_street_inference_server as js_server\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that torch is working and sees the GPU\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collected = gc.collect()\n",
    "# Prints Garbage collector \n",
    "# as 0 object\n",
    "print(\"Garbage collector: collected\",\n",
    "          \"%d objects.\" % collected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Globals and prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = os.getcwd() + '/input/'\n",
    "\n",
    "METAS = ['date_id', 'time_id', 'symbol_id', 'weight']\n",
    "FEATURES = [f'feature_{i:02}' for i in range(79)]\n",
    "RESPONDERS = [f'responder_{i}' for i in range(9)]\n",
    "TARGET = 'responder_6'\n",
    "SEQUENCE_LEN = 16 # not sure what this is for\n",
    "\n",
    "SEED = 728\n",
    "\n",
    "TEST=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        # Ensure deterministic behavior (may impact performance)\n",
    "        torch.backends.cudnn.deterministic = False\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def lazy_load(par_path):\n",
    "    return pl.scan_parquet(par_path).select(\n",
    "        pl.int_range(pl.len(), dtype=pl.UInt64).alias(\"index\"),\n",
    "        pl.all()\n",
    "    )\n",
    "seed_everything(SEED) # just do it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use synthetic test?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is just copied boilerplate, don't have synthetic testing atm\n",
    "USE_SYNTHETIC = False\n",
    "\n",
    "if USE_SYNTHETIC:\n",
    "    syn_dir = '/kaggle/input/js24-rmf-generate-synthetic-test-data'\n",
    "    test_parquet = f'{syn_dir}/synthetic_test.parquet'\n",
    "    lag_parquet = f'{syn_dir}/synthetic_lag.parquet'\n",
    "    total_time_steps = pl.scan_parquet(test_parquet).select(\n",
    "        (pl.col(\"date_id\")*10000+pl.col('time_id')).n_unique()   \n",
    "        ).collect().item()\n",
    "else:\n",
    "    test_parquet_path = PATH + 'test.parquet/'\n",
    "    lag_parquet_path =  PATH + 'lags.parquet/'\n",
    "    total_time_steps = 1\n",
    "    \n",
    "print(\"Test parquet:\", test_parquet_path)\n",
    "print(\"Lag parquet:\", lag_parquet_path)\n",
    "print(\"Total time steps:\", total_time_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST:\n",
    "    df1 = pl.read_parquet(PATH+'train.parquet/partition_id=7/part-0.parquet')\n",
    "    df2 = pl.read_parquet(PATH+'train.parquet/partition_id=8/part-0.parquet')\n",
    "    df3 = pl.read_parquet(PATH+'train.parquet/partition_id=9/part-0.parquet')\n",
    "    train_df = pl.concat([df1,df2, df3])\n",
    "    del df1, df2, df3\n",
    "else:\n",
    "    # very important to optimize memory when loading the full damn thing\n",
    "    cols = ['date_id', 'time_id', 'weight', 'symbol_id']\n",
    "    cols.extend(['feature_{:02d}'.format(num) for num in range(0, 79)])\n",
    "    cols.append('responder_6')\n",
    "    df1 = pl.read_parquet(PATH+'train.parquet/partition_id=0/part-0.parquet', columns=cols)\n",
    "    df2 = pl.read_parquet(PATH+'train.parquet/partition_id=1/part-0.parquet', columns=cols)\n",
    "    df3 = pl.read_parquet(PATH+'train.parquet/partition_id=2/part-0.parquet', columns=cols)\n",
    "    df4 = pl.read_parquet(PATH+'train.parquet/partition_id=3/part-0.parquet', columns=cols)\n",
    "    df5 = pl.read_parquet(PATH+'train.parquet/partition_id=4/part-0.parquet', columns=cols)\n",
    "    df6 = pl.read_parquet(PATH+'train.parquet/partition_id=5/part-0.parquet', columns=cols)\n",
    "    df7 = pl.read_parquet(PATH+'train.parquet/partition_id=6/part-0.parquet', columns=cols)\n",
    "    df8 = pl.read_parquet(PATH+'train.parquet/partition_id=7/part-0.parquet', columns=cols)\n",
    "    df9 = pl.read_parquet(PATH+'train.parquet/partition_id=8/part-0.parquet', columns=cols)\n",
    "    df10 = pl.read_parquet(PATH+'train.parquet/partition_id=9/part-0.parquet', columns=cols)\n",
    "    train_df = pl.concat([df1, df2, df3, df4, df5, df6, df7, df8, df9, df10])\n",
    "    del df1, df2, df3, df4, df5, df6, df7, df8, df9, df10\n",
    "\n",
    "lags = pl.read_parquet(lag_parquet_path+'date_id=0/part-0.parquet')\n",
    "feature_tags = pl.read_csv(PATH+'features.csv') # no one seems to use this....\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Filling...')\n",
    "# next two lines were for pandas not polars\n",
    "#f_mean = train[features].mean() \n",
    "# print(train.weight.gt(0).sum() == train.shape[0]) # check if weights>0 is true for entire dataframe\n",
    "print(train_df.shape)\n",
    "train_df = train_df.fill_null(0)\n",
    "#train.dropna(inplace=True)\n",
    "print(train_df.null_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# for the purged group time series split, code is copied from somewhere\n",
    "# TODO: make GitHub GIST\n",
    "# TODO: add as dataset\n",
    "# TODO: add logging with verbose\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection._split import _BaseKFold, indexable, _num_samples\n",
    "from sklearn.utils.validation import _deprecate_positional_args\n",
    "\n",
    "# modified code for group gaps; source\n",
    "# https://github.com/getgaurav2/scikit-learn/blob/d4a3af5cc9da3a76f0266932644b884c99724c57/sklearn/model_selection/_split.py#L2243\n",
    "class PurgedGroupTimeSeriesSplit(_BaseKFold):\n",
    "    \"\"\"Time Series cross-validator variant with non-overlapping groups.|\n",
    "    Allows for a gap in groups to avoid potentially leaking info from\n",
    "    train into test if the model has windowed or lag features.\n",
    "    Provides train/test indices to split time series data samples\n",
    "    that are observed at fixed time intervals according to a\n",
    "    third-party provided group.\n",
    "    In each split, test indices must be higher than before, and thus shuffling\n",
    "    in cross validator is inappropriate.\n",
    "    This cross-validation object is a variation of :class:`KFold`.\n",
    "    In the kth split, it returns first k folds as train set and the\n",
    "    (k+1)th fold as test set.\n",
    "    The same group will not appear in two different folds (the number of\n",
    "    distinct groups has to be at least equal to the number of folds).\n",
    "    Note that unlike standard cross-validation methods, successive\n",
    "    training sets are supersets of those that come before them.\n",
    "    Read more in the :ref:`User Guide <cross_validation>`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_splits : int, default=5\n",
    "        Number of splits. Must be at least 2.\n",
    "    max_train_group_size : int, default=Inf\n",
    "        Maximum group size for a single training set.\n",
    "    group_gap : int, default=None\n",
    "        Gap between train and test\n",
    "    max_test_group_size : int, default=Inf\n",
    "        We discard this number of groups from the end of each train split\n",
    "    \"\"\"\n",
    "\n",
    "    @_deprecate_positional_args\n",
    "    def __init__(self,\n",
    "                 n_splits=5,\n",
    "                 *,\n",
    "                 max_train_group_size=np.inf,\n",
    "                 max_test_group_size=np.inf,\n",
    "                 group_gap=None,\n",
    "                 verbose=False\n",
    "                 ):\n",
    "        super().__init__(n_splits, shuffle=False, random_state=None)\n",
    "        self.max_train_group_size = max_train_group_size\n",
    "        self.group_gap = group_gap\n",
    "        self.max_test_group_size = max_test_group_size\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        \"\"\"Generate indices to split data into training and test set.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Training data, where n_samples is the number of samples\n",
    "            and n_features is the number of features.\n",
    "        y : array-like of shape (n_samples,)\n",
    "            Always ignored, exists for compatibility.\n",
    "        groups : array-like of shape (n_samples,)\n",
    "            Group labels for the samples used while splitting the dataset into\n",
    "            train/test set.\n",
    "        Yields\n",
    "        ------\n",
    "        train : ndarray\n",
    "            The training set indices for that split.\n",
    "        test : ndarray\n",
    "            The testing set indices for that split.\n",
    "        \"\"\"\n",
    "        if groups is None:\n",
    "            raise ValueError(\n",
    "                \"The 'groups' parameter should not be None\")\n",
    "        X, y, groups = indexable(X, y, groups)\n",
    "        n_samples = _num_samples(X)\n",
    "        n_splits = self.n_splits\n",
    "        group_gap = self.group_gap\n",
    "        max_test_group_size = self.max_test_group_size\n",
    "        max_train_group_size = self.max_train_group_size\n",
    "        n_folds = n_splits + 1\n",
    "        group_dict = {}\n",
    "        u, ind = np.unique(groups, return_index=True)\n",
    "        unique_groups = u[np.argsort(ind)]\n",
    "        n_samples = _num_samples(X)\n",
    "        n_groups = _num_samples(unique_groups)\n",
    "        for idx in np.arange(n_samples):\n",
    "            if (groups[idx] in group_dict):\n",
    "                group_dict[groups[idx]].append(idx)\n",
    "            else:\n",
    "                group_dict[groups[idx]] = [idx]\n",
    "        if n_folds > n_groups:\n",
    "            raise ValueError(\n",
    "                (\"Cannot have number of folds={0} greater than\"\n",
    "                 \" the number of groups={1}\").format(n_folds,\n",
    "                                                     n_groups))\n",
    "\n",
    "        group_test_size = min(n_groups // n_folds, max_test_group_size)\n",
    "        group_test_starts = range(n_groups - n_splits * group_test_size,\n",
    "                                  n_groups, group_test_size)\n",
    "        for group_test_start in group_test_starts:\n",
    "            train_array = []\n",
    "            test_array = []\n",
    "\n",
    "            group_st = max(0, group_test_start - group_gap - max_train_group_size)\n",
    "            for train_group_idx in unique_groups[group_st:(group_test_start - group_gap)]:\n",
    "                train_array_tmp = group_dict[train_group_idx]\n",
    "                \n",
    "                train_array = np.sort(np.unique(\n",
    "                                      np.concatenate((train_array,\n",
    "                                                      train_array_tmp)),\n",
    "                                      axis=None), axis=None)\n",
    "\n",
    "            train_end = train_array.size\n",
    " \n",
    "            for test_group_idx in unique_groups[group_test_start:\n",
    "                                                group_test_start +\n",
    "                                                group_test_size]:\n",
    "                test_array_tmp = group_dict[test_group_idx]\n",
    "                test_array = np.sort(np.unique(\n",
    "                                              np.concatenate((test_array,\n",
    "                                                              test_array_tmp)),\n",
    "                                     axis=None), axis=None)\n",
    "\n",
    "            test_array  = test_array[group_gap:]\n",
    "            \n",
    "            \n",
    "            if self.verbose > 0:\n",
    "                    pass\n",
    "                    \n",
    "            yield [int(i) for i in train_array], [int(i) for i in test_array]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# from matplotlib.colors import ListedColormap\n",
    "    \n",
    "# # this is code slightly modified from the sklearn docs here:\n",
    "# # https://scikit-learn.org/stable/auto_examples/model_selection/plot_cv_indices.html#sphx-glr-auto-examples-model-selection-plot-cv-indices-py\n",
    "# def plot_cv_indices(cv, X, y, group, ax, n_splits, lw=10):\n",
    "#     \"\"\"Create a sample plot for indices of a cross-validation object.\"\"\"\n",
    "    \n",
    "#     cmap_cv = plt.cm.coolwarm\n",
    "\n",
    "#     jet = plt.cm.get_cmap('jet', 256)\n",
    "#     seq = np.linspace(0, 1, 256)\n",
    "#     _ = np.random.shuffle(seq)   # inplace\n",
    "#     cmap_data = ListedColormap(jet(seq))\n",
    "\n",
    "#     # Generate the training/testing visualizations for each CV split\n",
    "#     for ii, (tr, tt) in enumerate(cv.split(X=X, y=y, groups=group)):\n",
    "#         # Fill in indices with the training/test groups\n",
    "#         indices = np.array([np.nan] * len(X))\n",
    "#         indices[tt] = 1\n",
    "#         indices[tr] = 0\n",
    "\n",
    "#         # Visualize the results\n",
    "#         ax.scatter(range(len(indices)), [ii + .5] * len(indices),\n",
    "#                    c=indices, marker='_', lw=lw, cmap=cmap_cv,\n",
    "#                    vmin=-.2, vmax=1.2)\n",
    "\n",
    "#     # Plot the data classes and groups at the end\n",
    "#     ax.scatter(range(len(X)), [ii + 1.5] * len(X),\n",
    "#                c=y, marker='_', lw=lw, cmap=plt.cm.Set3)\n",
    "\n",
    "#     ax.scatter(range(len(X)), [ii + 2.5] * len(X),\n",
    "#                c=group, marker='_', lw=lw, cmap=cmap_data)\n",
    "\n",
    "#     # Formatting\n",
    "#     yticklabels = list(range(n_splits)) + ['target', 'day']\n",
    "#     ax.set(yticks=np.arange(n_splits+2) + .5, yticklabels=yticklabels,\n",
    "#            xlabel='Sample index', ylabel=\"CV iteration\",\n",
    "#            ylim=[n_splits+2.2, -.2], xlim=[0, len(y)])\n",
    "#     ax.set_title('{}'.format(type(cv).__name__), fontsize=15)\n",
    "#     return ax\n",
    "\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# #plot_cv_indices(cv, X_train, y_train, groups, ax, 5, lw=20)\n",
    "# plot_cv_indices(\n",
    "#     cv,\n",
    "#     train[features].values,\n",
    "#     train['responder_6'].values,\n",
    "#     train['date_id'].values,\n",
    "#     ax,\n",
    "#     5,\n",
    "#     lw=20\n",
    "# )\n",
    "# rubbish=gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# code structure copied/inspired from https://www.kaggle.com/code/shiyili/js2024-rmf-mlp-inference-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define various helper functions including r2 score\n",
    "# get cpu, gpu or mps device for training\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "def reset_weights(m):\n",
    "    '''\n",
    "    Try resetting model weights to avoid\n",
    "    weight leakage.\n",
    "    '''\n",
    "    for layer in m.children():\n",
    "        if hasattr(layer, 'reset_parameters'):\n",
    "            print(f'Reset trainable parameters of layer = {layer}')\n",
    "            layer.reset_parameters()\n",
    "\n",
    "# for monitoring layer weights\n",
    "def weight_histograms(writer, step, model):\n",
    "    print(\"Visualizing model weights...\")\n",
    "    # Iterate over all model layers\n",
    "    for layer_number in range(len(model.layers)):\n",
    "        layer = model.layers[layer_number]\n",
    "        try:\n",
    "            weights = layer.weight\n",
    "            flattened_weights = weights.flatten()\n",
    "            tag = f\"layer_{layer_number}\"\n",
    "            writer.add_histogram(tag, flattened_weights, global_step=step, bins='tensorflow')\n",
    "        except AttributeError:\n",
    "            return\n",
    "            \n",
    "# loss is as defined on competition homepage\n",
    "def r2_loss(outputs, targets, weights):\n",
    "    loss = torch.sum(weights*(targets - outputs)**2) / (torch.sum(weights*targets**2)+1e-38)\n",
    "    return loss\n",
    "\n",
    "# \"standard\" loss function\n",
    "test_loss_function= nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic LSTM model\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=self.input_dim, hidden_size=self.hidden_dim, num_layers=1, batch_first=True)\n",
    "        self.linear = nn.Linear(self.hidden_dim, self.output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''Forward pass'''\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.linear(x)\n",
    "        return x.squeeze()\n",
    "\n",
    "# copied LSTM equivalent model\n",
    "class LSTMEquavalentMLP(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(LSTMEquavalentMLP, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.input_gate = nn.Linear(input_size, hidden_size)\n",
    "        self.candidate_gate = nn.Linear(input_size, hidden_size)\n",
    "        self.output_gate = nn.Linear(input_size, hidden_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        i_t = torch.sigmoid(self.input_gate(x)) # input gate\n",
    "        c_t = torch.tanh(self.candidate_gate(x)) # candidate gate\n",
    "        o_t = torch.sigmoid(self.output_gate(x)) # output gate\n",
    "        \n",
    "        h_t = o_t * torch.tanh(c_t * i_t)\n",
    "        \n",
    "        return h_t\n",
    "    \n",
    "class LSTMFeedForward(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTMFeedForward, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.lstm = LSTMEquavalentMLP(input_size, hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        h_t = self.lstm(x)\n",
    "        y = self.fc(h_t).squeeze()\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training function\n",
    "def train(model, trainloader, optimizer, num_epochs, save=True):\n",
    "    model.train()\n",
    "    global writer\n",
    "    # Run the training loop for defined number of epochs\n",
    "    for epoch in range(0, num_epochs):\n",
    "        # Visualize weight histograms\n",
    "        weight_histograms(writer, epoch, model)\n",
    "        # Print epoch\n",
    "        print(f'Starting epoch {epoch+1}')\n",
    "        \n",
    "        # Set current loss value\n",
    "        current_loss = 0.0\n",
    "        # Iterate over Dataloader for training data\n",
    "        for i, data in enumerate(trainloader):\n",
    "            # Get inputs\n",
    "            inputs, targets, weights = data\n",
    "            inputs = inputs.to(device) # GPU must see the data and the model\n",
    "            targets = targets.to(device)\n",
    "            weights = weights.to(device)\n",
    "            \n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Perform forward pass\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Compute loss\n",
    "            if test_loss:\n",
    "                loss = test_loss_function(outputs, targets)\n",
    "            else:\n",
    "                loss = r2_loss(outputs, targets, weights)\n",
    "            # Perform backward pass\n",
    "            loss.backward()\n",
    "            # Perform optimization\n",
    "            optimizer.step()\n",
    "\n",
    "        # Finished iterating over Dataloader\n",
    "        writer.add_scalar(\"Loss/train/epoch\", loss, epoch)\n",
    "        writer.flush()\n",
    "      \n",
    "    # Saving the model\n",
    "    if save:\n",
    "        save_path = f'./models/lstm_{fold}.pth'\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "    return\n",
    "\n",
    "# testing function\n",
    "def test(model, testloader, ):\n",
    "    global writer\n",
    "    # Evaluation for this fold\n",
    "    R2_score = 0.0\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        # Iterate over the test data and generate predictions\n",
    "        for i, data in enumerate(testloader):\n",
    "            # Get inputs\n",
    "            inputs, targets, weights = data\n",
    "            inputs = inputs.to(device) # GPU must see the data and the model\n",
    "            targets = targets.to(device)\n",
    "            weights = weights.to(device)\n",
    "        \n",
    "            # Generate outputs\n",
    "            outputs = model(inputs)\n",
    "            if test_loss:\n",
    "                loss = test_loss_function(outputs, targets)\n",
    "            else:\n",
    "                 loss = my_loss(outputs, targets, weights)\n",
    "\n",
    "            # writer.add_scalar('Loss/test/minibatches', loss, batch_test_tally)\n",
    "            # batch_test_tally += 1\n",
    "            total_loss += loss\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Configuration options\n",
    "num_folds = 2\n",
    "num_epochs = 3 \n",
    "test_loss = False\n",
    "fold_function = 'PurgedGroupTimeSeries'\n",
    "# fold_function = 'KFold'\n",
    "fold_function = 'Custom'\n",
    "# For fold results\n",
    "results = {}\n",
    "\n",
    "# extract needed parameters from dataframe as np arrays, will later turn into tensors\n",
    "X = train_df[FEATURES].to_numpy()\n",
    "y = train_df[TARGET].to_numpy()\n",
    "group = train_df['date_id'].to_numpy()\n",
    "w = train_df['weight'].to_numpy()\n",
    "\n",
    "if fold_function == 'PurgedGroupTimeSeries':\n",
    "    # Define the PurgedGroupTimeSeriesCV\n",
    "    cv = PurgedGroupTimeSeriesSplit(\n",
    "    n_splits=folds,\n",
    "    max_train_group_size=80,\n",
    "    group_gap=10,\n",
    "    max_test_group_size=20\n",
    "    )\n",
    "    folds = cv.split(X=X, y=y, groups=group)\n",
    "elif fold_function == 'KFold': # TODO\n",
    "    folds = _\n",
    "elif fold_function == 'Custom' :\n",
    "    \n",
    "    folds = _\n",
    "else:\n",
    "    folds = _\n",
    "\n",
    "# Define the neural network parameters\n",
    "input_dim = len(FEATURES) \n",
    "output_dim = 1\n",
    "hidden_dim = 256\n",
    "learning_rate = 1e-4\n",
    "print('--------------------------------')\n",
    "\n",
    "# neat hack to ensure the original dataframe is gone\n",
    "# del train_df\n",
    "# gc.collect()\n",
    "# train_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train and cross validation loop\n",
    "for fold, (train_ids, test_ids) in enumerate(folds):\n",
    "\n",
    "    # Print\n",
    "    print(f'FOLD {fold}')\n",
    "    print(f'train_ids: [{train_ids[0]}, {train_ids[-1]}]')\n",
    "    print(f'test_ids: [{test_ids[0]}, {test_ids[-1]}]')\n",
    "    print('--------------------------------')\n",
    "    \n",
    "    # Sample elements randomly from a given list of ids, no replacement.\n",
    "    # does this make sense for a time series?... I don't think so\n",
    "    #train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "    #test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n",
    "\n",
    "    # define train/test sets\n",
    "    X_tr, y_tr, w_tr = X[train_ids], y[train_ids], w[train_ids]\n",
    "    X_te, y_te, w_te = X[test_ids], y[test_ids], w[test_ids]\n",
    "    # Initialize Dataset objects to make PyTorch play nice\n",
    "    trainset = TensorDataset(torch.tensor(X_tr), torch.tensor(y_tr), torch.tensor(w_tr))\n",
    "    testset = TensorDataset(torch.tensor(X_te), torch.tensor(y_te), torch.tensor(w_te))\n",
    "    \n",
    "    # Define data loaders\n",
    "    BATCH_SIZE = 4096\n",
    "    NUM_WORKERS = 4 # num of parallel subprocesses for data loading (CPU task)\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "        trainset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_workers = NUM_WORKERS\n",
    "    )\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "        testset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_workers = NUM_WORKERS\n",
    "    )\n",
    "\n",
    "    # Init the neural network\n",
    "    network = MLP(input_dim, output_dim, hidden_dim, dropout_rates).to(device) # .to(device) sends model to GPU\n",
    "    network.apply(reset_weights) # not sure if I need this but w/e\n",
    "    # Initialize optimizer\n",
    "    optimizer = torch.optim.Adam(network.parameters(), lr=learning_rate)\n",
    "    network.train()\n",
    "\n",
    "    batch_train_tally = 0\n",
    "    # Run the training loop for defined number of epochs\n",
    "    for epoch in range(0, num_epochs):\n",
    "\n",
    "        # Visualize weight histograms\n",
    "        weight_histograms(writer, epoch, network)\n",
    "        \n",
    "        # Print epoch\n",
    "        print(f'Starting epoch {epoch+1}')\n",
    "        \n",
    "        # Set current loss value\n",
    "        current_loss = 0.0\n",
    "\n",
    "        # Iterate over Dataloader for training data\n",
    "        for i, data in enumerate(trainloader):\n",
    "\n",
    "            # Get inputs\n",
    "            inputs, targets, weights = data\n",
    "            inputs = inputs.to(device) # GPU must see the data and the model\n",
    "            targets = targets.to(device)\n",
    "            weights = weights.to(device)\n",
    "            \n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Perform forward pass\n",
    "            outputs = network(inputs)\n",
    "            \n",
    "            # Compute loss\n",
    "            if test_loss:\n",
    "                loss = test_loss_function(outputs[:,0], targets)\n",
    "            else:\n",
    "                loss = my_loss(outputs[:,0], targets, weights)\n",
    "            writer.add_scalar(\"Loss/train/minibatches\", loss, batch_train_tally)\n",
    "            batch_train_tally += 1\n",
    "            \n",
    "            # Perform backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Perform optimization\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Print statistics\n",
    "            current_loss += loss.item()\n",
    "            if i % 1000 == 999:\n",
    "                print('Loss after mini-batch %5d: %.3f' %\n",
    "                      (i + 1, current_loss / 1000))\n",
    "                current_loss = 0.0\n",
    "\n",
    "        # Finished iterating over Dataloader\n",
    "        writer.add_scalar(\"Loss/train/epoch\", loss, epoch)\n",
    "        writer.flush()\n",
    "    \n",
    "    # Process is complete.\n",
    "    print('Training process has finished')\n",
    "    \n",
    "    # Print about testing\n",
    "    print('Starting testing')\n",
    "    \n",
    "    # Saving the model\n",
    "    save_path = f'./model-fold-{fold}.pth'\n",
    "    torch.save(network.state_dict(), save_path)\n",
    "\n",
    "    # Evaluation for this fold\n",
    "    R2_score = 0.0\n",
    "    total_loss = 0.0\n",
    "    batch_test_tally = 0\n",
    "    with torch.no_grad():\n",
    "        network.eval()\n",
    "        # Iterate over the test data and generate predictions\n",
    "        for i, data in enumerate(testloader):\n",
    "            # Get inputs\n",
    "            inputs, targets, weights = data\n",
    "            inputs = inputs.to(device) # GPU must see the data and the model\n",
    "            targets = targets.to(device)\n",
    "            weights = weights.to(device)\n",
    "        \n",
    "            # Generate outputs\n",
    "            outputs = network(inputs)\n",
    "            if test_loss:\n",
    "                loss = test_loss_function(outputs[:,0], targets)\n",
    "            else:\n",
    "                 loss = my_loss(outputs[:,0], targets, weights)\n",
    "\n",
    "            writer.add_scalar('Loss/test/minibatches', loss, batch_test_tally)\n",
    "            batch_test_tally += 1\n",
    "            \n",
    "            total_loss += loss\n",
    "\n",
    "        print('--------------------------------')\n",
    "        num_batches = len(testloader)\n",
    "        results[fold] = total_loss / num_batches\n",
    "        writer.add_scalar('Loss/test/fold', results[fold], fold)\n",
    "        writer.flush()\n",
    "    \n",
    "    # Print fold results\n",
    "    print(f'CROSS VALIDATION RESULTS FOR {folds} FOLDS')\n",
    "    print('--------------------------------')\n",
    "    sum = 0.0\n",
    "    for key, value in results.items():\n",
    "        print(f'Fold {key}: {value}')\n",
    "        sum += value\n",
    "    print(f'Average: {sum/len(results.items())}')\n",
    "writer.close()\n",
    "rubbish=gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train and cross validation loop\n",
    "for fold, (train_ids, test_ids) in enumerate(cv.split(X=X, y=y, groups=group)):\n",
    "\n",
    "    # Print\n",
    "    print(f'FOLD {fold}')\n",
    "    print(f'train_ids: [{train_ids[0]}, {train_ids[-1]}]')\n",
    "    print(f'test_ids: [{test_ids[0]}, {test_ids[-1]}]')\n",
    "    print('--------------------------------')\n",
    "    \n",
    "    # Sample elements randomly from a given list of ids, no replacement.\n",
    "    # does this make sense for a time series?... I don't think so\n",
    "    #train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "    #test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n",
    "\n",
    "    # define train/test sets\n",
    "    X_tr, y_tr, w_tr = X[train_ids], y[train_ids], w[train_ids]\n",
    "    X_te, y_te, w_te = X[test_ids], y[test_ids], w[test_ids]\n",
    "    # Initialize Dataset objects to make PyTorch play nice\n",
    "    trainset = TensorDataset(torch.tensor(X_tr), torch.tensor(y_tr), torch.tensor(w_tr))\n",
    "    testset = TensorDataset(torch.tensor(X_te), torch.tensor(y_te), torch.tensor(w_te))\n",
    "    \n",
    "    # Define data loaders\n",
    "    BATCH_SIZE = 4096\n",
    "    NUM_WORKERS = 4 # num of parallel subprocesses for data loading (CPU task)\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "        trainset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_workers = NUM_WORKERS\n",
    "    )\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "        testset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_workers = NUM_WORKERS\n",
    "    )\n",
    "\n",
    "    # Init the neural network\n",
    "    network = MLP(input_dim, output_dim, hidden_dim, dropout_rates).to(device) # .to(device) sends model to GPU\n",
    "    network.apply(reset_weights) # not sure if I need this but w/e\n",
    "    # Initialize optimizer\n",
    "    optimizer = torch.optim.Adam(network.parameters(), lr=learning_rate)\n",
    "    network.train()\n",
    "\n",
    "    batch_train_tally = 0\n",
    "    # Run the training loop for defined number of epochs\n",
    "    for epoch in range(0, num_epochs):\n",
    "\n",
    "        # Visualize weight histograms\n",
    "        weight_histograms(writer, epoch, network)\n",
    "        \n",
    "        # Print epoch\n",
    "        print(f'Starting epoch {epoch+1}')\n",
    "        \n",
    "        # Set current loss value\n",
    "        current_loss = 0.0\n",
    "\n",
    "        # Iterate over Dataloader for training data\n",
    "        for i, data in enumerate(trainloader):\n",
    "\n",
    "            # Get inputs\n",
    "            inputs, targets, weights = data\n",
    "            inputs = inputs.to(device) # GPU must see the data and the model\n",
    "            targets = targets.to(device)\n",
    "            weights = weights.to(device)\n",
    "            \n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Perform forward pass\n",
    "            outputs = network(inputs)\n",
    "            \n",
    "            # Compute loss\n",
    "            if test_loss:\n",
    "                loss = test_loss_function(outputs[:,0], targets)\n",
    "            else:\n",
    "                loss = my_loss(outputs[:,0], targets, weights)\n",
    "            writer.add_scalar(\"Loss/train/minibatches\", loss, batch_train_tally)\n",
    "            batch_train_tally += 1\n",
    "            \n",
    "            # Perform backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Perform optimization\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Print statistics\n",
    "            current_loss += loss.item()\n",
    "            if i % 1000 == 999:\n",
    "                print('Loss after mini-batch %5d: %.3f' %\n",
    "                      (i + 1, current_loss / 1000))\n",
    "                current_loss = 0.0\n",
    "\n",
    "        # Finished iterating over Dataloader\n",
    "        writer.add_scalar(\"Loss/train/epoch\", loss, epoch)\n",
    "        writer.flush()\n",
    "    \n",
    "    # Process is complete.\n",
    "    print('Training process has finished')\n",
    "    \n",
    "    # Print about testing\n",
    "    print('Starting testing')\n",
    "    \n",
    "    # Saving the model\n",
    "    save_path = f'./model-fold-{fold}.pth'\n",
    "    torch.save(network.state_dict(), save_path)\n",
    "\n",
    "    # Evaluation for this fold\n",
    "    R2_score = 0.0\n",
    "    total_loss = 0.0\n",
    "    batch_test_tally = 0\n",
    "    with torch.no_grad():\n",
    "        network.eval()\n",
    "        # Iterate over the test data and generate predictions\n",
    "        for i, data in enumerate(testloader):\n",
    "            # Get inputs\n",
    "            inputs, targets, weights = data\n",
    "            inputs = inputs.to(device) # GPU must see the data and the model\n",
    "            targets = targets.to(device)\n",
    "            weights = weights.to(device)\n",
    "        \n",
    "            # Generate outputs\n",
    "            outputs = network(inputs)\n",
    "            if test_loss:\n",
    "                loss = test_loss_function(outputs[:,0], targets)\n",
    "            else:\n",
    "                 loss = my_loss(outputs[:,0], targets, weights)\n",
    "\n",
    "            writer.add_scalar('Loss/test/minibatches', loss, batch_test_tally)\n",
    "            batch_test_tally += 1\n",
    "            \n",
    "            total_loss += loss\n",
    "\n",
    "        print('--------------------------------')\n",
    "        num_batches = len(testloader)\n",
    "        results[fold] = total_loss / num_batches\n",
    "        writer.add_scalar('Loss/test/fold', results[fold], fold)\n",
    "        writer.flush()\n",
    "    \n",
    "    # Print fold results\n",
    "    print(f'CROSS VALIDATION RESULTS FOR {folds} FOLDS')\n",
    "    print('--------------------------------')\n",
    "    sum = 0.0\n",
    "    for key, value in results.items():\n",
    "        print(f'Fold {key}: {value}')\n",
    "        sum += value\n",
    "    print(f'Average: {sum/len(results.items())}')\n",
    "writer.close()\n",
    "rubbish=gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lmao\n",
    "out = outputs.cpu().detach().numpy()\n",
    "tar = targets.cpu().detach().numpy()\n",
    "wei = weights.cpu().detach().numpy()\n",
    "print(out[10,0])\n",
    "nbins = 20\n",
    "plt.hist(tar, color='orange', label='targets', bins=nbins)\n",
    "plt.hist(wei, color='green', label='weights', bins=nbins)\n",
    "plt.hist(out[:,0], color='blue', bins=nbins, label='predicted')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
