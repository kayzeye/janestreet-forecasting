{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --quiet -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Jan 11 04:07:22 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3060        Off |   00000000:01:00.0  On |                  N/A |\n",
      "|  0%   44C    P3             32W /  170W |     862MiB /  12288MiB |     27%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os, gc\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import random\n",
    "import psutil\n",
    "import xgboost as xgb\n",
    "#import catboost as cbt # needs numpy <2.0\n",
    "import numpy as np\n",
    "#from hyperopt import hp, fmin, tpe, Trials\n",
    "#from hyperopt.pyll.base import scope\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import matplotlib.pyplot as plt\n",
    "#from tqdm.notebook import tqdm\n",
    "#from joblib import dump, load\n",
    "#import datatable as dtable\n",
    "#from mlxtend.evaluate import GroupTimeSeriesSplit\n",
    "import kaggle_evaluation.jane_street_inference_server as js_server\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1+cu121\n",
      "True\n",
      "NVIDIA GeForce RTX 3060\n"
     ]
    }
   ],
   "source": [
    "# check that torch is working and sees the GPU\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "torch.set_default_dtype(torch.float16) # because memory is fucking huge and I have to do this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Garbage collector: collected 72 objects.\n"
     ]
    }
   ],
   "source": [
    "collected = gc.collect()\n",
    "# Prints Garbage collector \n",
    "# as 0 object\n",
    "print(\"Garbage collector: collected\",\n",
    "          \"%d objects.\" % collected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Globals and prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['feature_00', 'feature_01', 'feature_02', 'feature_03', 'feature_04', 'feature_05', 'feature_06', 'feature_07', 'feature_08', 'feature_09', 'feature_10', 'feature_11', 'feature_12', 'feature_13', 'feature_14', 'feature_15', 'feature_16', 'feature_17', 'feature_18', 'feature_19', 'feature_20', 'feature_21', 'feature_22', 'feature_23', 'feature_24', 'feature_25', 'feature_26', 'feature_27', 'feature_28', 'feature_29', 'feature_30', 'feature_31', 'feature_32', 'feature_33', 'feature_34', 'feature_35', 'feature_36', 'feature_37', 'feature_38', 'feature_39', 'feature_40', 'feature_41', 'feature_42', 'feature_43', 'feature_44', 'feature_45', 'feature_46', 'feature_47', 'feature_48', 'feature_49', 'feature_50', 'feature_51', 'feature_52', 'feature_53', 'feature_54', 'feature_55', 'feature_56', 'feature_57', 'feature_58', 'feature_59', 'feature_60', 'feature_61', 'feature_62', 'feature_63', 'feature_64', 'feature_65', 'feature_66', 'feature_67', 'feature_68', 'feature_69', 'feature_70', 'feature_71', 'feature_72', 'feature_73', 'feature_74', 'feature_75', 'feature_76', 'feature_77', 'feature_78', 'responder_6_lag_1']\n"
     ]
    }
   ],
   "source": [
    "PATH = os.getcwd() + '/input/'\n",
    "\n",
    "METAS = ['date_id', 'time_id', 'symbol_id', 'weight']\n",
    "FEATURES = [f'feature_{i:02}' for i in range(79)]\n",
    "RESPONDERS = [f'responder_{i}' for i in range(9)]\n",
    "RESPONDERS_LAGS = [f'responder_{i}_lag_1' for i in range(9)]\n",
    "TARGET = 'responder_6'\n",
    "FEATURES_AND_LAGS=True\n",
    "ONLY_TARGET_LAGS=True\n",
    "if FEATURES_AND_LAGS and not ONLY_TARGET_LAGS:\n",
    "    FEATURES_WORKING = FEATURES + RESPONDERS + RESPONDERS_LAGS\n",
    "    FEATURES_WORKING.remove(TARGET)\n",
    "elif FEATURES_AND_LAGS and ONLY_TARGET_LAGS:\n",
    "    FEATURES_WORKING = FEATURES + ['responder_6_lag_1']\n",
    "else:\n",
    "    FEATURES_WORKING = FEATURES\n",
    "print(FEATURES_WORKING)\n",
    "\n",
    "USEFUL_COLS = FEATURES_WORKING + [TARGET] + ['weight']\n",
    "\n",
    "SEED = 728"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        # Ensure deterministic behavior (may impact performance)\n",
    "        #torch.backends.cudnn.deterministic = False\n",
    "        #torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def lazy_load(par_path):\n",
    "    return pl.scan_parquet(par_path).select(\n",
    "        pl.int_range(pl.len(), dtype=pl.UInt64).alias(\"index\"),\n",
    "        pl.all()\n",
    "    )\n",
    "seed_everything(SEED) # just do it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the purged group time series split, code is copied from somewhere\n",
    "# TODO: make GitHub GIST\n",
    "\n",
    "# TODO: add as dataset\n",
    "# TODO: add logging with verbose\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection._split import _BaseKFold, indexable, _num_samples\n",
    "from sklearn.utils.validation import _deprecate_positional_args\n",
    "\n",
    "# modified code for group gaps; source\n",
    "# https://github.com/getgaurav2/scikit-learn/blob/d4a3af5cc9da3a76f0266932644b884c99724c57/sklearn/model_selection/_split.py#L2243\n",
    "class PurgedGroupTimeSeriesSplit(_BaseKFold):\n",
    "    \"\"\"Time Series cross-validator variant with non-overlapping groups.|\n",
    "    Allows for a gap in groups to avoid potentially leaking info from\n",
    "    train into test if the model has windowed or lag features.\n",
    "    Provides train/test indices to split time series data samples\n",
    "    that are observed at fixed time intervals according to a\n",
    "    third-party provided group.\n",
    "    In each split, test indices must be higher than before, and thus shuffling\n",
    "    in cross validator is inappropriate.\n",
    "    This cross-validation object is a variation of :class:`KFold`.\n",
    "    In the kth split, it returns first k folds as train set and the\n",
    "    (k+1)th fold as test set.\n",
    "    The same group will not appear in two different folds (the number of\n",
    "    distinct groups has to be at least equal to the number of folds).\n",
    "    Note that unlike standard cross-validation methods, successive\n",
    "    training sets are supersets of those that come before them.\n",
    "    Read more in the :ref:`User Guide <cross_validation>`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_splits : int, default=5\n",
    "        Number of splits. Must be at least 2.\n",
    "    max_train_group_size : int, default=Inf\n",
    "        Maximum group size for a single training set.\n",
    "    group_gap : int, default=None\n",
    "        Gap between train and test\n",
    "    max_test_group_size : int, default=Inf\n",
    "        We discard this number of groups from the end of each train split\n",
    "    \"\"\"\n",
    "\n",
    "    @_deprecate_positional_args\n",
    "    def __init__(self,\n",
    "                 n_splits=5,\n",
    "                 *,\n",
    "                 max_train_group_size=np.inf,\n",
    "                 max_test_group_size=np.inf,\n",
    "                 group_gap=None,\n",
    "                 verbose=False\n",
    "                 ):\n",
    "        super().__init__(n_splits, shuffle=False, random_state=None)\n",
    "        self.max_train_group_size = max_train_group_size\n",
    "        self.group_gap = group_gap\n",
    "        self.max_test_group_size = max_test_group_size\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        \"\"\"Generate indices to split data into training and test set.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Training data, where n_samples is the number of samples\n",
    "            and n_features is the number of features.\n",
    "        y : array-like of shape (n_samples,)\n",
    "            Always ignored, exists for compatibility.\n",
    "        groups : array-like of shape (n_samples,)\n",
    "            Group labels for the samples used while splitting the dataset into\n",
    "            train/test set.\n",
    "        Yields\n",
    "        ------\n",
    "        train : ndarray\n",
    "            The training set indices for that split.\n",
    "        test : ndarray\n",
    "            The testing set indices for that split.\n",
    "        \"\"\"\n",
    "        if groups is None:\n",
    "            raise ValueError(\n",
    "                \"The 'groups' parameter should not be None\")\n",
    "        X, y, groups = indexable(X, y, groups)\n",
    "        n_samples = _num_samples(X)\n",
    "        n_splits = self.n_splits\n",
    "        group_gap = self.group_gap\n",
    "        max_test_group_size = self.max_test_group_size\n",
    "        max_train_group_size = self.max_train_group_size\n",
    "        n_folds = n_splits + 1\n",
    "        group_dict = {}\n",
    "        u, ind = np.unique(groups, return_index=True)\n",
    "        unique_groups = u[np.argsort(ind)]\n",
    "        n_samples = _num_samples(X)\n",
    "        n_groups = _num_samples(unique_groups)\n",
    "        for idx in np.arange(n_samples):\n",
    "            if (groups[idx] in group_dict):\n",
    "                group_dict[groups[idx]].append(idx)\n",
    "            else:\n",
    "                group_dict[groups[idx]] = [idx]\n",
    "        if n_folds > n_groups:\n",
    "            raise ValueError(\n",
    "                (\"Cannot have number of folds={0} greater than\"\n",
    "                 \" the number of groups={1}\").format(n_folds,\n",
    "                                                     n_groups))\n",
    "\n",
    "        group_test_size = min(n_groups // n_folds, max_test_group_size)\n",
    "        group_test_starts = range(n_groups - n_splits * group_test_size,\n",
    "                                  n_groups, group_test_size)\n",
    "        for group_test_start in group_test_starts:\n",
    "            train_array = []\n",
    "            test_array = []\n",
    "\n",
    "            group_st = max(0, group_test_start - group_gap - max_train_group_size)\n",
    "            for train_group_idx in unique_groups[group_st:(group_test_start - group_gap)]:\n",
    "                train_array_tmp = group_dict[train_group_idx]\n",
    "                \n",
    "                train_array = np.sort(np.unique(\n",
    "                                      np.concatenate((train_array,\n",
    "                                                      train_array_tmp)),\n",
    "                                      axis=None), axis=None)\n",
    "\n",
    "            train_end = train_array.size\n",
    " \n",
    "            for test_group_idx in unique_groups[group_test_start:\n",
    "                                                group_test_start +\n",
    "                                                group_test_size]:\n",
    "                test_array_tmp = group_dict[test_group_idx]\n",
    "                test_array = np.sort(np.unique(\n",
    "                                              np.concatenate((test_array,\n",
    "                                                              test_array_tmp)),\n",
    "                                     axis=None), axis=None)\n",
    "\n",
    "            test_array  = test_array[group_gap:]\n",
    "            \n",
    "            \n",
    "            if self.verbose > 0:\n",
    "                    pass\n",
    "                    \n",
    "            yield [int(i) for i in train_array], [int(i) for i in test_array]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# from matplotlib.colors import ListedColormap\n",
    "    \n",
    "# # this is code slightly modified from the sklearn docs here:\n",
    "# # https://scikit-learn.org/stable/auto_examples/model_selection/plot_cv_indices.html#sphx-glr-auto-examples-model-selection-plot-cv-indices-py\n",
    "# def plot_cv_indices(cv, X, y, group, ax, n_splits, lw=10):\n",
    "#     \"\"\"Create a sample plot for indices of a cross-validation object.\"\"\"\n",
    "    \n",
    "#     cmap_cv = plt.cm.coolwarm\n",
    "\n",
    "#     jet = plt.cm.get_cmap('jet', 256)\n",
    "#     seq = np.linspace(0, 1, 256)\n",
    "#     _ = np.random.shuffle(seq)   # inplace\n",
    "#     cmap_data = ListedColormap(jet(seq))\n",
    "\n",
    "#     # Generate the training/testing visualizations for each CV split\n",
    "#     for ii, (tr, tt) in enumerate(cv.split(X=X, y=y, groups=group)):\n",
    "#         # Fill in indices with the training/test groups\n",
    "#         indices = np.array([np.nan] * len(X))\n",
    "#         indices[tt] = 1\n",
    "#         indices[tr] = 0\n",
    "\n",
    "#         # Visualize the results\n",
    "#         ax.scatter(range(len(indices)), [ii + .5] * len(indices),\n",
    "#                    c=indices, marker='_', lw=lw, cmap=cmap_cv,\n",
    "#                    vmin=-.2, vmax=1.2)\n",
    "\n",
    "#     # Plot the data classes and groups at the end\n",
    "#     ax.scatter(range(len(X)), [ii + 1.5] * len(X),\n",
    "#                c=y, marker='_', lw=lw, cmap=plt.cm.Set3)\n",
    "\n",
    "#     ax.scatter(range(len(X)), [ii + 2.5] * len(X),\n",
    "#                c=group, marker='_', lw=lw, cmap=cmap_data)\n",
    "\n",
    "#     # Formatting\n",
    "#     yticklabels = list(range(n_splits)) + ['target', 'day']\n",
    "#     ax.set(yticks=np.arange(n_splits+2) + .5, yticklabels=yticklabels,\n",
    "#            xlabel='Sample index', ylabel=\"CV iteration\",\n",
    "#            ylim=[n_splits+2.2, -.2], xlim=[0, len(y)])\n",
    "#     ax.set_title('{}'.format(type(cv).__name__), fontsize=15)\n",
    "#     return ax\n",
    "\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# #plot_cv_indices(cv, X_train, y_train, groups, ax, 5, lw=20)\n",
    "# plot_cv_indices(\n",
    "#     cv,\n",
    "#     train[features].values,\n",
    "#     train['responder_6'].values,\n",
    "#     train['date_id'].values,\n",
    "#     ax,\n",
    "#     5,\n",
    "#     lw=20\n",
    "# )\n",
    "# rubbish=gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# code structure copied/inspired from https://www.kaggle.com/code/shiyili/js2024-rmf-mlp-inference-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "### Define various helper functions including r2 score\n",
    "writer = SummaryWriter()\n",
    "# get cpu, gpu or mps device for training\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "def reset_weights(m):\n",
    "    '''\n",
    "    Try resetting model weights to avoid\n",
    "    weight leakage.\n",
    "    '''\n",
    "    for layer in m.children():\n",
    "        if hasattr(layer, 'reset_parameters'):\n",
    "            print(f'Reset trainable parameters of layer = {layer}')\n",
    "            layer.reset_parameters()\n",
    "\n",
    "# for monitoring layer weights\n",
    "def weight_histograms(writer, step, model):\n",
    "    print(\"Visualizing model weights...\")\n",
    "    # Iterate over all model layers\n",
    "    for layer_number in range(len(model.layers)):\n",
    "        layer = model.layers[layer_number]\n",
    "        try:\n",
    "            weights = layer.weight\n",
    "            flattened_weights = weights.flatten()\n",
    "            tag = f\"layer_{layer_number}\"\n",
    "            writer.add_histogram(tag, flattened_weights, global_step=step, bins='tensorflow')\n",
    "        except AttributeError:\n",
    "            return\n",
    "            \n",
    "# loss is as defined on competition homepage\n",
    "# score will be = 1 - loss\n",
    "def r2_loss(outputs, targets, weights):\n",
    "    loss = torch.sum(weights*(targets - outputs)**2) / (torch.sum(weights*targets**2)+1e-38)\n",
    "    return loss\n",
    "\n",
    "# \"standard\" loss function\n",
    "test_loss_function= nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic LSTM model\n",
    "# class LSTM(nn.Module):\n",
    "#     def __init__(self, input_dim, output_dim, hidden_dim):\n",
    "#         super().__init__()\n",
    "#         self.input_dim = input_dim\n",
    "#         self.hidden_dim = hidden_dim\n",
    "#         self.output_dim = output_dim\n",
    "\n",
    "#         self.lstm = nn.LSTM(input_size=self.input_dim, hidden_size=self.hidden_dim, num_layers=1, batch_first=True)\n",
    "#         self.linear = nn.Linear(self.hidden_dim, self.output_dim)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         '''Forward pass'''\n",
    "#         x, _ = self.lstm(x)\n",
    "#         x = self.linear(x)\n",
    "#         return x.squeeze()\n",
    "# basic NN model\n",
    "class MLP(nn.Module):\n",
    "    '''\n",
    "    Simple Neural Network/MultiLayer Perceptron\n",
    "    '''\n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout_rates):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_rates = dropout_rates\n",
    "        current_size = input_size\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(nn.BatchNorm1d(current_size))\n",
    "        self.layers.append(nn.Dropout(dropout_rates[0]))\n",
    "        for i in range(len(hidden_size)):\n",
    "            self.layers.append(nn.Linear(current_size, self.hidden_size[i]))\n",
    "            self.layers.append(nn.BatchNorm1d(self.hidden_size[i]))\n",
    "            self.layers.append(nn.SiLU())\n",
    "            self.layers.append(nn.Dropout(self.dropout_rates[i+1]))\n",
    "            current_size = self.hidden_size[i]\n",
    "        self.layers.append(nn.Linear(current_size, self.output_size))\n",
    "\n",
    "        self.seq = nn.Sequential(*self.layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''Forward pass'''\n",
    "        return self.seq(x)\n",
    "\n",
    "# copied LSTM equivalent model\n",
    "class LSTMEquavalentMLP(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(LSTMEquavalentMLP, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.input_gate = nn.Linear(input_size, hidden_size)\n",
    "        self.candidate_gate = nn.Linear(input_size, hidden_size)\n",
    "        self.output_gate = nn.Linear(input_size, hidden_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        i_t = torch.sigmoid(self.input_gate(x)) # input gate\n",
    "        c_t = torch.tanh(self.candidate_gate(x)) # candidate gate\n",
    "        o_t = torch.sigmoid(self.output_gate(x)) # output gate\n",
    "        \n",
    "        h_t = o_t * torch.tanh(c_t * i_t)\n",
    "        \n",
    "        return h_t\n",
    "    \n",
    "class LSTMFeedForward(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout_rates=0):\n",
    "        super(LSTMFeedForward, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.lstm = LSTMEquavalentMLP(input_size, hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h_t = self.lstm(x)\n",
    "        y = self.fc(h_t).squeeze()\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training function\n",
    "def train(model, trainloader, optimizer, num_epochs, writer, gradclip=1.0, SAVE=True):\n",
    "    print_loss_every = 3000\n",
    "    model.train()\n",
    "    batch_train_tally = 0\n",
    "    # Run the training loop for defined number of epochs\n",
    "    for epoch in range(0, num_epochs):\n",
    "        # Visualize weight histograms\n",
    "        #weight_histograms(writer, epoch, model)\n",
    "        # Print epoch\n",
    "        print(f'Starting epoch {epoch+1}')\n",
    "        \n",
    "        # Set current loss value\n",
    "        current_loss = 0.0\n",
    "        # Iterate over Dataloader for training data\n",
    "        for i, data in enumerate(trainloader):\n",
    "            model.half()\n",
    "            # Get inputs\n",
    "            inputs, targets, weights = data\n",
    "            # GPU must see the data and the model\n",
    "            inputs = inputs.to(device) \n",
    "            targets = targets.to(device)\n",
    "            weights = weights.to(device)\n",
    "            \n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Perform forward pass\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = r2_loss(outputs, targets, weights)\n",
    "            writer.add_scalar(\"Loss/train/minibatches\", loss, batch_train_tally)\n",
    "            batch_train_tally += 1\n",
    "            current_loss += loss.item()\n",
    "            if i % print_loss_every == 0:\n",
    "                print('Loss after mini-batch %5d: %.3f' %\n",
    "                      (i + 1, current_loss / print_loss_every))\n",
    "                current_loss = 0.0\n",
    "            # Perform backward pass\n",
    "            loss.backward()\n",
    "        \n",
    "            for name, param in model.named_parameters():\n",
    "                if param.grad is not None:\n",
    "                    if torch.isnan(param.grad).any():\n",
    "                        print(f\"NaN found in the gradient of {name}\")\n",
    "                        \n",
    "            # gradient clipping to try to stop explosions\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), gradclip)\n",
    "            model.float() # update weights using regular precision (else you get nans when working with half data)\n",
    "            # Update weights\n",
    "            optimizer.step()\n",
    "\n",
    "        # Finished iterating over Dataloader\n",
    "        writer.add_scalar(\"Loss/train/epoch\", loss, epoch)\n",
    "        writer.flush()\n",
    "\n",
    "        # save after every epoch\n",
    "        if SAVE:\n",
    "            save_path = f'./models/lstm_e{epoch}.pth'\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "      \n",
    "    # Saving the final model\n",
    "    if SAVE:\n",
    "        save_path = f'./models/working_lstm_f{fold}.pth'\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "    return\n",
    "\n",
    "# testing function\n",
    "def test(model, testloader, writer):\n",
    "    # Evaluation for this fold\n",
    "    batch_test_tally = 0\n",
    "    R2_score = 0.0\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        # Iterate over the test data and generate predictions\n",
    "        for i, data in enumerate(testloader):\n",
    "            model.half()\n",
    "            # Get inputs\n",
    "            inputs, targets, weights = data\n",
    "            inputs = inputs.to(device) # GPU must see the data and the model\n",
    "            targets = targets.to(device)\n",
    "            weights = weights.to(device)\n",
    "        \n",
    "            # Generate outputs\n",
    "            outputs = model(inputs)\n",
    "            loss = r2_loss(outputs, targets, weights)\n",
    "            if i%1000==0:\n",
    "                print(f'i={i}, loss={loss}')\n",
    "            writer.add_scalar('Loss/test/minibatches', loss, batch_test_tally)\n",
    "            batch_test_tally += 1\n",
    "            total_loss += loss\n",
    "        return total_loss\n",
    "\n",
    "# training function\n",
    "def train_and_test(model, trainloader, testloader, optimizer, num_epochs, writer, gradclip=1.0, SAVE=True, model_type='type'):\n",
    "    print_loss_every = 3000\n",
    "    val_every = 1\n",
    "    batch_train_tally = 0\n",
    "    # Run the training loop for defined number of epochs\n",
    "    for epoch in range(0, num_epochs):\n",
    "        model.train()\n",
    "        # Visualize weight histograms\n",
    "        #weight_histograms(writer, epoch, model)\n",
    "        # Print epoch\n",
    "        print(f'Starting epoch {epoch+1}')\n",
    "        \n",
    "        # Set current loss value\n",
    "        current_loss = 0.0\n",
    "        # Iterate over Dataloader for training data\n",
    "        for i, data in enumerate(trainloader):\n",
    "            model.half()\n",
    "            # Get inputs\n",
    "            inputs, targets, weights = data\n",
    "            # GPU must see the data and the model\n",
    "            inputs = inputs.to(device) \n",
    "            targets = targets.to(device)\n",
    "            weights = weights.to(device)\n",
    "            \n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Perform forward pass\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = r2_loss(outputs, targets, weights)\n",
    "            writer.add_scalar(\"Loss/train/minibatches\", loss, batch_train_tally)\n",
    "            batch_train_tally += 1\n",
    "            current_loss += loss.item()\n",
    "            if i % print_loss_every == 0:\n",
    "                print('Loss after mini-batch %5d: %.3f' %\n",
    "                      (i + 1, current_loss / print_loss_every))\n",
    "                current_loss = 0.0\n",
    "            # Perform backward pass\n",
    "            loss.backward()\n",
    "        \n",
    "            for name, param in model.named_parameters():\n",
    "                if param.grad is not None:\n",
    "                    if torch.isnan(param.grad).any():\n",
    "                        print(f\"NaN found in the gradient of {name}\")\n",
    "                        \n",
    "            # gradient clipping to try to stop explosions\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), gradclip)\n",
    "            model.float() # update weights using regular precision (else you get nans when working with half data)\n",
    "            # Update weights\n",
    "            optimizer.step()\n",
    "\n",
    "        # Finished iterating over Dataloader\n",
    "        writer.add_scalar(\"Loss/train/epoch\", loss, epoch)\n",
    "        writer.flush()\n",
    "\n",
    "        # save after every epoch\n",
    "        if SAVE:\n",
    "            save_path = f'./models/{model_type}_e{epoch}.pth'\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "\n",
    "\n",
    "        # validate after every epoch\n",
    "        if epoch % val_every == 0:\n",
    "            # Evaluation for this epoch\n",
    "            batch_test_tally = 0\n",
    "            R2_score = 0.0\n",
    "            total_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                # Iterate over the test data and generate predictions\n",
    "                for i, data in enumerate(testloader):\n",
    "                    model.half()\n",
    "                    # Get inputs\n",
    "                    inputs, targets, weights = data\n",
    "                    inputs = inputs.to(device) # GPU must see the data and the model\n",
    "                    targets = targets.to(device)\n",
    "                    weights = weights.to(device)\n",
    "                \n",
    "                    # Generate outputs\n",
    "                    outputs = model(inputs)\n",
    "                    loss = r2_loss(outputs, targets, weights)\n",
    "                    # if i%1000==0:\n",
    "                    #     print(f'i={i}, loss={loss}')\n",
    "                    writer.add_scalar('Loss/test/minibatches', loss, batch_test_tally)\n",
    "                    batch_test_tally += 1\n",
    "                    total_loss += loss\n",
    "                    num_batches = len(testloader)\n",
    "                    results = total_loss / num_batches\n",
    "                    writer.add_scalar('Loss/test/epoch', results, epoch)\n",
    "                    writer.flush()\n",
    "                print(f'Validation score after {epoch} epochs={results}')\n",
    "                #return total_loss\n",
    "      \n",
    "    # Saving the final model\n",
    "    if SAVE:\n",
    "        save_path = f'./models/working_{model_type}_f{fold}.pth'\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading trainset...\n",
      "Loaded.\n"
     ]
    }
   ],
   "source": [
    "print('Loading trainset...')\n",
    "TEST=False\n",
    "if TEST:\n",
    "    trainset = torch.load(f'./input/trainsets/working_trainset_TEST.pt')\n",
    "else:\n",
    "    trainset = torch.load(f'./input/trainsets/working_trainset.pt')\n",
    "print('Loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading validset...\n",
      "Loaded.\n"
     ]
    }
   ],
   "source": [
    "print('Loading validset...')\n",
    "if TEST:\n",
    "    validset = torch.load(f'./input/validsets/working_validset_TEST.pt')\n",
    "else:\n",
    "    validset = torch.load(f'./input/validsets/working_validset.pt')\n",
    "print('Loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_FF_CONFIG = {\n",
    "    'input_size':  len(trainset[-1][0]),\n",
    "    'hidden_size': 128,\n",
    "    'output_size': 1,\n",
    "    'dropout_rates': 0\n",
    "}\n",
    "\n",
    "MLP_CONFIG = {\n",
    "    'input_size': len(trainset[-1][0]),\n",
    "    'hidden_size': [128, 256, 256, 128, 64],\n",
    "    'output_size': 1,\n",
    "    'dropout_rates': [0.1, 0.1, 0.1, 0.1,0.1,0.1]\n",
    "}\n",
    "\n",
    "TRAINING_CONFIG = {\n",
    "    'num_epochs': 10,\n",
    "    'learning_rate': 1e-4,\n",
    "    'grad_clip':1.0,\n",
    "    'test_loss': False\n",
    "}\n",
    "\n",
    "fold_function = 'PurgedGroupTimeSeries'\n",
    "# fold_function = 'KFold'\n",
    "fold_function = 'None'\n",
    "\n",
    "# choose a model\n",
    "model_type = 'LSTM_FF' # 'LSTM_FF' or 'NN'\n",
    "# For holding fold results\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data loaders\n",
    "BATCH_SIZE = 2048\n",
    "NUM_WORKERS = 4 # num of parallel subprocesses for data loading (CPU task)\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers = NUM_WORKERS\n",
    ")\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    validset,\n",
    "    batch_size=1,\n",
    "    num_workers = NUM_WORKERS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started on 2025-01-11 04:47:49.340231\n",
      "FOLD 0\n",
      "train_ids: [0, 0]\n",
      "test_ids: [0, 0]\n",
      "--------------------------------\n",
      "Reset trainable parameters of layer = Linear(in_features=80, out_features=128, bias=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=80, out_features=128, bias=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=80, out_features=128, bias=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=128, out_features=1, bias=True)\n",
      "Starting epoch 1\n",
      "Loss after mini-batch     1: 0.000\n",
      "Loss after mini-batch  3001: 0.997\n",
      "Loss after mini-batch  6001: 0.996\n",
      "Loss after mini-batch  9001: 0.995\n",
      "Loss after mini-batch 12001: 0.993\n",
      "Loss after mini-batch 15001: 0.992\n",
      "Loss after mini-batch 18001: 0.988\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 35\u001b[0m\n\u001b[1;32m     33\u001b[0m BATCH_TRAIN_TALLY \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;66;03m# global var lmao\u001b[39;00m\n\u001b[1;32m     34\u001b[0m BATCH_TEST_TALLY \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 35\u001b[0m \u001b[43mtrain_and_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtestloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTRAINING_CONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnum_epochs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradclip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTRAINING_CONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_clip\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSAVE\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Process is complete.\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining process has finished\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[42], line 182\u001b[0m, in \u001b[0;36mtrain_and_test\u001b[0;34m(model, trainloader, testloader, optimizer, num_epochs, writer, gradclip, SAVE, model_type)\u001b[0m\n\u001b[1;32m    179\u001b[0m loss \u001b[38;5;241m=\u001b[39m r2_loss(outputs, targets, weights)\n\u001b[1;32m    180\u001b[0m \u001b[38;5;66;03m# if i%1000==0:\u001b[39;00m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;66;03m#     print(f'i={i}, loss={loss}')\u001b[39;00m\n\u001b[0;32m--> 182\u001b[0m \u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_scalar\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mLoss/test/minibatches\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_test_tally\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    183\u001b[0m batch_test_tally \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    184\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/tensorboard/writer.py:381\u001b[0m, in \u001b[0;36mSummaryWriter.add_scalar\u001b[0;34m(self, tag, scalar_value, global_step, walltime, new_style, double_precision)\u001b[0m\n\u001b[1;32m    376\u001b[0m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_log_api_usage_once(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensorboard.logging.add_scalar\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    378\u001b[0m summary \u001b[38;5;241m=\u001b[39m scalar(\n\u001b[1;32m    379\u001b[0m     tag, scalar_value, new_style\u001b[38;5;241m=\u001b[39mnew_style, double_precision\u001b[38;5;241m=\u001b[39mdouble_precision\n\u001b[1;32m    380\u001b[0m )\n\u001b[0;32m--> 381\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_file_writer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_summary\u001b[49m\u001b[43m(\u001b[49m\u001b[43msummary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglobal_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwalltime\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/tensorboard/writer.py:115\u001b[0m, in \u001b[0;36mFileWriter.add_summary\u001b[0;34m(self, summary, global_step, walltime)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Add a `Summary` protocol buffer to the event file.\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03mThis method wraps the provided summary in an `Event` protocol buffer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    walltime (from time.time()) seconds after epoch\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    114\u001b[0m event \u001b[38;5;241m=\u001b[39m event_pb2\u001b[38;5;241m.\u001b[39mEvent(summary\u001b[38;5;241m=\u001b[39msummary)\n\u001b[0;32m--> 115\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglobal_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwalltime\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/tensorboard/writer.py:98\u001b[0m, in \u001b[0;36mFileWriter.add_event\u001b[0;34m(self, event, step, walltime)\u001b[0m\n\u001b[1;32m     94\u001b[0m event\u001b[38;5;241m.\u001b[39mwall_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;28;01mif\u001b[39;00m walltime \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m walltime\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;66;03m# Make sure step is converted from numpy or other formats\u001b[39;00m\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;66;03m# since protobuf might not convert depending on version\u001b[39;00m\n\u001b[0;32m---> 98\u001b[0m     event\u001b[38;5;241m.\u001b[39mstep \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent_writer\u001b[38;5;241m.\u001b[39madd_event(event)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "now = datetime.datetime.now()\n",
    "print(f'Started on {now}')\n",
    "### Train and cross validation loop\n",
    "folds = [([0,0],[0,0])]\n",
    "for fold, (train_ids, test_ids) in enumerate(folds):\n",
    "    # Print\n",
    "    print(f'FOLD {fold}')\n",
    "    print(f'train_ids: [{train_ids[0]}, {train_ids[-1]}]')\n",
    "    print(f'test_ids: [{test_ids[0]}, {test_ids[-1]}]')\n",
    "    if fold_function != 'None':\n",
    "        print(\"Using a fold function...\")\n",
    "        # define train/test sets\n",
    "        X_tr, y_tr, w_tr = X[train_ids], y[train_ids], w[train_ids]\n",
    "        #X_te, y_te, w_te = X[test_ids], y[test_ids], w[test_ids]\n",
    "    \n",
    "    print('--------------------------------')\n",
    "\n",
    "    # Initialize Dataset objects to make PyTorch play nice\n",
    "    #trainset = TensorDataset(torch.tensor(X_tr).to(torch.float32), torch.tensor(y_tr), torch.tensor(w_tr))\n",
    "    #testset = TensorDataset(torch.tensor(X_te).to(torch.float32), torch.tensor(y_te), torch.tensor(w_te))\n",
    "    #trainset = TensorDataset(X_tr, y_tr, w_tr)\n",
    "    #testset = TensorDataset(X_te, y_te, w_te)\n",
    "\n",
    "    # Init the model\n",
    "    if model_type == 'NN':\n",
    "        model = MLP(**MLP_CONFIG).to(device)\n",
    "    elif model_type == 'LSTM_FF' :\n",
    "        model = LSTMFeedForward(**LSTM_FF_CONFIG).to(device)\n",
    "    model.apply(reset_weights) # not sure if I need this but w/e\n",
    "    # Initialize optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=TRAINING_CONFIG['learning_rate'])\n",
    "    BATCH_TRAIN_TALLY = 0 # global var lmao\n",
    "    BATCH_TEST_TALLY = 0\n",
    "    train_and_test(model, trainloader, testloader, optimizer, TRAINING_CONFIG[\"num_epochs\"], writer, gradclip=TRAINING_CONFIG[\"grad_clip\"], SAVE=False, model_type=model_type)\n",
    "    # Process is complete.\n",
    "    print('Training process has finished')\n",
    "    print('Deleting the training data variables...')\n",
    "    del trainset, trainloader\n",
    "    gc.collect()\n",
    "    \n",
    "writer.close()\n",
    "rubbish=gc.collect()\n",
    "now = datetime.datetime.now()\n",
    "print(f'Ended on {now}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "now = datetime.datetime.now()\n",
    "print(f'Started on {now}')\n",
    "### Train and cross validation loop\n",
    "folds = [([0,0],[0,0])]\n",
    "for fold, (train_ids, test_ids) in enumerate(folds):\n",
    "    # Print\n",
    "    print(f'FOLD {fold}')\n",
    "    print(f'train_ids: [{train_ids[0]}, {train_ids[-1]}]')\n",
    "    print(f'test_ids: [{test_ids[0]}, {test_ids[-1]}]')\n",
    "    if fold_function != 'None':\n",
    "        print(\"Using a fold function...\")\n",
    "        # define train/test sets\n",
    "        X_tr, y_tr, w_tr = X[train_ids], y[train_ids], w[train_ids]\n",
    "        #X_te, y_te, w_te = X[test_ids], y[test_ids], w[test_ids]\n",
    "    \n",
    "    print('--------------------------------')\n",
    "\n",
    "    # Initialize Dataset objects to make PyTorch play nice\n",
    "    #trainset = TensorDataset(torch.tensor(X_tr).to(torch.float32), torch.tensor(y_tr), torch.tensor(w_tr))\n",
    "    #testset = TensorDataset(torch.tensor(X_te).to(torch.float32), torch.tensor(y_te), torch.tensor(w_te))\n",
    "    #trainset = TensorDataset(X_tr, y_tr, w_tr)\n",
    "    #testset = TensorDataset(X_te, y_te, w_te)\n",
    "\n",
    "    # Init the model\n",
    "    model = LSTMFeedForward(**LSTM_FF_CONFIG).to(device)\n",
    "    model.apply(reset_weights) # not sure if I need this but w/e\n",
    "    # Initialize optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=TRAINING_CONFIG['learning_rate'])\n",
    "    BATCH_TRAIN_TALLY = 0 # global var lmao\n",
    "    train(model, trainloader, optimizer, TRAINING_CONFIG[\"num_epochs\"], writer, gradclip=TRAINING_CONFIG[\"grad_clip\"], SAVE=True)\n",
    "    # Process is complete.\n",
    "    print('Training process has finished')\n",
    "    print('Deleting the training data variables...')\n",
    "    del trainset, trainloader\n",
    "    gc.collect()\n",
    "    \n",
    "    # Print about testing\n",
    "    print('Starting testing')\n",
    "    print('Loading validset...')\n",
    "    if TEST:\n",
    "        validset = torch.load(f'./input/validsets/working_validset_TEST.pt')\n",
    "    else:\n",
    "        validset = torch.load(f'./input/validsets/working_validset.pt')\n",
    "    print('Loaded.')\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "        validset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_workers = NUM_WORKERS\n",
    "    )\n",
    "    \n",
    "    BATCH_TEST_TALLY = 0 # global var lmao\n",
    "    total_loss = test(model, testloader, writer)\n",
    "    num_batches = len(testloader)\n",
    "    results[fold] = total_loss / num_batches\n",
    "    writer.add_scalar('Loss/test/fold', results[fold], fold)\n",
    "    writer.flush()\n",
    "    del validset, testloader\n",
    "\n",
    "    # Print fold results\n",
    "    print(f'CROSS VALIDATION RESULTS FOR {folds} FOLDS')\n",
    "    print('--------------------------------')\n",
    "    sum = 0.0\n",
    "    for key, value in results.items():\n",
    "        print(f'Fold {key}: {value}')\n",
    "        sum += value\n",
    "    print(f'Average: {sum/len(results.items())}')\n",
    "writer.close()\n",
    "rubbish=gc.collect()\n",
    "now = datetime.datetime.now()\n",
    "print(f'Ended on {now}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arbitrarily testing model after certain epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state_dict:\n",
      "lstm.input_gate.weight \t torch.Size([128, 80])\n",
      "lstm.input_gate.bias \t torch.Size([128])\n",
      "lstm.candidate_gate.weight \t torch.Size([128, 80])\n",
      "lstm.candidate_gate.bias \t torch.Size([128])\n",
      "lstm.output_gate.weight \t torch.Size([128, 80])\n",
      "lstm.output_gate.bias \t torch.Size([128])\n",
      "fc.weight \t torch.Size([1, 128])\n",
      "fc.bias \t torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "epochtest = 70\n",
    "model = torch.load(f'./models/lstm_e{epochtest}.pth')\n",
    "# Print model's state_dict\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model:\n",
    "    print(param_tensor, \"\\t\", model[param_tensor].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting testing\n",
      "Loading validset...\n",
      "Loaded.\n",
      "i=0, loss=1.0107421875\n",
      "i=1000, loss=0.923828125\n",
      "i=2000, loss=1.0\n",
      "i=3000, loss=0.951171875\n",
      "i=4000, loss=1.03515625\n",
      "i=5000, loss=0.978515625\n",
      "i=6000, loss=1.015625\n",
      "i=7000, loss=1.048828125\n",
      "i=8000, loss=1.0322265625\n",
      "i=9000, loss=1.046875\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'fold' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m test(loadedmodel, testloader, writer)\n\u001b[1;32m     21\u001b[0m num_batches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(testloader)\n\u001b[0;32m---> 22\u001b[0m results[\u001b[43mfold\u001b[49m] \u001b[38;5;241m=\u001b[39m total_loss \u001b[38;5;241m/\u001b[39m num_batches\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_loss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, num_batches=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_batches\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m#writer.add_scalar('Loss/test/fold', results[fold], fold)\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m#writer.flush()\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m#del validset, testloader\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Print fold results\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fold' is not defined"
     ]
    }
   ],
   "source": [
    "loadedmodel = LSTMFeedForward(**LSTM_FF_CONFIG)\n",
    "epochtest = 3\n",
    "loadedmodel.load_state_dict(torch.load(f'./models/lstm_e{epochtest}.pth', weights_only=True))\n",
    "loadedmodel.to(device)\n",
    "# Print about testing\n",
    "print('Starting testing')\n",
    "print('Loading validset...')\n",
    "if TEST:\n",
    "    validset = torch.load(f'./input/validsets/working_validset_TEST.pt')\n",
    "else:\n",
    "    validset = torch.load(f'./input/validsets/working_validset.pt')\n",
    "print('Loaded.')\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    validset,\n",
    "    batch_size=int(BATCH_SIZE/2),\n",
    "    num_workers = NUM_WORKERS\n",
    ")\n",
    "\n",
    "BATCH_TEST_TALLY = 0 # global var lmao\n",
    "total_loss = test(loadedmodel, testloader, writer)\n",
    "num_batches = len(testloader)\n",
    "results[fold] = total_loss / num_batches\n",
    "print(f'total_loss={total_loss}, num_batches={num_batches}')\n",
    "#writer.add_scalar('Loss/test/fold', results[fold], fold)\n",
    "#writer.flush()\n",
    "#del validset, testloader\n",
    "\n",
    "# # Print fold results\n",
    "# print(f'CROSS VALIDATION RESULTS FOR {folds} FOLDS')\n",
    "# print('--------------------------------')\n",
    "# valsum = 0.0\n",
    "# for key, value in results.items():\n",
    "#     print(f'Fold {key}: {value}')\n",
    "#     valsum += value\n",
    "# print(f'Average: {valsum/len(results.items())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1 = LSTMFeedForward(**LSTM_FF_CONFIG)\n",
    "epochtest = 1\n",
    "model1.load_state_dict(torch.load(f'./models/lstm_e{epochtest}.pth', weights_only=True))\n",
    "\n",
    "model2 = LSTMFeedForward(**LSTM_FF_CONFIG)\n",
    "epochtest = 11\n",
    "model2.load_state_dict(torch.load(f'./models/lstm_e{epochtest}.pth', weights_only=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(1.178, dtype=float16)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test = np.arange(0, len(FEATURES)+1, 1)\n",
    "model2.to(device)\n",
    "model2.half()\n",
    "with torch.no_grad():\n",
    "    model2.eval()\n",
    "    x_test = torch.from_numpy(x_test)\n",
    "    x_test = x_test.half().to(device)\n",
    "    predict = model2(x_test).cpu().numpy()\n",
    "\n",
    "predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(1.178, dtype=float16)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lstm.input_gate.weight \t torch.Size([128, 80])\n",
      "lstm.input_gate.bias \t torch.Size([128])\n",
      "lstm.candidate_gate.weight \t torch.Size([128, 80])\n",
      "lstm.candidate_gate.bias \t torch.Size([128])\n",
      "lstm.output_gate.weight \t torch.Size([128, 80])\n",
      "lstm.output_gate.bias \t torch.Size([128])\n",
      "fc.weight \t torch.Size([1, 128])\n",
      "fc.bias \t torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "for param_tensor in model1.state_dict():\n",
    "    print(param_tensor, \"\\t\", model1.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.state_dict()['fc.bias']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.state_dict()['fc.bias']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validset[0][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_func(x, models):\n",
    "    result_all = np.zeros((len(models), len(x)))\n",
    "    with torch.no_grad():\n",
    "        x = torch.tensor(x, dtype=torch.float).to(DEVICE)\n",
    "        for i, model in enumerate(models):    \n",
    "            predict = model(x).cpu().numpy()\n",
    "            result_all[i, :] = predict\n",
    "    return result_all.mean(axis=0)\n",
    "       \n",
    "def predict_func(x, model):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        x = torch.tensor(x).half().to(device)\n",
    "        result = model(x).cpu().numpy()\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JaneStreetPredictor:\n",
    "    \n",
    "    def __init__(self, model, test_parquet, lag_parquet, sequence_len, feature_cols, pbar_length=0):\n",
    "        # Initialize model and parameters\n",
    "        self.model = model\n",
    "        self.sequence_len = sequence_len\n",
    "        self.feature_cols = feature_cols\n",
    "        self.responder_cols = [f'responder_{i}' for i in range(9)]\n",
    "        \n",
    "        # Initialize parquet data for test and lag\n",
    "        self.test_parquet = test_parquet\n",
    "        self.lag_parquet = lag_parquet\n",
    "        \n",
    "        # Initialize global variables as class attributes\n",
    "        self.history_cache = {}\n",
    "        self.test_ = None\n",
    "        self.lags_ = None\n",
    "        self.time_step_count = 0\n",
    "        # setup pbar:\n",
    "        #self.pbar = tqdm(total=pbar_length, disable=(pbar_length == 0))\n",
    "\n",
    "    \n",
    "\n",
    "    def predict(self, test: pl.DataFrame, lags: pl.DataFrame | None) -> pl.DataFrame | pd.DataFrame:\n",
    "        \n",
    "        # Handle lags if provided\n",
    "        if lags is not None:\n",
    "            self.lags_ = lags.to_pandas()\n",
    "            self.lag_stats = self.lags_.groupby('symbol_id')[[f'{col}_lag_1' for col in self.responder_cols]].agg([\n",
    "                'mean', 'std', 'max', 'min', lambda x: pd.Series(x).skew(), lambda x: pd.Series(x).kurt(), \n",
    "            ]).rename(columns={'<lambda_0>': 'skew', '<lambda_1>': 'kurt'})\n",
    "            \n",
    "            self.lag_stats.columns =  [f\"{'_'.join(c1.split('_')[:-1])}_{c2}\" for (c1, c2) in self.lag_stats.columns ]\n",
    "\n",
    "        # Cache the test data for future use\n",
    "        self.test_ = test\n",
    "\n",
    "        test = test.to_pandas()\n",
    "        test = test.fillna(0.0)\n",
    "        test['responder_6_lag_1'] = self.lags_['responder_6_lag_1']\n",
    "        print(test['responder_6_lag_1'][0:4])\n",
    "        x_stack = test[self.feature_cols]\n",
    "        print(x_stack.head())\n",
    "        #x_stack = train_scaler.transform(test[self.feature_cols])\n",
    "        #test = pd.merge(test, self.lag_stats, on='symbol_id', how='left')\n",
    "        print(test.columns)\n",
    "        x_stack = x_stack.values\n",
    "       \n",
    "        # Call the model's prediction function\n",
    "        predicts = predict_func(x_stack, self.model)  # Placeholder for the actual model prediction logic\n",
    "\n",
    "        # Create a DataFrame to store the predictions\n",
    "        predictions = pl.DataFrame({'row_id': test['row_id'].values, 'responder_6': predicts})\n",
    "\n",
    "        # sanity check\n",
    "        assert isinstance(predictions, pl.DataFrame | pd.DataFrame)\n",
    "        assert list(predictions.columns) == ['row_id', 'responder_6']\n",
    "        assert len(predictions) == len(test)\n",
    "\n",
    "        # update time_step_count\n",
    "        self.time_step_count += 1\n",
    "        #self.pbar.update(1)\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    def run_inference_server(self):\n",
    "    \n",
    "        #self.pbar.refresh()\n",
    "\n",
    "        inference_server = js_server.JSInferenceServer(self.predict)\n",
    "        \n",
    "        if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "            inference_server.serve()\n",
    "        else:\n",
    "            inference_server.run_local_gateway((self.test_parquet, self.lag_parquet))\n",
    "\n",
    "        #self.pbar.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   -1.305746\n",
      "1   -1.162801\n",
      "2   -1.574290\n",
      "3    0.329152\n",
      "Name: responder_6_lag_1, dtype: float32\n",
      "   feature_00  feature_01  feature_02  feature_03  feature_04  feature_05  \\\n",
      "0         0.0         0.0         0.0         0.0         0.0        -0.0   \n",
      "1         0.0        -0.0         0.0         0.0         0.0        -0.0   \n",
      "2         0.0        -0.0         0.0         0.0         0.0        -0.0   \n",
      "3         0.0         0.0         0.0         0.0         0.0        -0.0   \n",
      "4         0.0        -0.0         0.0         0.0         0.0        -0.0   \n",
      "\n",
      "   feature_06  feature_07  feature_08  feature_09  ...  feature_70  \\\n",
      "0        -0.0        -0.0         0.0         0.0  ...        -0.0   \n",
      "1        -0.0        -0.0         0.0         0.0  ...        -0.0   \n",
      "2        -0.0        -0.0         0.0         0.0  ...        -0.0   \n",
      "3        -0.0        -0.0         0.0         0.0  ...        -0.0   \n",
      "4        -0.0        -0.0         0.0         0.0  ...        -0.0   \n",
      "\n",
      "   feature_71  feature_72  feature_73  feature_74  feature_75  feature_76  \\\n",
      "0         0.0         0.0         0.0         0.0         0.0         0.0   \n",
      "1         0.0        -0.0         0.0         0.0         0.0         0.0   \n",
      "2         0.0         0.0         0.0         0.0         0.0         0.0   \n",
      "3         0.0         0.0         0.0         0.0         0.0         0.0   \n",
      "4         0.0        -0.0         0.0         0.0         0.0         0.0   \n",
      "\n",
      "   feature_77  feature_78  responder_6_lag_1  \n",
      "0        -0.0        -0.0          -1.305746  \n",
      "1         0.0         0.0          -1.162801  \n",
      "2        -0.0        -0.0          -1.574290  \n",
      "3        -0.0        -0.0           0.329152  \n",
      "4         0.0         0.0           0.219856  \n",
      "\n",
      "[5 rows x 80 columns]\n",
      "Index(['row_id', 'date_id', 'time_id', 'symbol_id', 'weight', 'is_scored',\n",
      "       'feature_00', 'feature_01', 'feature_02', 'feature_03', 'feature_04',\n",
      "       'feature_05', 'feature_06', 'feature_07', 'feature_08', 'feature_09',\n",
      "       'feature_10', 'feature_11', 'feature_12', 'feature_13', 'feature_14',\n",
      "       'feature_15', 'feature_16', 'feature_17', 'feature_18', 'feature_19',\n",
      "       'feature_20', 'feature_21', 'feature_22', 'feature_23', 'feature_24',\n",
      "       'feature_25', 'feature_26', 'feature_27', 'feature_28', 'feature_29',\n",
      "       'feature_30', 'feature_31', 'feature_32', 'feature_33', 'feature_34',\n",
      "       'feature_35', 'feature_36', 'feature_37', 'feature_38', 'feature_39',\n",
      "       'feature_40', 'feature_41', 'feature_42', 'feature_43', 'feature_44',\n",
      "       'feature_45', 'feature_46', 'feature_47', 'feature_48', 'feature_49',\n",
      "       'feature_50', 'feature_51', 'feature_52', 'feature_53', 'feature_54',\n",
      "       'feature_55', 'feature_56', 'feature_57', 'feature_58', 'feature_59',\n",
      "       'feature_60', 'feature_61', 'feature_62', 'feature_63', 'feature_64',\n",
      "       'feature_65', 'feature_66', 'feature_67', 'feature_68', 'feature_69',\n",
      "       'feature_70', 'feature_71', 'feature_72', 'feature_73', 'feature_74',\n",
      "       'feature_75', 'feature_76', 'feature_77', 'feature_78',\n",
      "       'responder_6_lag_1'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "model_eval = LSTMFeedForward(**LSTM_FF_CONFIG)\n",
    "model_eval.load_state_dict(torch.load(f'./models/working_lstm_f0.pth', weights_only=True))\n",
    "model_eval.to(device)\n",
    "test_parquet = os.getcwd() + '/input/test.parquet'\n",
    "lags_parquet = os.getcwd() + '/input/lags.parquet'\n",
    "SEQUENCE_LEN=16\n",
    "js_predictor = JaneStreetPredictor(\n",
    "    model_eval,\n",
    "    test_parquet,\n",
    "    lags_parquet,\n",
    "    sequence_len= SEQUENCE_LEN,\n",
    "    feature_cols = FEATURES_WORKING,\n",
    ")\n",
    "\n",
    "js_predictor.run_inference_server()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   -1.305746\n",
      "1   -1.162801\n",
      "2   -1.574290\n",
      "3    0.329152\n",
      "Name: responder_6_lag_1, dtype: float32\n",
      "   feature_00  feature_01  feature_02  feature_03  feature_04  feature_05  \\\n",
      "0         0.0         0.0         0.0         0.0         0.0        -0.0   \n",
      "1         0.0        -0.0         0.0         0.0         0.0        -0.0   \n",
      "2         0.0        -0.0         0.0         0.0         0.0        -0.0   \n",
      "3         0.0         0.0         0.0         0.0         0.0        -0.0   \n",
      "4         0.0        -0.0         0.0         0.0         0.0        -0.0   \n",
      "\n",
      "   feature_06  feature_07  feature_08  feature_09  ...  feature_70  \\\n",
      "0        -0.0        -0.0         0.0         0.0  ...        -0.0   \n",
      "1        -0.0        -0.0         0.0         0.0  ...        -0.0   \n",
      "2        -0.0        -0.0         0.0         0.0  ...        -0.0   \n",
      "3        -0.0        -0.0         0.0         0.0  ...        -0.0   \n",
      "4        -0.0        -0.0         0.0         0.0  ...        -0.0   \n",
      "\n",
      "   feature_71  feature_72  feature_73  feature_74  feature_75  feature_76  \\\n",
      "0         0.0         0.0         0.0         0.0         0.0         0.0   \n",
      "1         0.0        -0.0         0.0         0.0         0.0         0.0   \n",
      "2         0.0         0.0         0.0         0.0         0.0         0.0   \n",
      "3         0.0         0.0         0.0         0.0         0.0         0.0   \n",
      "4         0.0        -0.0         0.0         0.0         0.0         0.0   \n",
      "\n",
      "   feature_77  feature_78  responder_6_lag_1  \n",
      "0        -0.0        -0.0          -1.305746  \n",
      "1         0.0         0.0          -1.162801  \n",
      "2        -0.0        -0.0          -1.574290  \n",
      "3        -0.0        -0.0           0.329152  \n",
      "4         0.0         0.0           0.219856  \n",
      "\n",
      "[5 rows x 80 columns]\n",
      "Index(['row_id', 'date_id', 'time_id', 'symbol_id', 'weight', 'is_scored',\n",
      "       'feature_00', 'feature_01', 'feature_02', 'feature_03', 'feature_04',\n",
      "       'feature_05', 'feature_06', 'feature_07', 'feature_08', 'feature_09',\n",
      "       'feature_10', 'feature_11', 'feature_12', 'feature_13', 'feature_14',\n",
      "       'feature_15', 'feature_16', 'feature_17', 'feature_18', 'feature_19',\n",
      "       'feature_20', 'feature_21', 'feature_22', 'feature_23', 'feature_24',\n",
      "       'feature_25', 'feature_26', 'feature_27', 'feature_28', 'feature_29',\n",
      "       'feature_30', 'feature_31', 'feature_32', 'feature_33', 'feature_34',\n",
      "       'feature_35', 'feature_36', 'feature_37', 'feature_38', 'feature_39',\n",
      "       'feature_40', 'feature_41', 'feature_42', 'feature_43', 'feature_44',\n",
      "       'feature_45', 'feature_46', 'feature_47', 'feature_48', 'feature_49',\n",
      "       'feature_50', 'feature_51', 'feature_52', 'feature_53', 'feature_54',\n",
      "       'feature_55', 'feature_56', 'feature_57', 'feature_58', 'feature_59',\n",
      "       'feature_60', 'feature_61', 'feature_62', 'feature_63', 'feature_64',\n",
      "       'feature_65', 'feature_66', 'feature_67', 'feature_68', 'feature_69',\n",
      "       'feature_70', 'feature_71', 'feature_72', 'feature_73', 'feature_74',\n",
      "       'feature_75', 'feature_76', 'feature_77', 'feature_78',\n",
      "       'responder_6_lag_1'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "js_predictor.run_inference_server()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (39, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>row_id</th><th>responder_6</th></tr><tr><td>i64</td><td>f32</td></tr></thead><tbody><tr><td>0</td><td>0.050659</td></tr><tr><td>1</td><td>0.027863</td></tr><tr><td>2</td><td>0.093018</td></tr><tr><td>3</td><td>-0.231567</td></tr><tr><td>4</td><td>-0.210083</td></tr><tr><td>&hellip;</td><td>&hellip;</td></tr><tr><td>34</td><td>-0.070068</td></tr><tr><td>35</td><td>-0.27002</td></tr><tr><td>36</td><td>0.332764</td></tr><tr><td>37</td><td>-0.078796</td></tr><tr><td>38</td><td>-0.137207</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (39, 2)\n",
       "\n",
       " row_id  responder_6 \n",
       " ---     ---         \n",
       " i64     f32         \n",
       "\n",
       " 0       0.050659    \n",
       " 1       0.027863    \n",
       " 2       0.093018    \n",
       " 3       -0.231567   \n",
       " 4       -0.210083   \n",
       "                   \n",
       " 34      -0.070068   \n",
       " 35      -0.27002    \n",
       " 36      0.332764    \n",
       " 37      -0.078796   \n",
       " 38      -0.137207   \n",
       ""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if os.path.isfile('submission.parquet'):\n",
    "    pl_sub = pl.read_parquet('submission.parquet')\n",
    "    print(len(pl_sub))\n",
    "    display(pl_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (39, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>row_id</th><th>responder_6</th></tr><tr><td>i64</td><td>f32</td></tr></thead><tbody><tr><td>0</td><td>0.050659</td></tr><tr><td>1</td><td>0.027863</td></tr><tr><td>2</td><td>0.093018</td></tr><tr><td>3</td><td>-0.231567</td></tr><tr><td>4</td><td>-0.210083</td></tr><tr><td>&hellip;</td><td>&hellip;</td></tr><tr><td>34</td><td>-0.070068</td></tr><tr><td>35</td><td>-0.27002</td></tr><tr><td>36</td><td>0.332764</td></tr><tr><td>37</td><td>-0.078796</td></tr><tr><td>38</td><td>-0.137207</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (39, 2)\n",
       "\n",
       " row_id  responder_6 \n",
       " ---     ---         \n",
       " i64     f32         \n",
       "\n",
       " 0       0.050659    \n",
       " 1       0.027863    \n",
       " 2       0.093018    \n",
       " 3       -0.231567   \n",
       " 4       -0.210083   \n",
       "                   \n",
       " 34      -0.070068   \n",
       " 35      -0.27002    \n",
       " 36      0.332764    \n",
       " 37      -0.078796   \n",
       " 38      -0.137207   \n",
       ""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if os.path.isfile('submission.parquet'):\n",
    "    pl_sub = pl.read_parquet('submission.parquet')\n",
    "    print(len(pl_sub))\n",
    "    display(pl_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_id</th>\n",
       "      <th>time_id</th>\n",
       "      <th>symbol_id</th>\n",
       "      <th>responder_0_lag_1</th>\n",
       "      <th>responder_1_lag_1</th>\n",
       "      <th>responder_2_lag_1</th>\n",
       "      <th>responder_3_lag_1</th>\n",
       "      <th>responder_4_lag_1</th>\n",
       "      <th>responder_5_lag_1</th>\n",
       "      <th>responder_6_lag_1</th>\n",
       "      <th>responder_7_lag_1</th>\n",
       "      <th>responder_8_lag_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.442215</td>\n",
       "      <td>-0.322407</td>\n",
       "      <td>0.143594</td>\n",
       "      <td>-0.926890</td>\n",
       "      <td>-0.782236</td>\n",
       "      <td>-0.036595</td>\n",
       "      <td>-1.305746</td>\n",
       "      <td>-0.795677</td>\n",
       "      <td>-0.143724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.651829</td>\n",
       "      <td>-1.707840</td>\n",
       "      <td>-0.893942</td>\n",
       "      <td>-1.065488</td>\n",
       "      <td>-1.871338</td>\n",
       "      <td>-0.615652</td>\n",
       "      <td>-1.162801</td>\n",
       "      <td>-1.205924</td>\n",
       "      <td>-1.245934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.656373</td>\n",
       "      <td>-0.264575</td>\n",
       "      <td>-0.892879</td>\n",
       "      <td>-1.511886</td>\n",
       "      <td>-1.033480</td>\n",
       "      <td>-0.378265</td>\n",
       "      <td>-1.574290</td>\n",
       "      <td>-1.863071</td>\n",
       "      <td>-0.027343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.188186</td>\n",
       "      <td>-0.190970</td>\n",
       "      <td>-0.701490</td>\n",
       "      <td>0.098453</td>\n",
       "      <td>-1.015506</td>\n",
       "      <td>-0.054984</td>\n",
       "      <td>0.329152</td>\n",
       "      <td>-0.965471</td>\n",
       "      <td>0.576635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.257462</td>\n",
       "      <td>-0.471325</td>\n",
       "      <td>-0.297420</td>\n",
       "      <td>0.074018</td>\n",
       "      <td>-0.324194</td>\n",
       "      <td>-0.597093</td>\n",
       "      <td>0.219856</td>\n",
       "      <td>-0.276356</td>\n",
       "      <td>-0.904790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.027579</td>\n",
       "      <td>-0.020169</td>\n",
       "      <td>0.640348</td>\n",
       "      <td>-0.948373</td>\n",
       "      <td>-0.374251</td>\n",
       "      <td>-0.240350</td>\n",
       "      <td>-0.913801</td>\n",
       "      <td>-0.548867</td>\n",
       "      <td>-1.283726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.419646</td>\n",
       "      <td>-0.181228</td>\n",
       "      <td>-0.194079</td>\n",
       "      <td>0.667993</td>\n",
       "      <td>0.936857</td>\n",
       "      <td>0.517728</td>\n",
       "      <td>0.896325</td>\n",
       "      <td>1.068884</td>\n",
       "      <td>1.579290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.114118</td>\n",
       "      <td>-0.198511</td>\n",
       "      <td>-0.200027</td>\n",
       "      <td>-0.410021</td>\n",
       "      <td>-0.135167</td>\n",
       "      <td>-0.182887</td>\n",
       "      <td>-0.492168</td>\n",
       "      <td>-0.142915</td>\n",
       "      <td>-0.202081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>-0.374147</td>\n",
       "      <td>0.092127</td>\n",
       "      <td>0.294723</td>\n",
       "      <td>0.402989</td>\n",
       "      <td>2.060188</td>\n",
       "      <td>-0.225042</td>\n",
       "      <td>0.956460</td>\n",
       "      <td>2.185598</td>\n",
       "      <td>-0.435856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>-0.529529</td>\n",
       "      <td>0.040104</td>\n",
       "      <td>-0.333090</td>\n",
       "      <td>-0.959040</td>\n",
       "      <td>-1.318411</td>\n",
       "      <td>-0.774299</td>\n",
       "      <td>-0.716492</td>\n",
       "      <td>-1.471419</td>\n",
       "      <td>-1.107083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>-0.709064</td>\n",
       "      <td>-0.137431</td>\n",
       "      <td>-0.475960</td>\n",
       "      <td>-0.506644</td>\n",
       "      <td>-0.297788</td>\n",
       "      <td>-0.530738</td>\n",
       "      <td>-0.263427</td>\n",
       "      <td>-0.169489</td>\n",
       "      <td>-0.410877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>-0.182779</td>\n",
       "      <td>-0.262493</td>\n",
       "      <td>-0.349921</td>\n",
       "      <td>-0.725857</td>\n",
       "      <td>-0.469289</td>\n",
       "      <td>-1.125309</td>\n",
       "      <td>-0.832106</td>\n",
       "      <td>-0.240194</td>\n",
       "      <td>-0.760374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>-0.409564</td>\n",
       "      <td>-0.210898</td>\n",
       "      <td>-0.097313</td>\n",
       "      <td>0.420984</td>\n",
       "      <td>-1.611198</td>\n",
       "      <td>1.065879</td>\n",
       "      <td>0.798224</td>\n",
       "      <td>-3.035606</td>\n",
       "      <td>1.810822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0.254306</td>\n",
       "      <td>0.114433</td>\n",
       "      <td>0.064752</td>\n",
       "      <td>-0.685130</td>\n",
       "      <td>-0.384532</td>\n",
       "      <td>-0.765541</td>\n",
       "      <td>-1.385921</td>\n",
       "      <td>-0.441037</td>\n",
       "      <td>-1.359048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0.464961</td>\n",
       "      <td>0.077041</td>\n",
       "      <td>0.601805</td>\n",
       "      <td>-0.178444</td>\n",
       "      <td>1.127965</td>\n",
       "      <td>-0.445524</td>\n",
       "      <td>-0.507432</td>\n",
       "      <td>0.985169</td>\n",
       "      <td>-1.497043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>0.059957</td>\n",
       "      <td>0.173762</td>\n",
       "      <td>-0.248479</td>\n",
       "      <td>-0.187606</td>\n",
       "      <td>0.539572</td>\n",
       "      <td>0.244086</td>\n",
       "      <td>-0.256192</td>\n",
       "      <td>0.974333</td>\n",
       "      <td>0.636512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0.138667</td>\n",
       "      <td>0.062221</td>\n",
       "      <td>-0.140365</td>\n",
       "      <td>-2.740061</td>\n",
       "      <td>-1.360370</td>\n",
       "      <td>-1.297678</td>\n",
       "      <td>-2.992689</td>\n",
       "      <td>-2.387136</td>\n",
       "      <td>-1.834790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>-0.235209</td>\n",
       "      <td>-0.201598</td>\n",
       "      <td>0.406477</td>\n",
       "      <td>4.062799</td>\n",
       "      <td>1.399957</td>\n",
       "      <td>2.418881</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.615073</td>\n",
       "      <td>3.689315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>-1.760321</td>\n",
       "      <td>-0.488708</td>\n",
       "      <td>-0.883805</td>\n",
       "      <td>0.771329</td>\n",
       "      <td>1.164359</td>\n",
       "      <td>2.030865</td>\n",
       "      <td>2.570583</td>\n",
       "      <td>2.326662</td>\n",
       "      <td>2.364794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>-0.120426</td>\n",
       "      <td>-0.257001</td>\n",
       "      <td>0.461233</td>\n",
       "      <td>0.334276</td>\n",
       "      <td>-0.430230</td>\n",
       "      <td>-0.053538</td>\n",
       "      <td>0.386924</td>\n",
       "      <td>-0.633187</td>\n",
       "      <td>-0.465996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.100019</td>\n",
       "      <td>-0.064951</td>\n",
       "      <td>0.103129</td>\n",
       "      <td>0.194796</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.019647</td>\n",
       "      <td>0.360535</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>-0.118341</td>\n",
       "      <td>-0.156263</td>\n",
       "      <td>-0.315895</td>\n",
       "      <td>-1.117113</td>\n",
       "      <td>-0.318234</td>\n",
       "      <td>-1.938428</td>\n",
       "      <td>-1.851255</td>\n",
       "      <td>-0.355483</td>\n",
       "      <td>-2.953523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>0.077620</td>\n",
       "      <td>-0.095140</td>\n",
       "      <td>-0.195245</td>\n",
       "      <td>2.259837</td>\n",
       "      <td>-0.049306</td>\n",
       "      <td>0.815069</td>\n",
       "      <td>2.036146</td>\n",
       "      <td>0.032317</td>\n",
       "      <td>1.591067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>0.049998</td>\n",
       "      <td>-0.024269</td>\n",
       "      <td>0.399543</td>\n",
       "      <td>1.664860</td>\n",
       "      <td>1.774516</td>\n",
       "      <td>0.849245</td>\n",
       "      <td>2.734162</td>\n",
       "      <td>2.179242</td>\n",
       "      <td>2.438320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>0.053630</td>\n",
       "      <td>0.338161</td>\n",
       "      <td>0.828120</td>\n",
       "      <td>0.061310</td>\n",
       "      <td>0.594190</td>\n",
       "      <td>-2.695299</td>\n",
       "      <td>0.086643</td>\n",
       "      <td>0.464238</td>\n",
       "      <td>-5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>0.859229</td>\n",
       "      <td>-0.752454</td>\n",
       "      <td>0.372965</td>\n",
       "      <td>1.137440</td>\n",
       "      <td>-0.969744</td>\n",
       "      <td>0.149047</td>\n",
       "      <td>0.522117</td>\n",
       "      <td>-0.690022</td>\n",
       "      <td>-0.084661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>-0.727778</td>\n",
       "      <td>-0.085832</td>\n",
       "      <td>-0.500547</td>\n",
       "      <td>0.080261</td>\n",
       "      <td>-0.168391</td>\n",
       "      <td>-0.017894</td>\n",
       "      <td>0.453380</td>\n",
       "      <td>-0.151904</td>\n",
       "      <td>0.455708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>-0.947047</td>\n",
       "      <td>0.345270</td>\n",
       "      <td>-0.416790</td>\n",
       "      <td>0.068557</td>\n",
       "      <td>2.012803</td>\n",
       "      <td>1.190820</td>\n",
       "      <td>0.641240</td>\n",
       "      <td>2.143041</td>\n",
       "      <td>2.033262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>0.185609</td>\n",
       "      <td>0.019040</td>\n",
       "      <td>0.928451</td>\n",
       "      <td>0.699763</td>\n",
       "      <td>0.094604</td>\n",
       "      <td>-0.093131</td>\n",
       "      <td>0.815095</td>\n",
       "      <td>0.101383</td>\n",
       "      <td>-0.993829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>0.410091</td>\n",
       "      <td>0.073472</td>\n",
       "      <td>0.327349</td>\n",
       "      <td>1.020221</td>\n",
       "      <td>-0.262787</td>\n",
       "      <td>0.182656</td>\n",
       "      <td>0.794698</td>\n",
       "      <td>-0.455156</td>\n",
       "      <td>0.103020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>-0.165723</td>\n",
       "      <td>0.038975</td>\n",
       "      <td>-0.088357</td>\n",
       "      <td>-0.745973</td>\n",
       "      <td>-0.318136</td>\n",
       "      <td>-0.699026</td>\n",
       "      <td>-1.051536</td>\n",
       "      <td>-0.334043</td>\n",
       "      <td>-1.380436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>-0.434564</td>\n",
       "      <td>-2.683184</td>\n",
       "      <td>-0.439510</td>\n",
       "      <td>2.700525</td>\n",
       "      <td>2.373121</td>\n",
       "      <td>0.748074</td>\n",
       "      <td>2.698507</td>\n",
       "      <td>3.004424</td>\n",
       "      <td>1.457855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>-0.221083</td>\n",
       "      <td>0.319112</td>\n",
       "      <td>-0.359562</td>\n",
       "      <td>-1.037934</td>\n",
       "      <td>1.451325</td>\n",
       "      <td>-1.596154</td>\n",
       "      <td>-0.922532</td>\n",
       "      <td>1.574045</td>\n",
       "      <td>-3.389474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>-0.322945</td>\n",
       "      <td>-0.100357</td>\n",
       "      <td>0.044535</td>\n",
       "      <td>-0.853970</td>\n",
       "      <td>-1.932011</td>\n",
       "      <td>-1.108600</td>\n",
       "      <td>-1.377528</td>\n",
       "      <td>-2.624511</td>\n",
       "      <td>-0.798179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>-0.185392</td>\n",
       "      <td>-0.187891</td>\n",
       "      <td>-0.206658</td>\n",
       "      <td>-0.634903</td>\n",
       "      <td>-0.643175</td>\n",
       "      <td>-0.443875</td>\n",
       "      <td>-0.556474</td>\n",
       "      <td>-1.122211</td>\n",
       "      <td>-0.884185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>-0.308923</td>\n",
       "      <td>-0.434147</td>\n",
       "      <td>-1.354941</td>\n",
       "      <td>0.300540</td>\n",
       "      <td>-0.830827</td>\n",
       "      <td>0.424937</td>\n",
       "      <td>0.518839</td>\n",
       "      <td>-0.687369</td>\n",
       "      <td>1.440577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>-0.074661</td>\n",
       "      <td>-0.261698</td>\n",
       "      <td>-0.007051</td>\n",
       "      <td>-2.600390</td>\n",
       "      <td>-1.146709</td>\n",
       "      <td>-1.601274</td>\n",
       "      <td>-3.216254</td>\n",
       "      <td>-1.249338</td>\n",
       "      <td>-2.868875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>-0.658366</td>\n",
       "      <td>-0.282258</td>\n",
       "      <td>-0.438998</td>\n",
       "      <td>-0.709998</td>\n",
       "      <td>-1.143526</td>\n",
       "      <td>-1.562932</td>\n",
       "      <td>-0.506418</td>\n",
       "      <td>-1.355508</td>\n",
       "      <td>-2.630985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>0.572666</td>\n",
       "      <td>0.066861</td>\n",
       "      <td>-0.552490</td>\n",
       "      <td>0.107840</td>\n",
       "      <td>0.535348</td>\n",
       "      <td>-0.501347</td>\n",
       "      <td>-0.169114</td>\n",
       "      <td>0.457801</td>\n",
       "      <td>-0.136777</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    date_id  time_id  symbol_id  responder_0_lag_1  responder_1_lag_1  \\\n",
       "0         0        0          0          -0.442215          -0.322407   \n",
       "1         0        0          1          -0.651829          -1.707840   \n",
       "2         0        0          2          -0.656373          -0.264575   \n",
       "3         0        0          3          -0.188186          -0.190970   \n",
       "4         0        0          4          -0.257462          -0.471325   \n",
       "5         0        0          5           0.027579          -0.020169   \n",
       "6         0        0          6          -0.419646          -0.181228   \n",
       "7         0        0          7          -0.114118          -0.198511   \n",
       "8         0        0          8          -0.374147           0.092127   \n",
       "9         0        0          9          -0.529529           0.040104   \n",
       "10        0        0         10          -0.709064          -0.137431   \n",
       "11        0        0         11          -0.182779          -0.262493   \n",
       "12        0        0         12          -0.409564          -0.210898   \n",
       "13        0        0         13           0.254306           0.114433   \n",
       "14        0        0         14           0.464961           0.077041   \n",
       "15        0        0         15           0.059957           0.173762   \n",
       "16        0        0         16           0.138667           0.062221   \n",
       "17        0        0         17          -0.235209          -0.201598   \n",
       "18        0        0         18          -1.760321          -0.488708   \n",
       "19        0        0         19          -0.120426          -0.257001   \n",
       "20        0        0         20           0.100019          -0.064951   \n",
       "21        0        0         21          -0.118341          -0.156263   \n",
       "22        0        0         22           0.077620          -0.095140   \n",
       "23        0        0         23           0.049998          -0.024269   \n",
       "24        0        0         24           0.053630           0.338161   \n",
       "25        0        0         25           0.859229          -0.752454   \n",
       "26        0        0         26          -0.727778          -0.085832   \n",
       "27        0        0         27          -0.947047           0.345270   \n",
       "28        0        0         28           0.185609           0.019040   \n",
       "29        0        0         29           0.410091           0.073472   \n",
       "30        0        0         30          -0.165723           0.038975   \n",
       "31        0        0         31          -0.434564          -2.683184   \n",
       "32        0        0         32          -0.221083           0.319112   \n",
       "33        0        0         33          -0.322945          -0.100357   \n",
       "34        0        0         34          -0.185392          -0.187891   \n",
       "35        0        0         35          -0.308923          -0.434147   \n",
       "36        0        0         36          -0.074661          -0.261698   \n",
       "37        0        0         37          -0.658366          -0.282258   \n",
       "38        0        0         38           0.572666           0.066861   \n",
       "\n",
       "    responder_2_lag_1  responder_3_lag_1  responder_4_lag_1  \\\n",
       "0            0.143594          -0.926890          -0.782236   \n",
       "1           -0.893942          -1.065488          -1.871338   \n",
       "2           -0.892879          -1.511886          -1.033480   \n",
       "3           -0.701490           0.098453          -1.015506   \n",
       "4           -0.297420           0.074018          -0.324194   \n",
       "5            0.640348          -0.948373          -0.374251   \n",
       "6           -0.194079           0.667993           0.936857   \n",
       "7           -0.200027          -0.410021          -0.135167   \n",
       "8            0.294723           0.402989           2.060188   \n",
       "9           -0.333090          -0.959040          -1.318411   \n",
       "10          -0.475960          -0.506644          -0.297788   \n",
       "11          -0.349921          -0.725857          -0.469289   \n",
       "12          -0.097313           0.420984          -1.611198   \n",
       "13           0.064752          -0.685130          -0.384532   \n",
       "14           0.601805          -0.178444           1.127965   \n",
       "15          -0.248479          -0.187606           0.539572   \n",
       "16          -0.140365          -2.740061          -1.360370   \n",
       "17           0.406477           4.062799           1.399957   \n",
       "18          -0.883805           0.771329           1.164359   \n",
       "19           0.461233           0.334276          -0.430230   \n",
       "20           0.103129           0.194796           5.000000   \n",
       "21          -0.315895          -1.117113          -0.318234   \n",
       "22          -0.195245           2.259837          -0.049306   \n",
       "23           0.399543           1.664860           1.774516   \n",
       "24           0.828120           0.061310           0.594190   \n",
       "25           0.372965           1.137440          -0.969744   \n",
       "26          -0.500547           0.080261          -0.168391   \n",
       "27          -0.416790           0.068557           2.012803   \n",
       "28           0.928451           0.699763           0.094604   \n",
       "29           0.327349           1.020221          -0.262787   \n",
       "30          -0.088357          -0.745973          -0.318136   \n",
       "31          -0.439510           2.700525           2.373121   \n",
       "32          -0.359562          -1.037934           1.451325   \n",
       "33           0.044535          -0.853970          -1.932011   \n",
       "34          -0.206658          -0.634903          -0.643175   \n",
       "35          -1.354941           0.300540          -0.830827   \n",
       "36          -0.007051          -2.600390          -1.146709   \n",
       "37          -0.438998          -0.709998          -1.143526   \n",
       "38          -0.552490           0.107840           0.535348   \n",
       "\n",
       "    responder_5_lag_1  responder_6_lag_1  responder_7_lag_1  responder_8_lag_1  \n",
       "0           -0.036595          -1.305746          -0.795677          -0.143724  \n",
       "1           -0.615652          -1.162801          -1.205924          -1.245934  \n",
       "2           -0.378265          -1.574290          -1.863071          -0.027343  \n",
       "3           -0.054984           0.329152          -0.965471           0.576635  \n",
       "4           -0.597093           0.219856          -0.276356          -0.904790  \n",
       "5           -0.240350          -0.913801          -0.548867          -1.283726  \n",
       "6            0.517728           0.896325           1.068884           1.579290  \n",
       "7           -0.182887          -0.492168          -0.142915          -0.202081  \n",
       "8           -0.225042           0.956460           2.185598          -0.435856  \n",
       "9           -0.774299          -0.716492          -1.471419          -1.107083  \n",
       "10          -0.530738          -0.263427          -0.169489          -0.410877  \n",
       "11          -1.125309          -0.832106          -0.240194          -0.760374  \n",
       "12           1.065879           0.798224          -3.035606           1.810822  \n",
       "13          -0.765541          -1.385921          -0.441037          -1.359048  \n",
       "14          -0.445524          -0.507432           0.985169          -1.497043  \n",
       "15           0.244086          -0.256192           0.974333           0.636512  \n",
       "16          -1.297678          -2.992689          -2.387136          -1.834790  \n",
       "17           2.418881           5.000000           1.615073           3.689315  \n",
       "18           2.030865           2.570583           2.326662           2.364794  \n",
       "19          -0.053538           0.386924          -0.633187          -0.465996  \n",
       "20           0.019647           0.360535           5.000000           0.000687  \n",
       "21          -1.938428          -1.851255          -0.355483          -2.953523  \n",
       "22           0.815069           2.036146           0.032317           1.591067  \n",
       "23           0.849245           2.734162           2.179242           2.438320  \n",
       "24          -2.695299           0.086643           0.464238          -5.000000  \n",
       "25           0.149047           0.522117          -0.690022          -0.084661  \n",
       "26          -0.017894           0.453380          -0.151904           0.455708  \n",
       "27           1.190820           0.641240           2.143041           2.033262  \n",
       "28          -0.093131           0.815095           0.101383          -0.993829  \n",
       "29           0.182656           0.794698          -0.455156           0.103020  \n",
       "30          -0.699026          -1.051536          -0.334043          -1.380436  \n",
       "31           0.748074           2.698507           3.004424           1.457855  \n",
       "32          -1.596154          -0.922532           1.574045          -3.389474  \n",
       "33          -1.108600          -1.377528          -2.624511          -0.798179  \n",
       "34          -0.443875          -0.556474          -1.122211          -0.884185  \n",
       "35           0.424937           0.518839          -0.687369           1.440577  \n",
       "36          -1.601274          -3.216254          -1.249338          -2.868875  \n",
       "37          -1.562932          -0.506418          -1.355508          -2.630985  \n",
       "38          -0.501347          -0.169114           0.457801          -0.136777  "
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "js_predictor.lags_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (39, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>row_id</th><th>responder_6</th></tr><tr><td>i64</td><td>f64</td></tr></thead><tbody><tr><td>0</td><td>0.0</td></tr><tr><td>1</td><td>0.0</td></tr><tr><td>2</td><td>0.0</td></tr><tr><td>3</td><td>0.0</td></tr><tr><td>4</td><td>0.0</td></tr><tr><td>&hellip;</td><td>&hellip;</td></tr><tr><td>34</td><td>0.0</td></tr><tr><td>35</td><td>0.0</td></tr><tr><td>36</td><td>0.0</td></tr><tr><td>37</td><td>0.0</td></tr><tr><td>38</td><td>0.0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (39, 2)\n",
       "\n",
       " row_id  responder_6 \n",
       " ---     ---         \n",
       " i64     f64         \n",
       "\n",
       " 0       0.0         \n",
       " 1       0.0         \n",
       " 2       0.0         \n",
       " 3       0.0         \n",
       " 4       0.0         \n",
       "                   \n",
       " 34      0.0         \n",
       " 35      0.0         \n",
       " 36      0.0         \n",
       " 37      0.0         \n",
       " 38      0.0         \n",
       ""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if os.path.isfile('submission.parquet'):\n",
    "    pl_sub = pl.read_parquet('submission.parquet')\n",
    "    print(len(pl_sub))\n",
    "    display(pl_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMFeedForward(\n",
       "  (lstm): LSTMEquavalentMLP(\n",
       "    (input_gate): Linear(in_features=80, out_features=128, bias=True)\n",
       "    (candidate_gate): Linear(in_features=80, out_features=128, bias=True)\n",
       "    (output_gate): Linear(in_features=80, out_features=128, bias=True)\n",
       "  )\n",
       "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (39, 85)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>row_id</th><th>date_id</th><th>time_id</th><th>symbol_id</th><th>weight</th><th>is_scored</th><th>feature_00</th><th>feature_01</th><th>feature_02</th><th>feature_03</th><th>feature_04</th><th>feature_05</th><th>feature_06</th><th>feature_07</th><th>feature_08</th><th>feature_09</th><th>feature_10</th><th>feature_11</th><th>feature_12</th><th>feature_13</th><th>feature_14</th><th>feature_15</th><th>feature_16</th><th>feature_17</th><th>feature_18</th><th>feature_19</th><th>feature_20</th><th>feature_21</th><th>feature_22</th><th>feature_23</th><th>feature_24</th><th>feature_25</th><th>feature_26</th><th>feature_27</th><th>feature_28</th><th>feature_29</th><th>feature_30</th><th>&hellip;</th><th>feature_42</th><th>feature_43</th><th>feature_44</th><th>feature_45</th><th>feature_46</th><th>feature_47</th><th>feature_48</th><th>feature_49</th><th>feature_50</th><th>feature_51</th><th>feature_52</th><th>feature_53</th><th>feature_54</th><th>feature_55</th><th>feature_56</th><th>feature_57</th><th>feature_58</th><th>feature_59</th><th>feature_60</th><th>feature_61</th><th>feature_62</th><th>feature_63</th><th>feature_64</th><th>feature_65</th><th>feature_66</th><th>feature_67</th><th>feature_68</th><th>feature_69</th><th>feature_70</th><th>feature_71</th><th>feature_72</th><th>feature_73</th><th>feature_74</th><th>feature_75</th><th>feature_76</th><th>feature_77</th><th>feature_78</th></tr><tr><td>i64</td><td>i16</td><td>i16</td><td>i8</td><td>f32</td><td>bool</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f64</td><td>f64</td><td>f64</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>&hellip;</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td></tr></thead><tbody><tr><td>0</td><td>0</td><td>0</td><td>0</td><td>3.169998</td><td>true</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>null</td><td>-0.0</td><td>null</td><td>-0.0</td><td>-0.0</td><td>0.0</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>-0.0</td><td>&hellip;</td><td>null</td><td>-0.0</td><td>null</td><td>-0.0</td><td>0.0</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>null</td><td>0.0</td><td>null</td><td>null</td><td>-0.0</td><td>null</td><td>-0.0</td><td>0.0</td><td>null</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>0.0</td><td>-0.0</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>null</td><td>null</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>-0.0</td></tr><tr><td>1</td><td>0</td><td>0</td><td>1</td><td>2.165993</td><td>true</td><td>0.0</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>0.0</td><td>-0.0</td><td>null</td><td>-0.0</td><td>null</td><td>-0.0</td><td>-0.0</td><td>0.0</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>-0.0</td><td>&hellip;</td><td>null</td><td>-0.0</td><td>null</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>null</td><td>0.0</td><td>null</td><td>null</td><td>-0.0</td><td>null</td><td>-0.0</td><td>0.0</td><td>null</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>0.0</td><td>-0.0</td><td>-0.0</td><td>0.0</td><td>-0.0</td><td>null</td><td>null</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><td>2</td><td>0</td><td>0</td><td>2</td><td>3.06555</td><td>true</td><td>0.0</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>null</td><td>-0.0</td><td>null</td><td>-0.0</td><td>-0.0</td><td>0.0</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>-0.0</td><td>&hellip;</td><td>null</td><td>-0.0</td><td>null</td><td>-0.0</td><td>0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>null</td><td>0.0</td><td>null</td><td>null</td><td>-0.0</td><td>null</td><td>-0.0</td><td>0.0</td><td>null</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>null</td><td>null</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>-0.0</td></tr><tr><td>3</td><td>0</td><td>0</td><td>3</td><td>2.698642</td><td>true</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>null</td><td>-0.0</td><td>null</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>&hellip;</td><td>null</td><td>-0.0</td><td>null</td><td>-0.0</td><td>0.0</td><td>-0.0</td><td>0.0</td><td>-0.0</td><td>null</td><td>-0.0</td><td>null</td><td>null</td><td>-0.0</td><td>null</td><td>-0.0</td><td>0.0</td><td>null</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>null</td><td>null</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>-0.0</td></tr><tr><td>4</td><td>0</td><td>0</td><td>4</td><td>1.80333</td><td>true</td><td>0.0</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>0.0</td><td>-0.0</td><td>null</td><td>-0.0</td><td>null</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>&hellip;</td><td>null</td><td>-0.0</td><td>null</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>null</td><td>0.0</td><td>null</td><td>null</td><td>-0.0</td><td>null</td><td>-0.0</td><td>0.0</td><td>null</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>0.0</td><td>-0.0</td><td>-0.0</td><td>0.0</td><td>-0.0</td><td>null</td><td>null</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>34</td><td>0</td><td>0</td><td>34</td><td>3.240565</td><td>true</td><td>0.0</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>null</td><td>-0.0</td><td>null</td><td>-0.0</td><td>-0.0</td><td>0.0</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>-0.0</td><td>&hellip;</td><td>null</td><td>-0.0</td><td>null</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>null</td><td>-0.0</td><td>null</td><td>null</td><td>-0.0</td><td>null</td><td>-0.0</td><td>-0.0</td><td>null</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>null</td><td>null</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><td>35</td><td>0</td><td>0</td><td>35</td><td>1.057221</td><td>true</td><td>0.0</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>0.0</td><td>-0.0</td><td>null</td><td>-0.0</td><td>null</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>0.0</td><td>-0.0</td><td>0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>&hellip;</td><td>null</td><td>0.0</td><td>null</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>null</td><td>0.0</td><td>null</td><td>null</td><td>0.0</td><td>null</td><td>-0.0</td><td>0.0</td><td>null</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>0.0</td><td>-0.0</td><td>null</td><td>null</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>-0.0</td></tr><tr><td>36</td><td>0</td><td>0</td><td>36</td><td>0.907022</td><td>true</td><td>0.0</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>0.0</td><td>-0.0</td><td>null</td><td>-0.0</td><td>null</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>null</td><td>-0.0</td><td>-0.0</td><td>0.0</td><td>-0.0</td><td>null</td><td>null</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>&hellip;</td><td>null</td><td>-0.0</td><td>null</td><td>-0.0</td><td>0.0</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>null</td><td>0.0</td><td>null</td><td>null</td><td>0.0</td><td>null</td><td>-0.0</td><td>0.0</td><td>null</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>0.0</td><td>-0.0</td><td>-0.0</td><td>0.0</td><td>-0.0</td><td>null</td><td>null</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><td>37</td><td>0</td><td>0</td><td>37</td><td>1.393967</td><td>true</td><td>0.0</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>0.0</td><td>-0.0</td><td>null</td><td>-0.0</td><td>null</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>&hellip;</td><td>null</td><td>-0.0</td><td>null</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>null</td><td>0.0</td><td>null</td><td>null</td><td>0.0</td><td>null</td><td>-0.0</td><td>0.0</td><td>null</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>0.0</td><td>-0.0</td><td>-0.0</td><td>0.0</td><td>-0.0</td><td>null</td><td>null</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><td>38</td><td>0</td><td>0</td><td>38</td><td>4.100645</td><td>true</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>null</td><td>-0.0</td><td>null</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>-0.0</td><td>&hellip;</td><td>null</td><td>-0.0</td><td>null</td><td>-0.0</td><td>0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>null</td><td>0.0</td><td>null</td><td>null</td><td>-0.0</td><td>null</td><td>-0.0</td><td>0.0</td><td>null</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>0.0</td><td>-0.0</td><td>null</td><td>null</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>-0.0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (39, 85)\n",
       "\n",
       " row_id  date_id  time_id  symbol_id    feature_75  feature_76  feature_77  feature_78 \n",
       " ---     ---      ---      ---           ---         ---         ---         ---        \n",
       " i64     i16      i16      i8            f32         f32         f32         f32        \n",
       "\n",
       " 0       0        0        0            0.0         0.0         -0.0        -0.0       \n",
       " 1       0        0        1            0.0         0.0         0.0         0.0        \n",
       " 2       0        0        2            0.0         0.0         -0.0        -0.0       \n",
       " 3       0        0        3            0.0         0.0         -0.0        -0.0       \n",
       " 4       0        0        4            0.0         0.0         0.0         0.0        \n",
       "                                                                               \n",
       " 34      0        0        34           0.0         0.0         0.0         0.0        \n",
       " 35      0        0        35           0.0         0.0         -0.0        -0.0       \n",
       " 36      0        0        36           0.0         0.0         0.0         0.0        \n",
       " 37      0        0        37           0.0         0.0         0.0         0.0        \n",
       " 38      0        0        38           0.0         0.0         -0.0        -0.0       \n",
       ""
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.read_parquet(os.getcwd() + '/input/test.parquet/date_id=0/part-0.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "lags_ : pl.DataFrame | None = None\n",
    "\n",
    "\n",
    "# Replace this function with your inference code.\n",
    "# You can return either a Pandas or Polars dataframe, though Polars is recommended.\n",
    "# Each batch of predictions (except the very first) must be returned within 1 minute of the batch features being provided.\n",
    "def predict(test: pl.DataFrame, lags: pl.DataFrame | None) -> pl.DataFrame | pd.DataFrame:\n",
    "    \"\"\"Make a prediction.\"\"\"\n",
    "    # All the responders from the previous day are passed in at time_id == 0. We save them in a global variable for access at every time_id.\n",
    "    # Use them as extra features, if you like.\n",
    "    global lags_\n",
    "    if lags is not None:\n",
    "        lags_ = lags\n",
    "\n",
    "    # Replace this section with your own predictions\n",
    "    predictions = test.select(\n",
    "        'row_id',\n",
    "        pl.lit(0.0).alias('responder_6'),\n",
    "    )\n",
    "\n",
    "    if isinstance(predictions, pl.DataFrame):\n",
    "        assert predictions.columns == ['row_id', 'responder_6']\n",
    "    elif isinstance(predictions, pd.DataFrame):\n",
    "        assert (predictions.columns == ['row_id', 'responder_6']).all()\n",
    "    else:\n",
    "        raise TypeError('The predict function must return a DataFrame')\n",
    "    # Confirm has as many rows as the test data.\n",
    "    assert len(predictions) == len(test)\n",
    "\n",
    "    return predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_server = js_server.JSInferenceServer(predict)\n",
    "\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    inference_server.serve()\n",
    "else:\n",
    "    inference_server.run_local_gateway(\n",
    "        (\n",
    "            test_parquet,\n",
    "            lags_parquet\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/app/input/test.parquet'"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (39, 85)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>row_id</th><th>date_id</th><th>time_id</th><th>symbol_id</th><th>weight</th><th>is_scored</th><th>feature_00</th><th>feature_01</th><th>feature_02</th><th>feature_03</th><th>feature_04</th><th>feature_05</th><th>feature_06</th><th>feature_07</th><th>feature_08</th><th>feature_09</th><th>feature_10</th><th>feature_11</th><th>feature_12</th><th>feature_13</th><th>feature_14</th><th>feature_15</th><th>feature_16</th><th>feature_17</th><th>feature_18</th><th>feature_19</th><th>feature_20</th><th>feature_21</th><th>feature_22</th><th>feature_23</th><th>feature_24</th><th>feature_25</th><th>feature_26</th><th>feature_27</th><th>feature_28</th><th>feature_29</th><th>feature_30</th><th>&hellip;</th><th>feature_42</th><th>feature_43</th><th>feature_44</th><th>feature_45</th><th>feature_46</th><th>feature_47</th><th>feature_48</th><th>feature_49</th><th>feature_50</th><th>feature_51</th><th>feature_52</th><th>feature_53</th><th>feature_54</th><th>feature_55</th><th>feature_56</th><th>feature_57</th><th>feature_58</th><th>feature_59</th><th>feature_60</th><th>feature_61</th><th>feature_62</th><th>feature_63</th><th>feature_64</th><th>feature_65</th><th>feature_66</th><th>feature_67</th><th>feature_68</th><th>feature_69</th><th>feature_70</th><th>feature_71</th><th>feature_72</th><th>feature_73</th><th>feature_74</th><th>feature_75</th><th>feature_76</th><th>feature_77</th><th>feature_78</th></tr><tr><td>i64</td><td>i16</td><td>i16</td><td>i8</td><td>f32</td><td>bool</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f64</td><td>f64</td><td>f64</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>&hellip;</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td></tr></thead><tbody><tr><td>0</td><td>0</td><td>0</td><td>0</td><td>3.169998</td><td>true</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>null</td><td>-0.0</td><td>null</td><td>-0.0</td><td>-0.0</td><td>0.0</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>-0.0</td><td>&hellip;</td><td>null</td><td>-0.0</td><td>null</td><td>-0.0</td><td>0.0</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>null</td><td>0.0</td><td>null</td><td>null</td><td>-0.0</td><td>null</td><td>-0.0</td><td>0.0</td><td>null</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>0.0</td><td>-0.0</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>null</td><td>null</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>-0.0</td></tr><tr><td>1</td><td>0</td><td>0</td><td>1</td><td>2.165993</td><td>true</td><td>0.0</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>0.0</td><td>-0.0</td><td>null</td><td>-0.0</td><td>null</td><td>-0.0</td><td>-0.0</td><td>0.0</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>-0.0</td><td>&hellip;</td><td>null</td><td>-0.0</td><td>null</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>null</td><td>0.0</td><td>null</td><td>null</td><td>-0.0</td><td>null</td><td>-0.0</td><td>0.0</td><td>null</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>0.0</td><td>-0.0</td><td>-0.0</td><td>0.0</td><td>-0.0</td><td>null</td><td>null</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><td>2</td><td>0</td><td>0</td><td>2</td><td>3.06555</td><td>true</td><td>0.0</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>null</td><td>-0.0</td><td>null</td><td>-0.0</td><td>-0.0</td><td>0.0</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>-0.0</td><td>&hellip;</td><td>null</td><td>-0.0</td><td>null</td><td>-0.0</td><td>0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>null</td><td>0.0</td><td>null</td><td>null</td><td>-0.0</td><td>null</td><td>-0.0</td><td>0.0</td><td>null</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>null</td><td>null</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>-0.0</td></tr><tr><td>3</td><td>0</td><td>0</td><td>3</td><td>2.698642</td><td>true</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>null</td><td>-0.0</td><td>null</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>&hellip;</td><td>null</td><td>-0.0</td><td>null</td><td>-0.0</td><td>0.0</td><td>-0.0</td><td>0.0</td><td>-0.0</td><td>null</td><td>-0.0</td><td>null</td><td>null</td><td>-0.0</td><td>null</td><td>-0.0</td><td>0.0</td><td>null</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>null</td><td>null</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>-0.0</td></tr><tr><td>4</td><td>0</td><td>0</td><td>4</td><td>1.80333</td><td>true</td><td>0.0</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>0.0</td><td>-0.0</td><td>null</td><td>-0.0</td><td>null</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>&hellip;</td><td>null</td><td>-0.0</td><td>null</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>null</td><td>0.0</td><td>null</td><td>null</td><td>-0.0</td><td>null</td><td>-0.0</td><td>0.0</td><td>null</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>0.0</td><td>-0.0</td><td>-0.0</td><td>0.0</td><td>-0.0</td><td>null</td><td>null</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>34</td><td>0</td><td>0</td><td>34</td><td>3.240565</td><td>true</td><td>0.0</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>null</td><td>-0.0</td><td>null</td><td>-0.0</td><td>-0.0</td><td>0.0</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>-0.0</td><td>&hellip;</td><td>null</td><td>-0.0</td><td>null</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>null</td><td>-0.0</td><td>null</td><td>null</td><td>-0.0</td><td>null</td><td>-0.0</td><td>-0.0</td><td>null</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>null</td><td>null</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><td>35</td><td>0</td><td>0</td><td>35</td><td>1.057221</td><td>true</td><td>0.0</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>0.0</td><td>-0.0</td><td>null</td><td>-0.0</td><td>null</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>0.0</td><td>-0.0</td><td>0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>&hellip;</td><td>null</td><td>0.0</td><td>null</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>null</td><td>0.0</td><td>null</td><td>null</td><td>0.0</td><td>null</td><td>-0.0</td><td>0.0</td><td>null</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>0.0</td><td>-0.0</td><td>null</td><td>null</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>-0.0</td></tr><tr><td>36</td><td>0</td><td>0</td><td>36</td><td>0.907022</td><td>true</td><td>0.0</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>0.0</td><td>-0.0</td><td>null</td><td>-0.0</td><td>null</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>null</td><td>-0.0</td><td>-0.0</td><td>0.0</td><td>-0.0</td><td>null</td><td>null</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>&hellip;</td><td>null</td><td>-0.0</td><td>null</td><td>-0.0</td><td>0.0</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>null</td><td>0.0</td><td>null</td><td>null</td><td>0.0</td><td>null</td><td>-0.0</td><td>0.0</td><td>null</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>0.0</td><td>-0.0</td><td>-0.0</td><td>0.0</td><td>-0.0</td><td>null</td><td>null</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><td>37</td><td>0</td><td>0</td><td>37</td><td>1.393967</td><td>true</td><td>0.0</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>0.0</td><td>-0.0</td><td>null</td><td>-0.0</td><td>null</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>&hellip;</td><td>null</td><td>-0.0</td><td>null</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>null</td><td>0.0</td><td>null</td><td>null</td><td>0.0</td><td>null</td><td>-0.0</td><td>0.0</td><td>null</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>0.0</td><td>-0.0</td><td>-0.0</td><td>0.0</td><td>-0.0</td><td>null</td><td>null</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><td>38</td><td>0</td><td>0</td><td>38</td><td>4.100645</td><td>true</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>null</td><td>-0.0</td><td>null</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>-0.0</td><td>&hellip;</td><td>null</td><td>-0.0</td><td>null</td><td>-0.0</td><td>0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>null</td><td>0.0</td><td>null</td><td>null</td><td>-0.0</td><td>null</td><td>-0.0</td><td>0.0</td><td>null</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>0.0</td><td>-0.0</td><td>null</td><td>null</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>-0.0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (39, 85)\n",
       "\n",
       " row_id  date_id  time_id  symbol_id    feature_75  feature_76  feature_77  feature_78 \n",
       " ---     ---      ---      ---           ---         ---         ---         ---        \n",
       " i64     i16      i16      i8            f32         f32         f32         f32        \n",
       "\n",
       " 0       0        0        0            0.0         0.0         -0.0        -0.0       \n",
       " 1       0        0        1            0.0         0.0         0.0         0.0        \n",
       " 2       0        0        2            0.0         0.0         -0.0        -0.0       \n",
       " 3       0        0        3            0.0         0.0         -0.0        -0.0       \n",
       " 4       0        0        4            0.0         0.0         0.0         0.0        \n",
       "                                                                               \n",
       " 34      0        0        34           0.0         0.0         0.0         0.0        \n",
       " 35      0        0        35           0.0         0.0         -0.0        -0.0       \n",
       " 36      0        0        36           0.0         0.0         0.0         0.0        \n",
       " 37      0        0        37           0.0         0.0         0.0         0.0        \n",
       " 38      0        0        38           0.0         0.0         -0.0        -0.0       \n",
       ""
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.read_parquet(test_parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
