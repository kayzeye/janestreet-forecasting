{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --quiet -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Nov 19 05:09:00 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3060        Off |   00000000:01:00.0  On |                  N/A |\n",
      "|  0%   36C    P5             23W /  170W |     602MiB /  12288MiB |     33%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os, gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "#from hyperopt import hp, fmin, tpe, Trials\n",
    "#from hyperopt.pyll.base import scope\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import matplotlib.pyplot as plt\n",
    "#from tqdm.notebook import tqdm\n",
    "#from joblib import dump, load\n",
    "#import datatable as dtable\n",
    "#from mlxtend.evaluate import GroupTimeSeriesSplit\n",
    "import kaggle_evaluation.jane_street_inference_server\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "TEST=False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1+cu121\n",
      "True\n",
      "NVIDIA GeForce RTX 3060\n"
     ]
    }
   ],
   "source": [
    "# check that torch is working and sees the GPU\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Garbage collector: collected 15 objects.\n"
     ]
    }
   ],
   "source": [
    "collected = gc.collect()\n",
    "# Prints Garbage collector \n",
    "# as 0 object\n",
    "print(\"Garbage collector: collected\",\n",
    "          \"%d objects.\" % collected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = os.getcwd()\n",
    "if TEST:\n",
    "    df1 = pl.read_parquet(path+'/input/train.parquet/partition_id=0/part-0.parquet')\n",
    "    df2 = pl.read_parquet(path+'/input/train.parquet/partition_id=1/part-0.parquet')\n",
    "    df3 = pl.read_parquet(path+'/input/train.parquet/partition_id=2/part-0.parquet')\n",
    "    train = pl.concat([df1,df2,df3])\n",
    "    del df1, df2, df3\n",
    "\n",
    "else:\n",
    "    # very important to optimize memory when loading the full damn thing\n",
    "    cols = ['date_id', 'time_id', 'weight']\n",
    "    cols.extend(['feature_{:02d}'.format(num) for num in range(0, 79)])\n",
    "    cols.append('responder_6')\n",
    "    df1 = pl.read_parquet(path+'/input/train.parquet/partition_id=0/part-0.parquet', columns=cols)\n",
    "    df2 = pl.read_parquet(path+'/input/train.parquet/partition_id=1/part-0.parquet', columns=cols)\n",
    "    df3 = pl.read_parquet(path+'/input/train.parquet/partition_id=2/part-0.parquet', columns=cols)\n",
    "    df4 = pl.read_parquet(path+'/input/train.parquet/partition_id=3/part-0.parquet', columns=cols)\n",
    "    df5 = pl.read_parquet(path+'/input/train.parquet/partition_id=4/part-0.parquet', columns=cols)\n",
    "    df6 = pl.read_parquet(path+'/input/train.parquet/partition_id=5/part-0.parquet', columns=cols)\n",
    "    df7 = pl.read_parquet(path+'/input/train.parquet/partition_id=6/part-0.parquet', columns=cols)\n",
    "    df8 = pl.read_parquet(path+'/input/train.parquet/partition_id=7/part-0.parquet', columns=cols)\n",
    "    df9 = pl.read_parquet(path+'/input/train.parquet/partition_id=8/part-0.parquet', columns=cols)\n",
    "    df10 = pl.read_parquet(path+'/input/train.parquet/partition_id=9/part-0.parquet', columns=cols)\n",
    "    train = pl.concat([df1, df2, df3, df4, df5, df6, df7, df8, df9, df10])\n",
    "    del df1, df2, df3, df4, df5, df6, df7, df8, df9, df10\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(47127338, 83)\n",
      "['date_id', 'time_id', 'weight', 'feature_00', 'feature_01', 'feature_02', 'feature_03', 'feature_04', 'feature_05', 'feature_06', 'feature_07', 'feature_08', 'feature_09', 'feature_10', 'feature_11', 'feature_12', 'feature_13', 'feature_14', 'feature_15', 'feature_16', 'feature_17', 'feature_18', 'feature_19', 'feature_20', 'feature_21', 'feature_22', 'feature_23', 'feature_24', 'feature_25', 'feature_26', 'feature_27', 'feature_28', 'feature_29', 'feature_30', 'feature_31', 'feature_32', 'feature_33', 'feature_34', 'feature_35', 'feature_36', 'feature_37', 'feature_38', 'feature_39', 'feature_40', 'feature_41', 'feature_42', 'feature_43', 'feature_44', 'feature_45', 'feature_46', 'feature_47', 'feature_48', 'feature_49', 'feature_50', 'feature_51', 'feature_52', 'feature_53', 'feature_54', 'feature_55', 'feature_56', 'feature_57', 'feature_58', 'feature_59', 'feature_60', 'feature_61', 'feature_62', 'feature_63', 'feature_64', 'feature_65', 'feature_66', 'feature_67', 'feature_68', 'feature_69', 'feature_70', 'feature_71', 'feature_72', 'feature_73', 'feature_74', 'feature_75', 'feature_76', 'feature_77', 'feature_78', 'responder_6']\n",
      "shape: (1, 79)\n",
      "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
      "│ feature_0 ┆ feature_0 ┆ feature_0 ┆ feature_0 ┆ … ┆ feature_7 ┆ feature_7 ┆ feature_7 ┆ feature_ │\n",
      "│ 0         ┆ 1         ┆ 2         ┆ 3         ┆   ┆ 5         ┆ 6         ┆ 7         ┆ 78       │\n",
      "│ ---       ┆ ---       ┆ ---       ┆ ---       ┆   ┆ ---       ┆ ---       ┆ ---       ┆ ---      │\n",
      "│ u32       ┆ u32       ┆ u32       ┆ u32       ┆   ┆ u32       ┆ u32       ┆ u32       ┆ u32      │\n",
      "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
      "│ 3182052   ┆ 3182052   ┆ 3182052   ┆ 3182052   ┆ … ┆ 58430     ┆ 58430     ┆ 20043     ┆ 20043    │\n",
      "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘\n",
      "Garbage collector: collected 0 objects.\n"
     ]
    }
   ],
   "source": [
    "features = [c for c in train.columns if 'feature' in c]\n",
    "#features.append('weight')\n",
    "#features.append('time_id')\n",
    "#features.append('symbol_id')\n",
    "target = 'responder_6'\n",
    "print(train.shape)\n",
    "print(train.columns)\n",
    "print(train[features].null_count())\n",
    "# Returns the number of\n",
    "# objects it has collected\n",
    "# and deallocated\n",
    "collected = gc.collect()\n",
    "# Prints Garbage collector \n",
    "# as 0 object\n",
    "print(\"Garbage collector: collected\",\n",
    "          \"%d objects.\" % collected)\n",
    "\n",
    "# note to self, incorporate features.csv true/false tables at some point "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling...\n",
      "(47127338, 83)\n",
      "shape: (1, 83)\n",
      "┌─────────┬─────────┬────────┬────────────┬───┬────────────┬────────────┬────────────┬─────────────┐\n",
      "│ date_id ┆ time_id ┆ weight ┆ feature_00 ┆ … ┆ feature_76 ┆ feature_77 ┆ feature_78 ┆ responder_6 │\n",
      "│ ---     ┆ ---     ┆ ---    ┆ ---        ┆   ┆ ---        ┆ ---        ┆ ---        ┆ ---         │\n",
      "│ u32     ┆ u32     ┆ u32    ┆ u32        ┆   ┆ u32        ┆ u32        ┆ u32        ┆ u32         │\n",
      "╞═════════╪═════════╪════════╪════════════╪═══╪════════════╪════════════╪════════════╪═════════════╡\n",
      "│ 0       ┆ 0       ┆ 0      ┆ 0          ┆ … ┆ 0          ┆ 0          ┆ 0          ┆ 0           │\n",
      "└─────────┴─────────┴────────┴────────────┴───┴────────────┴────────────┴────────────┴─────────────┘\n"
     ]
    }
   ],
   "source": [
    "print('Filling...')\n",
    "# next two lines were for pandas not polars\n",
    "#f_mean = train[features].mean() \n",
    "# print(train.weight.gt(0).sum() == train.shape[0]) # check if weights>0 is true for entire dataframe\n",
    "print(train.shape)\n",
    "train = train.fill_null(0)\n",
    "#train.dropna(inplace=True)\n",
    "print(train.null_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# for the purged group time series split, code is copied from somewhere\n",
    "# TODO: make GitHub GIST\n",
    "# TODO: add as dataset\n",
    "# TODO: add logging with verbose\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection._split import _BaseKFold, indexable, _num_samples\n",
    "from sklearn.utils.validation import _deprecate_positional_args\n",
    "\n",
    "# modified code for group gaps; source\n",
    "# https://github.com/getgaurav2/scikit-learn/blob/d4a3af5cc9da3a76f0266932644b884c99724c57/sklearn/model_selection/_split.py#L2243\n",
    "class PurgedGroupTimeSeriesSplit(_BaseKFold):\n",
    "    \"\"\"Time Series cross-validator variant with non-overlapping groups.|\n",
    "    Allows for a gap in groups to avoid potentially leaking info from\n",
    "    train into test if the model has windowed or lag features.\n",
    "    Provides train/test indices to split time series data samples\n",
    "    that are observed at fixed time intervals according to a\n",
    "    third-party provided group.\n",
    "    In each split, test indices must be higher than before, and thus shuffling\n",
    "    in cross validator is inappropriate.\n",
    "    This cross-validation object is a variation of :class:`KFold`.\n",
    "    In the kth split, it returns first k folds as train set and the\n",
    "    (k+1)th fold as test set.\n",
    "    The same group will not appear in two different folds (the number of\n",
    "    distinct groups has to be at least equal to the number of folds).\n",
    "    Note that unlike standard cross-validation methods, successive\n",
    "    training sets are supersets of those that come before them.\n",
    "    Read more in the :ref:`User Guide <cross_validation>`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_splits : int, default=5\n",
    "        Number of splits. Must be at least 2.\n",
    "    max_train_group_size : int, default=Inf\n",
    "        Maximum group size for a single training set.\n",
    "    group_gap : int, default=None\n",
    "        Gap between train and test\n",
    "    max_test_group_size : int, default=Inf\n",
    "        We discard this number of groups from the end of each train split\n",
    "    \"\"\"\n",
    "\n",
    "    @_deprecate_positional_args\n",
    "    def __init__(self,\n",
    "                 n_splits=5,\n",
    "                 *,\n",
    "                 max_train_group_size=np.inf,\n",
    "                 max_test_group_size=np.inf,\n",
    "                 group_gap=None,\n",
    "                 verbose=False\n",
    "                 ):\n",
    "        super().__init__(n_splits, shuffle=False, random_state=None)\n",
    "        self.max_train_group_size = max_train_group_size\n",
    "        self.group_gap = group_gap\n",
    "        self.max_test_group_size = max_test_group_size\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        \"\"\"Generate indices to split data into training and test set.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Training data, where n_samples is the number of samples\n",
    "            and n_features is the number of features.\n",
    "        y : array-like of shape (n_samples,)\n",
    "            Always ignored, exists for compatibility.\n",
    "        groups : array-like of shape (n_samples,)\n",
    "            Group labels for the samples used while splitting the dataset into\n",
    "            train/test set.\n",
    "        Yields\n",
    "        ------\n",
    "        train : ndarray\n",
    "            The training set indices for that split.\n",
    "        test : ndarray\n",
    "            The testing set indices for that split.\n",
    "        \"\"\"\n",
    "        if groups is None:\n",
    "            raise ValueError(\n",
    "                \"The 'groups' parameter should not be None\")\n",
    "        X, y, groups = indexable(X, y, groups)\n",
    "        n_samples = _num_samples(X)\n",
    "        n_splits = self.n_splits\n",
    "        group_gap = self.group_gap\n",
    "        max_test_group_size = self.max_test_group_size\n",
    "        max_train_group_size = self.max_train_group_size\n",
    "        n_folds = n_splits + 1\n",
    "        group_dict = {}\n",
    "        u, ind = np.unique(groups, return_index=True)\n",
    "        unique_groups = u[np.argsort(ind)]\n",
    "        n_samples = _num_samples(X)\n",
    "        n_groups = _num_samples(unique_groups)\n",
    "        for idx in np.arange(n_samples):\n",
    "            if (groups[idx] in group_dict):\n",
    "                group_dict[groups[idx]].append(idx)\n",
    "            else:\n",
    "                group_dict[groups[idx]] = [idx]\n",
    "        if n_folds > n_groups:\n",
    "            raise ValueError(\n",
    "                (\"Cannot have number of folds={0} greater than\"\n",
    "                 \" the number of groups={1}\").format(n_folds,\n",
    "                                                     n_groups))\n",
    "\n",
    "        group_test_size = min(n_groups // n_folds, max_test_group_size)\n",
    "        group_test_starts = range(n_groups - n_splits * group_test_size,\n",
    "                                  n_groups, group_test_size)\n",
    "        for group_test_start in group_test_starts:\n",
    "            train_array = []\n",
    "            test_array = []\n",
    "\n",
    "            group_st = max(0, group_test_start - group_gap - max_train_group_size)\n",
    "            for train_group_idx in unique_groups[group_st:(group_test_start - group_gap)]:\n",
    "                train_array_tmp = group_dict[train_group_idx]\n",
    "                \n",
    "                train_array = np.sort(np.unique(\n",
    "                                      np.concatenate((train_array,\n",
    "                                                      train_array_tmp)),\n",
    "                                      axis=None), axis=None)\n",
    "\n",
    "            train_end = train_array.size\n",
    " \n",
    "            for test_group_idx in unique_groups[group_test_start:\n",
    "                                                group_test_start +\n",
    "                                                group_test_size]:\n",
    "                test_array_tmp = group_dict[test_group_idx]\n",
    "                test_array = np.sort(np.unique(\n",
    "                                              np.concatenate((test_array,\n",
    "                                                              test_array_tmp)),\n",
    "                                     axis=None), axis=None)\n",
    "\n",
    "            test_array  = test_array[group_gap:]\n",
    "            \n",
    "            \n",
    "            if self.verbose > 0:\n",
    "                    pass\n",
    "                    \n",
    "            yield [int(i) for i in train_array], [int(i) for i in test_array]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# from matplotlib.colors import ListedColormap\n",
    "    \n",
    "# # this is code slightly modified from the sklearn docs here:\n",
    "# # https://scikit-learn.org/stable/auto_examples/model_selection/plot_cv_indices.html#sphx-glr-auto-examples-model-selection-plot-cv-indices-py\n",
    "# def plot_cv_indices(cv, X, y, group, ax, n_splits, lw=10):\n",
    "#     \"\"\"Create a sample plot for indices of a cross-validation object.\"\"\"\n",
    "    \n",
    "#     cmap_cv = plt.cm.coolwarm\n",
    "\n",
    "#     jet = plt.cm.get_cmap('jet', 256)\n",
    "#     seq = np.linspace(0, 1, 256)\n",
    "#     _ = np.random.shuffle(seq)   # inplace\n",
    "#     cmap_data = ListedColormap(jet(seq))\n",
    "\n",
    "#     # Generate the training/testing visualizations for each CV split\n",
    "#     for ii, (tr, tt) in enumerate(cv.split(X=X, y=y, groups=group)):\n",
    "#         # Fill in indices with the training/test groups\n",
    "#         indices = np.array([np.nan] * len(X))\n",
    "#         indices[tt] = 1\n",
    "#         indices[tr] = 0\n",
    "\n",
    "#         # Visualize the results\n",
    "#         ax.scatter(range(len(indices)), [ii + .5] * len(indices),\n",
    "#                    c=indices, marker='_', lw=lw, cmap=cmap_cv,\n",
    "#                    vmin=-.2, vmax=1.2)\n",
    "\n",
    "#     # Plot the data classes and groups at the end\n",
    "#     ax.scatter(range(len(X)), [ii + 1.5] * len(X),\n",
    "#                c=y, marker='_', lw=lw, cmap=plt.cm.Set3)\n",
    "\n",
    "#     ax.scatter(range(len(X)), [ii + 2.5] * len(X),\n",
    "#                c=group, marker='_', lw=lw, cmap=cmap_data)\n",
    "\n",
    "#     # Formatting\n",
    "#     yticklabels = list(range(n_splits)) + ['target', 'day']\n",
    "#     ax.set(yticks=np.arange(n_splits+2) + .5, yticklabels=yticklabels,\n",
    "#            xlabel='Sample index', ylabel=\"CV iteration\",\n",
    "#            ylim=[n_splits+2.2, -.2], xlim=[0, len(y)])\n",
    "#     ax.set_title('{}'.format(type(cv).__name__), fontsize=15)\n",
    "#     return ax\n",
    "\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# #plot_cv_indices(cv, X_train, y_train, groups, ax, 5, lw=20)\n",
    "# plot_cv_indices(\n",
    "#     cv,\n",
    "#     train[features].values,\n",
    "#     train['responder_6'].values,\n",
    "#     train['date_id'].values,\n",
    "#     ax,\n",
    "#     5,\n",
    "#     lw=20\n",
    "# )\n",
    "# rubbish=gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# code structure copied from good tutorial: https://github.com/christianversloot/machine-learning-articles/blob/main/how-to-use-k-fold-cross-validation-with-pytorch.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "### Define model and various functions\n",
    "\n",
    "# get cpu, gpu or mps device for training\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "def reset_weights(m):\n",
    "    '''\n",
    "    Try resetting model weights to avoid\n",
    "    weight leakage.\n",
    "    '''\n",
    "    for layer in m.children():\n",
    "        if hasattr(layer, 'reset_parameters'):\n",
    "            print(f'Reset trainable parameters of layer = {layer}')\n",
    "            layer.reset_parameters()\n",
    "\n",
    "# for monitoring layer weights\n",
    "def weight_histograms(writer, step, model):\n",
    "    print(\"Visualizing model weights...\")\n",
    "    # Iterate over all model layers\n",
    "    for layer_number in range(len(model.layers)):\n",
    "        layer = model.layers[layer_number]\n",
    "        try:\n",
    "            weights = layer.weight\n",
    "            flattened_weights = weights.flatten()\n",
    "            tag = f\"layer_{layer_number}\"\n",
    "            writer.add_histogram(tag, flattened_weights, global_step=step, bins='tensorflow')\n",
    "        except AttributeError:\n",
    "            return\n",
    "            \n",
    "# loss is as defined on competition homepage\n",
    "def my_loss(outputs, targets, weights):\n",
    "    loss = torch.sum(weights*(targets - outputs)**2) / torch.sum(weights*targets**2)\n",
    "    return loss\n",
    "\n",
    "# \"standard\" loss function\n",
    "test_loss_function= nn.MSELoss()\n",
    "#test_loss_function = nn.L1Loss()\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    '''\n",
    "    Simple Neural Network/MultiLayer Perceptron\n",
    "    '''\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim, dropout_rates):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.dropout_rates = dropout_rates\n",
    "        current_dim = input_dim\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(nn.BatchNorm1d(current_dim))\n",
    "        self.layers.append(nn.Dropout(dropout_rates[0]))\n",
    "        for i in range(len(hidden_dim)):\n",
    "            self.layers.append(nn.Linear(current_dim, self.hidden_dim[i]))\n",
    "            self.layers.append(nn.BatchNorm1d(self.hidden_dim[i]))\n",
    "            self.layers.append(nn.SiLU())\n",
    "            self.layers.append(nn.Dropout(self.dropout_rates[i+1]))\n",
    "            current_dim = self.hidden_dim[i]\n",
    "        self.layers.append(nn.Linear(current_dim, self.output_dim))\n",
    "\n",
    "        self.seq = nn.Sequential(*self.layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''Forward pass'''\n",
    "        return self.seq(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "### Configuration options\n",
    "folds = 10\n",
    "num_epochs = 10\n",
    "test_loss = False\n",
    "# For fold results\n",
    "results = {}\n",
    "\n",
    "# Set fixed random number seed\n",
    "#torch.manual_seed(42)\n",
    "\n",
    "# Define the PurgedGroupTimeSeriesCV\n",
    "cv = PurgedGroupTimeSeriesSplit(\n",
    "n_splits=folds,\n",
    "max_train_group_size=150,\n",
    "group_gap=30,\n",
    "max_test_group_size=30\n",
    ")\n",
    "\n",
    "# extract needed parameters from dataframe as np arrays, will later turn into tensors\n",
    "X = train[features].to_numpy()\n",
    "y = train['responder_6'].to_numpy()\n",
    "group = train['date_id'].to_numpy()\n",
    "w = train['weight'].to_numpy()\n",
    "\n",
    "# Define the neural network parameters\n",
    "input_dim = len(features) \n",
    "output_dim = 1\n",
    "#hidden_dim = [32, 64, 64, 32, 8]\n",
    "hidden_dim = [128, 256, 256, 128, 64]\n",
    "#hidden_dim = [384, 896, 896, 394, 128]\n",
    "#dropout_rates = [0.01,0.01,0.01,0.01]\n",
    "dropout_rates=[0.1,0.1,0.1,0.1, 0.1, 0.1]*2\n",
    "learning_rate = 1e-4\n",
    "print('--------------------------------')\n",
    "\n",
    "# neat hack to ensure the original dataframe is gone\n",
    "del train\n",
    "gc.collect()\n",
    "train = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 0\n",
      "train_ids: [29443914, 35051537]\n",
      "test_ids: [36098944, 37195657]\n",
      "--------------------------------\n",
      "Reset trainable parameters of layer = BatchNorm1d(79, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=79, out_features=128, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=128, out_features=256, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=256, out_features=256, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=256, out_features=128, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=128, out_features=64, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=64, out_features=1, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(79, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=79, out_features=128, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=128, out_features=256, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=256, out_features=256, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=256, out_features=128, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=128, out_features=64, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=64, out_features=1, bias=True)\n",
      "Visualizing model weights...\n",
      "Starting epoch 1\n",
      "Loss after mini-batch  1000: 1.034\n",
      "Visualizing model weights...\n",
      "Starting epoch 2\n",
      "Loss after mini-batch  1000: 0.994\n",
      "Visualizing model weights...\n",
      "Starting epoch 3\n",
      "Loss after mini-batch  1000: 0.988\n",
      "Visualizing model weights...\n",
      "Starting epoch 4\n",
      "Loss after mini-batch  1000: 0.984\n",
      "Visualizing model weights...\n",
      "Starting epoch 5\n",
      "Loss after mini-batch  1000: 0.982\n",
      "Visualizing model weights...\n",
      "Starting epoch 6\n",
      "Loss after mini-batch  1000: 0.980\n",
      "Visualizing model weights...\n",
      "Starting epoch 7\n",
      "Loss after mini-batch  1000: 0.979\n",
      "Visualizing model weights...\n",
      "Starting epoch 8\n",
      "Loss after mini-batch  1000: 0.977\n",
      "Visualizing model weights...\n",
      "Starting epoch 9\n",
      "Loss after mini-batch  1000: 0.976\n",
      "Visualizing model weights...\n",
      "Starting epoch 10\n",
      "Loss after mini-batch  1000: 0.975\n",
      "Training process has finished\n",
      "Starting testing\n",
      "--------------------------------\n",
      "CROSS VALIDATION RESULTS FOR 10 FOLDS\n",
      "--------------------------------\n",
      "Fold 0: 1.1868382692337036\n",
      "Average: 1.1868382692337036\n",
      "FOLD 1\n",
      "train_ids: [30548402, 36098913]\n",
      "test_ids: [37195688, 38257553]\n",
      "--------------------------------\n",
      "Reset trainable parameters of layer = BatchNorm1d(79, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=79, out_features=128, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=128, out_features=256, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=256, out_features=256, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=256, out_features=128, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=128, out_features=64, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=64, out_features=1, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(79, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=79, out_features=128, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=128, out_features=256, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=256, out_features=256, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=256, out_features=128, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=128, out_features=64, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=64, out_features=1, bias=True)\n",
      "Visualizing model weights...\n",
      "Starting epoch 1\n",
      "Loss after mini-batch  1000: 1.041\n",
      "Visualizing model weights...\n",
      "Starting epoch 2\n",
      "Loss after mini-batch  1000: 0.993\n",
      "Visualizing model weights...\n",
      "Starting epoch 3\n",
      "Loss after mini-batch  1000: 0.987\n",
      "Visualizing model weights...\n",
      "Starting epoch 4\n",
      "Loss after mini-batch  1000: 0.985\n",
      "Visualizing model weights...\n",
      "Starting epoch 5\n",
      "Loss after mini-batch  1000: 0.983\n",
      "Visualizing model weights...\n",
      "Starting epoch 6\n",
      "Loss after mini-batch  1000: 0.982\n",
      "Visualizing model weights...\n",
      "Starting epoch 7\n",
      "Loss after mini-batch  1000: 0.981\n",
      "Visualizing model weights...\n",
      "Starting epoch 8\n",
      "Loss after mini-batch  1000: 0.979\n",
      "Visualizing model weights...\n",
      "Starting epoch 9\n",
      "Loss after mini-batch  1000: 0.978\n",
      "Visualizing model weights...\n",
      "Starting epoch 10\n",
      "Loss after mini-batch  1000: 0.977\n",
      "Training process has finished\n",
      "Starting testing\n",
      "--------------------------------\n",
      "CROSS VALIDATION RESULTS FOR 10 FOLDS\n",
      "--------------------------------\n",
      "Fold 0: 1.1868382692337036\n",
      "Fold 1: 1.2082412242889404\n",
      "Average: 1.1975398063659668\n",
      "FOLD 2\n",
      "train_ids: [31673218, 37195657]\n",
      "test_ids: [38257584, 39333001]\n",
      "--------------------------------\n",
      "Reset trainable parameters of layer = BatchNorm1d(79, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=79, out_features=128, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=128, out_features=256, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=256, out_features=256, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=256, out_features=128, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=128, out_features=64, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=64, out_features=1, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(79, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=79, out_features=128, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=128, out_features=256, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=256, out_features=256, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=256, out_features=128, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=128, out_features=64, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=64, out_features=1, bias=True)\n",
      "Visualizing model weights...\n",
      "Starting epoch 1\n",
      "Loss after mini-batch  1000: 1.028\n",
      "Visualizing model weights...\n",
      "Starting epoch 2\n",
      "Loss after mini-batch  1000: 0.997\n",
      "Visualizing model weights...\n",
      "Starting epoch 3\n",
      "Loss after mini-batch  1000: 0.991\n",
      "Visualizing model weights...\n",
      "Starting epoch 4\n",
      "Loss after mini-batch  1000: 0.988\n",
      "Visualizing model weights...\n",
      "Starting epoch 5\n",
      "Loss after mini-batch  1000: 0.985\n",
      "Visualizing model weights...\n",
      "Starting epoch 6\n",
      "Loss after mini-batch  1000: 0.983\n",
      "Visualizing model weights...\n",
      "Starting epoch 7\n",
      "Loss after mini-batch  1000: 0.981\n",
      "Visualizing model weights...\n",
      "Starting epoch 8\n",
      "Loss after mini-batch  1000: 0.980\n",
      "Visualizing model weights...\n",
      "Starting epoch 9\n",
      "Loss after mini-batch  1000: 0.978\n",
      "Visualizing model weights...\n",
      "Starting epoch 10\n",
      "Loss after mini-batch  1000: 0.977\n",
      "Training process has finished\n",
      "Starting testing\n",
      "--------------------------------\n",
      "CROSS VALIDATION RESULTS FOR 10 FOLDS\n",
      "--------------------------------\n",
      "Fold 0: 1.1868382692337036\n",
      "Fold 1: 1.2082412242889404\n",
      "Fold 2: 2.097119092941284\n",
      "Average: 1.4973996877670288\n",
      "FOLD 3\n",
      "train_ids: [32796098, 38257553]\n",
      "test_ids: [39333032, 40440393]\n",
      "--------------------------------\n",
      "Reset trainable parameters of layer = BatchNorm1d(79, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=79, out_features=128, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=128, out_features=256, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=256, out_features=256, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=256, out_features=128, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=128, out_features=64, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=64, out_features=1, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(79, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=79, out_features=128, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=128, out_features=256, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=256, out_features=256, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=256, out_features=128, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=128, out_features=64, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=64, out_features=1, bias=True)\n",
      "Visualizing model weights...\n",
      "Starting epoch 1\n",
      "Loss after mini-batch  1000: 1.016\n",
      "Visualizing model weights...\n",
      "Starting epoch 2\n",
      "Loss after mini-batch  1000: 0.991\n",
      "Visualizing model weights...\n",
      "Starting epoch 3\n",
      "Loss after mini-batch  1000: 0.988\n",
      "Visualizing model weights...\n",
      "Starting epoch 4\n",
      "Loss after mini-batch  1000: 0.986\n",
      "Visualizing model weights...\n",
      "Starting epoch 5\n",
      "Loss after mini-batch  1000: 0.985\n",
      "Visualizing model weights...\n",
      "Starting epoch 6\n",
      "Loss after mini-batch  1000: 0.984\n",
      "Visualizing model weights...\n",
      "Starting epoch 7\n",
      "Loss after mini-batch  1000: 0.983\n",
      "Visualizing model weights...\n",
      "Starting epoch 8\n",
      "Loss after mini-batch  1000: 0.982\n",
      "Visualizing model weights...\n",
      "Starting epoch 9\n",
      "Loss after mini-batch  1000: 0.980\n",
      "Visualizing model weights...\n",
      "Starting epoch 10\n",
      "Loss after mini-batch  1000: 0.979\n",
      "Training process has finished\n",
      "Starting testing\n",
      "--------------------------------\n",
      "CROSS VALIDATION RESULTS FOR 10 FOLDS\n",
      "--------------------------------\n",
      "Fold 0: 1.1868382692337036\n",
      "Fold 1: 1.2082412242889404\n",
      "Fold 2: 2.097119092941284\n",
      "Fold 3: 1.895150899887085\n",
      "Average: 1.5968375205993652\n",
      "FOLD 4\n",
      "train_ids: [33920914, 39333001]\n",
      "test_ids: [40440424, 41555529]\n",
      "--------------------------------\n",
      "Reset trainable parameters of layer = BatchNorm1d(79, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=79, out_features=128, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=128, out_features=256, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=256, out_features=256, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=256, out_features=128, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=128, out_features=64, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=64, out_features=1, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(79, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=79, out_features=128, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=128, out_features=256, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=256, out_features=256, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=256, out_features=128, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=128, out_features=64, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=64, out_features=1, bias=True)\n",
      "Visualizing model weights...\n",
      "Starting epoch 1\n",
      "Loss after mini-batch  1000: 1.019\n",
      "Visualizing model weights...\n",
      "Starting epoch 2\n",
      "Loss after mini-batch  1000: 0.994\n",
      "Visualizing model weights...\n",
      "Starting epoch 3\n",
      "Loss after mini-batch  1000: 0.990\n",
      "Visualizing model weights...\n",
      "Starting epoch 4\n",
      "Loss after mini-batch  1000: 0.988\n",
      "Visualizing model weights...\n",
      "Starting epoch 5\n",
      "Loss after mini-batch  1000: 0.986\n",
      "Visualizing model weights...\n",
      "Starting epoch 6\n",
      "Loss after mini-batch  1000: 0.985\n",
      "Visualizing model weights...\n",
      "Starting epoch 7\n",
      "Loss after mini-batch  1000: 0.984\n",
      "Visualizing model weights...\n",
      "Starting epoch 8\n",
      "Loss after mini-batch  1000: 0.983\n",
      "Visualizing model weights...\n",
      "Starting epoch 9\n",
      "Loss after mini-batch  1000: 0.982\n",
      "Visualizing model weights...\n",
      "Starting epoch 10\n",
      "Loss after mini-batch  1000: 0.981\n",
      "Training process has finished\n",
      "Starting testing\n",
      "--------------------------------\n",
      "CROSS VALIDATION RESULTS FOR 10 FOLDS\n",
      "--------------------------------\n",
      "Fold 0: 1.1868382692337036\n",
      "Fold 1: 1.2082412242889404\n",
      "Fold 2: 2.097119092941284\n",
      "Fold 3: 1.895150899887085\n",
      "Fold 4: 1.2025792598724365\n",
      "Average: 1.5179859399795532\n",
      "FOLD 5\n",
      "train_ids: [35051538, 40440393]\n",
      "test_ids: [41555560, 42670665]\n",
      "--------------------------------\n",
      "Reset trainable parameters of layer = BatchNorm1d(79, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=79, out_features=128, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=128, out_features=256, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=256, out_features=256, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=256, out_features=128, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=128, out_features=64, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=64, out_features=1, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(79, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=79, out_features=128, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=128, out_features=256, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=256, out_features=256, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=256, out_features=128, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=128, out_features=64, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=64, out_features=1, bias=True)\n",
      "Visualizing model weights...\n",
      "Starting epoch 1\n",
      "Loss after mini-batch  1000: 1.016\n",
      "Visualizing model weights...\n",
      "Starting epoch 2\n",
      "Loss after mini-batch  1000: 0.995\n",
      "Visualizing model weights...\n",
      "Starting epoch 3\n",
      "Loss after mini-batch  1000: 0.991\n",
      "Visualizing model weights...\n",
      "Starting epoch 4\n",
      "Loss after mini-batch  1000: 0.988\n",
      "Visualizing model weights...\n",
      "Starting epoch 5\n",
      "Loss after mini-batch  1000: 0.986\n",
      "Visualizing model weights...\n",
      "Starting epoch 6\n",
      "Loss after mini-batch  1000: 0.984\n",
      "Visualizing model weights...\n",
      "Starting epoch 7\n",
      "Loss after mini-batch  1000: 0.982\n",
      "Visualizing model weights...\n",
      "Starting epoch 8\n",
      "Loss after mini-batch  1000: 0.981\n",
      "Visualizing model weights...\n",
      "Starting epoch 9\n",
      "Loss after mini-batch  1000: 0.980\n",
      "Visualizing model weights...\n",
      "Starting epoch 10\n",
      "Loss after mini-batch  1000: 0.979\n",
      "Training process has finished\n",
      "Starting testing\n",
      "--------------------------------\n",
      "CROSS VALIDATION RESULTS FOR 10 FOLDS\n",
      "--------------------------------\n",
      "Fold 0: 1.1868382692337036\n",
      "Fold 1: 1.2082412242889404\n",
      "Fold 2: 2.097119092941284\n",
      "Fold 3: 1.895150899887085\n",
      "Fold 4: 1.2025792598724365\n",
      "Fold 5: 1.0396661758422852\n",
      "Average: 1.4382660388946533\n",
      "FOLD 6\n",
      "train_ids: [36098914, 41555529]\n",
      "test_ids: [42670696, 43779993]\n",
      "--------------------------------\n",
      "Reset trainable parameters of layer = BatchNorm1d(79, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=79, out_features=128, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=128, out_features=256, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=256, out_features=256, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=256, out_features=128, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=128, out_features=64, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=64, out_features=1, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(79, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=79, out_features=128, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=128, out_features=256, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=256, out_features=256, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=256, out_features=128, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=128, out_features=64, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=64, out_features=1, bias=True)\n",
      "Visualizing model weights...\n",
      "Starting epoch 1\n",
      "Loss after mini-batch  1000: 1.026\n",
      "Visualizing model weights...\n",
      "Starting epoch 2\n",
      "Loss after mini-batch  1000: 0.998\n",
      "Visualizing model weights...\n",
      "Starting epoch 3\n",
      "Loss after mini-batch  1000: 0.992\n",
      "Visualizing model weights...\n",
      "Starting epoch 4\n",
      "Loss after mini-batch  1000: 0.989\n",
      "Visualizing model weights...\n",
      "Starting epoch 5\n",
      "Loss after mini-batch  1000: 0.986\n",
      "Visualizing model weights...\n",
      "Starting epoch 6\n",
      "Loss after mini-batch  1000: 0.985\n",
      "Visualizing model weights...\n",
      "Starting epoch 7\n",
      "Loss after mini-batch  1000: 0.983\n",
      "Visualizing model weights...\n",
      "Starting epoch 8\n",
      "Loss after mini-batch  1000: 0.982\n",
      "Visualizing model weights...\n",
      "Starting epoch 9\n",
      "Loss after mini-batch  1000: 0.981\n",
      "Visualizing model weights...\n",
      "Starting epoch 10\n",
      "Loss after mini-batch  1000: 0.980\n",
      "Training process has finished\n",
      "Starting testing\n",
      "--------------------------------\n",
      "CROSS VALIDATION RESULTS FOR 10 FOLDS\n",
      "--------------------------------\n",
      "Fold 0: 1.1868382692337036\n",
      "Fold 1: 1.2082412242889404\n",
      "Fold 2: 2.097119092941284\n",
      "Fold 3: 1.895150899887085\n",
      "Fold 4: 1.2025792598724365\n",
      "Fold 5: 1.0396661758422852\n",
      "Fold 6: 2.4699976444244385\n",
      "Average: 1.5856562852859497\n",
      "FOLD 7\n",
      "train_ids: [37195658, 42670665]\n",
      "test_ids: [43780024, 44891257]\n",
      "--------------------------------\n",
      "Reset trainable parameters of layer = BatchNorm1d(79, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=79, out_features=128, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=128, out_features=256, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=256, out_features=256, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=256, out_features=128, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=128, out_features=64, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=64, out_features=1, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(79, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=79, out_features=128, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=128, out_features=256, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=256, out_features=256, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=256, out_features=128, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=128, out_features=64, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=64, out_features=1, bias=True)\n",
      "Visualizing model weights...\n",
      "Starting epoch 1\n",
      "Loss after mini-batch  1000: 1.030\n",
      "Visualizing model weights...\n",
      "Starting epoch 2\n",
      "Loss after mini-batch  1000: 0.994\n",
      "Visualizing model weights...\n",
      "Starting epoch 3\n",
      "Loss after mini-batch  1000: 0.989\n",
      "Visualizing model weights...\n",
      "Starting epoch 4\n",
      "Loss after mini-batch  1000: 0.986\n",
      "Visualizing model weights...\n",
      "Starting epoch 5\n",
      "Loss after mini-batch  1000: 0.984\n",
      "Visualizing model weights...\n",
      "Starting epoch 6\n",
      "Loss after mini-batch  1000: 0.983\n",
      "Visualizing model weights...\n",
      "Starting epoch 7\n",
      "Loss after mini-batch  1000: 0.981\n",
      "Visualizing model weights...\n",
      "Starting epoch 8\n",
      "Loss after mini-batch  1000: 0.980\n",
      "Visualizing model weights...\n",
      "Starting epoch 9\n",
      "Loss after mini-batch  1000: 0.979\n",
      "Visualizing model weights...\n",
      "Starting epoch 10\n",
      "Loss after mini-batch  1000: 0.977\n",
      "Training process has finished\n",
      "Starting testing\n",
      "--------------------------------\n",
      "CROSS VALIDATION RESULTS FOR 10 FOLDS\n",
      "--------------------------------\n",
      "Fold 0: 1.1868382692337036\n",
      "Fold 1: 1.2082412242889404\n",
      "Fold 2: 2.097119092941284\n",
      "Fold 3: 1.895150899887085\n",
      "Fold 4: 1.2025792598724365\n",
      "Fold 5: 1.0396661758422852\n",
      "Fold 6: 2.4699976444244385\n",
      "Fold 7: 1.1416404247283936\n",
      "Average: 1.5301542282104492\n",
      "FOLD 8\n",
      "train_ids: [38257554, 43779993]\n",
      "test_ids: [44891288, 46008329]\n",
      "--------------------------------\n",
      "Reset trainable parameters of layer = BatchNorm1d(79, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=79, out_features=128, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=128, out_features=256, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=256, out_features=256, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=256, out_features=128, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=128, out_features=64, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=64, out_features=1, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(79, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=79, out_features=128, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=128, out_features=256, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=256, out_features=256, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=256, out_features=128, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=128, out_features=64, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=64, out_features=1, bias=True)\n",
      "Visualizing model weights...\n",
      "Starting epoch 1\n",
      "Loss after mini-batch  1000: 1.026\n",
      "Visualizing model weights...\n",
      "Starting epoch 2\n",
      "Loss after mini-batch  1000: 0.998\n",
      "Visualizing model weights...\n",
      "Starting epoch 3\n",
      "Loss after mini-batch  1000: 0.991\n",
      "Visualizing model weights...\n",
      "Starting epoch 4\n",
      "Loss after mini-batch  1000: 0.988\n",
      "Visualizing model weights...\n",
      "Starting epoch 5\n",
      "Loss after mini-batch  1000: 0.985\n",
      "Visualizing model weights...\n",
      "Starting epoch 6\n",
      "Loss after mini-batch  1000: 0.983\n",
      "Visualizing model weights...\n",
      "Starting epoch 7\n",
      "Loss after mini-batch  1000: 0.981\n",
      "Visualizing model weights...\n",
      "Starting epoch 8\n",
      "Loss after mini-batch  1000: 0.980\n",
      "Visualizing model weights...\n",
      "Starting epoch 9\n",
      "Loss after mini-batch  1000: 0.979\n",
      "Visualizing model weights...\n",
      "Starting epoch 10\n",
      "Loss after mini-batch  1000: 0.978\n",
      "Training process has finished\n",
      "Starting testing\n",
      "--------------------------------\n",
      "CROSS VALIDATION RESULTS FOR 10 FOLDS\n",
      "--------------------------------\n",
      "Fold 0: 1.1868382692337036\n",
      "Fold 1: 1.2082412242889404\n",
      "Fold 2: 2.097119092941284\n",
      "Fold 3: 1.895150899887085\n",
      "Fold 4: 1.2025792598724365\n",
      "Fold 5: 1.0396661758422852\n",
      "Fold 6: 2.4699976444244385\n",
      "Fold 7: 1.1416404247283936\n",
      "Fold 8: 1.2044062614440918\n",
      "Average: 1.4939600229263306\n",
      "FOLD 9\n",
      "train_ids: [39333002, 44891257]\n",
      "test_ids: [46008360, 47127337]\n",
      "--------------------------------\n",
      "Reset trainable parameters of layer = BatchNorm1d(79, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=79, out_features=128, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=128, out_features=256, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=256, out_features=256, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=256, out_features=128, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=128, out_features=64, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=64, out_features=1, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(79, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=79, out_features=128, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=128, out_features=256, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=256, out_features=256, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=256, out_features=128, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=128, out_features=64, bias=True)\n",
      "Reset trainable parameters of layer = BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=64, out_features=1, bias=True)\n",
      "Visualizing model weights...\n",
      "Starting epoch 1\n",
      "Loss after mini-batch  1000: 1.042\n",
      "Visualizing model weights...\n",
      "Starting epoch 2\n",
      "Loss after mini-batch  1000: 0.998\n",
      "Visualizing model weights...\n",
      "Starting epoch 3\n",
      "Loss after mini-batch  1000: 0.991\n",
      "Visualizing model weights...\n",
      "Starting epoch 4\n",
      "Loss after mini-batch  1000: 0.988\n",
      "Visualizing model weights...\n",
      "Starting epoch 5\n",
      "Loss after mini-batch  1000: 0.986\n",
      "Visualizing model weights...\n",
      "Starting epoch 6\n",
      "Loss after mini-batch  1000: 0.984\n",
      "Visualizing model weights...\n",
      "Starting epoch 7\n",
      "Loss after mini-batch  1000: 0.982\n",
      "Visualizing model weights...\n",
      "Starting epoch 8\n",
      "Loss after mini-batch  1000: 0.981\n",
      "Visualizing model weights...\n",
      "Starting epoch 9\n",
      "Loss after mini-batch  1000: 0.980\n",
      "Visualizing model weights...\n",
      "Starting epoch 10\n",
      "Loss after mini-batch  1000: 0.979\n",
      "Training process has finished\n",
      "Starting testing\n",
      "--------------------------------\n",
      "CROSS VALIDATION RESULTS FOR 10 FOLDS\n",
      "--------------------------------\n",
      "Fold 0: 1.1868382692337036\n",
      "Fold 1: 1.2082412242889404\n",
      "Fold 2: 2.097119092941284\n",
      "Fold 3: 1.895150899887085\n",
      "Fold 4: 1.2025792598724365\n",
      "Fold 5: 1.0396661758422852\n",
      "Fold 6: 2.4699976444244385\n",
      "Fold 7: 1.1416404247283936\n",
      "Fold 8: 1.2044062614440918\n",
      "Fold 9: 1.0740958452224731\n",
      "Average: 1.4519736766815186\n"
     ]
    }
   ],
   "source": [
    "### Train and cross validation loop\n",
    "for fold, (train_ids, test_ids) in enumerate(cv.split(X=X, y=y, groups=group)):\n",
    "\n",
    "    # Print\n",
    "    print(f'FOLD {fold}')\n",
    "    print(f'train_ids: [{train_ids[0]}, {train_ids[-1]}]')\n",
    "    print(f'test_ids: [{test_ids[0]}, {test_ids[-1]}]')\n",
    "    print('--------------------------------')\n",
    "    \n",
    "    # Sample elements randomly from a given list of ids, no replacement.\n",
    "    # does this make sense for a time series?... I don't think so\n",
    "    #train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "    #test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n",
    "\n",
    "    # define train/test sets\n",
    "    X_tr, y_tr, w_tr = X[train_ids], y[train_ids], w[train_ids]\n",
    "    X_te, y_te, w_te = X[test_ids], y[test_ids], w[test_ids]\n",
    "    # Initialize Dataset objects to make PyTorch play nice\n",
    "    trainset = TensorDataset(torch.tensor(X_tr), torch.tensor(y_tr), torch.tensor(w_tr))\n",
    "    testset = TensorDataset(torch.tensor(X_te), torch.tensor(y_te), torch.tensor(w_te))\n",
    "    \n",
    "    # Define data loaders\n",
    "    BATCH_SIZE = 4096\n",
    "    NUM_WORKERS = 4 # num of parallel subprocesses for data loading (CPU task)\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "        trainset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_workers = NUM_WORKERS\n",
    "    )\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "        testset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_workers = NUM_WORKERS\n",
    "    )\n",
    "\n",
    "    # Init the neural network\n",
    "    network = MLP(input_dim, output_dim, hidden_dim, dropout_rates).to(device) # .to(device) sends model to GPU\n",
    "    network.apply(reset_weights) # not sure if I need this but w/e\n",
    "    # Initialize optimizer\n",
    "    optimizer = torch.optim.Adam(network.parameters(), lr=learning_rate)\n",
    "    network.train()\n",
    "\n",
    "    batch_train_tally = 0\n",
    "    # Run the training loop for defined number of epochs\n",
    "    for epoch in range(0, num_epochs):\n",
    "\n",
    "        # Visualize weight histograms\n",
    "        weight_histograms(writer, epoch, network)\n",
    "        \n",
    "        # Print epoch\n",
    "        print(f'Starting epoch {epoch+1}')\n",
    "        \n",
    "        # Set current loss value\n",
    "        current_loss = 0.0\n",
    "\n",
    "        # Iterate over Dataloader for training data\n",
    "        for i, data in enumerate(trainloader):\n",
    "\n",
    "            # Get inputs\n",
    "            inputs, targets, weights = data\n",
    "            inputs = inputs.to(device) # GPU must see the data and the model\n",
    "            targets = targets.to(device)\n",
    "            weights = weights.to(device)\n",
    "            \n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Perform forward pass\n",
    "            outputs = network(inputs)\n",
    "            \n",
    "            # Compute loss\n",
    "            if test_loss:\n",
    "                loss = test_loss_function(outputs[:,0], targets)\n",
    "            else:\n",
    "                loss = my_loss(outputs[:,0], targets, weights)\n",
    "            writer.add_scalar(\"Loss/train/minibatches\", loss, batch_train_tally)\n",
    "            batch_train_tally += 1\n",
    "            \n",
    "            # Perform backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Perform optimization\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Print statistics\n",
    "            current_loss += loss.item()\n",
    "            if i % 1000 == 999:\n",
    "                print('Loss after mini-batch %5d: %.3f' %\n",
    "                      (i + 1, current_loss / 1000))\n",
    "                current_loss = 0.0\n",
    "\n",
    "        # Finished iterating over Dataloader\n",
    "        writer.add_scalar(\"Loss/train/epoch\", loss, epoch)\n",
    "        writer.flush()\n",
    "    \n",
    "    # Process is complete.\n",
    "    print('Training process has finished')\n",
    "    \n",
    "    # Print about testing\n",
    "    print('Starting testing')\n",
    "    \n",
    "    # Saving the model\n",
    "    save_path = f'./model-fold-{fold}.pth'\n",
    "    torch.save(network.state_dict(), save_path)\n",
    "\n",
    "    # Evaluation for this fold\n",
    "    R2_score = 0.0\n",
    "    total_loss = 0.0\n",
    "    batch_test_tally = 0\n",
    "    with torch.no_grad():\n",
    "        network.eval()\n",
    "        # Iterate over the test data and generate predictions\n",
    "        for i, data in enumerate(testloader):\n",
    "            # Get inputs\n",
    "            inputs, targets, weights = data\n",
    "            inputs = inputs.to(device) # GPU must see the data and the model\n",
    "            targets = targets.to(device)\n",
    "            weights = weights.to(device)\n",
    "        \n",
    "            # Generate outputs\n",
    "            outputs = network(inputs)\n",
    "            if test_loss:\n",
    "                loss = test_loss_function(outputs[:,0], targets)\n",
    "            else:\n",
    "                 loss = my_loss(outputs[:,0], targets, weights)\n",
    "\n",
    "            writer.add_scalar('Loss/test/minibatches', loss, batch_test_tally)\n",
    "            batch_test_tally += 1\n",
    "            \n",
    "            total_loss += loss\n",
    "\n",
    "        print('--------------------------------')\n",
    "        num_batches = len(testloader)\n",
    "        results[fold] = total_loss / num_batches\n",
    "        writer.add_scalar('Loss/test/fold', results[fold], fold)\n",
    "        writer.flush()\n",
    "    \n",
    "    # Print fold results\n",
    "    print(f'CROSS VALIDATION RESULTS FOR {folds} FOLDS')\n",
    "    print('--------------------------------')\n",
    "    sum = 0.0\n",
    "    for key, value in results.items():\n",
    "        print(f'Fold {key}: {value}')\n",
    "        sum += value\n",
    "    print(f'Average: {sum/len(results.items())}')\n",
    "writer.close()\n",
    "rubbish=gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.26465315\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f553256db40>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# lmao\n",
    "out = outputs.cpu().detach().numpy()\n",
    "tar = targets.cpu().detach().numpy()\n",
    "wei = weights.cpu().detach().numpy()\n",
    "print(out[10,0])\n",
    "nbins = 20\n",
    "plt.hist(tar, color='orange', label='targets', bins=nbins)\n",
    "plt.hist(wei, color='green', label='weights', bins=nbins)\n",
    "plt.hist(out[:,0], color='blue', bins=nbins, label='predicted')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
