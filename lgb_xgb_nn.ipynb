{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shamelessly inspired by https://www.kaggle.com/code/yuanzhezhou/jane-street-baseline-lgb-xgb-and-catboost/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os, gc\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "#import catboost as cbt # needs numpy <2.0\n",
    "import joblib\n",
    "from joblib import Parallel, delayed\n",
    "import numpy as np\n",
    "#from hyperopt import hp, fmin, tpe, Trials\n",
    "#from hyperopt.pyll.base import scope\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import matplotlib.pyplot as plt\n",
    "#from tqdm.notebook import tqdm\n",
    "#from joblib import dump, load\n",
    "#import datatable as dtable\n",
    "#from mlxtend.evaluate import GroupTimeSeriesSplit\n",
    "import kaggle_evaluation.jane_street_inference_server as js_server\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "TEST=True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that torch is working and sees the GPU\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collected = gc.collect()\n",
    "# Prints Garbage collector \n",
    "# as 0 object\n",
    "print(\"Garbage collector: collected\",\n",
    "          \"%d objects.\" % collected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()\n",
    "if TEST:\n",
    "    df1 = pl.read_parquet(path+'/input/train.parquet/partition_id=7/part-0.parquet')\n",
    "    df2 = pl.read_parquet(path+'/input/train.parquet/partition_id=8/part-0.parquet')\n",
    "    df3 = pl.read_parquet(path+'/input/train.parquet/partition_id=9/part-0.parquet')\n",
    "    train_df = pl.concat([df1,df2, df3])\n",
    "    del df1, df2, df3\n",
    "else:\n",
    "    # very important to optimize memory when loading the full damn thing\n",
    "    cols = ['date_id', 'time_id', 'weight', 'symbol_id']\n",
    "    cols.extend(['feature_{:02d}'.format(num) for num in range(0, 79)])\n",
    "    cols.append('responder_6')\n",
    "    df1 = pl.read_parquet(path+'/input/train.parquet/partition_id=0/part-0.parquet', columns=cols)\n",
    "    df2 = pl.read_parquet(path+'/input/train.parquet/partition_id=1/part-0.parquet', columns=cols)\n",
    "    df3 = pl.read_parquet(path+'/input/train.parquet/partition_id=2/part-0.parquet', columns=cols)\n",
    "    df4 = pl.read_parquet(path+'/input/train.parquet/partition_id=3/part-0.parquet', columns=cols)\n",
    "    df5 = pl.read_parquet(path+'/input/train.parquet/partition_id=4/part-0.parquet', columns=cols)\n",
    "    df6 = pl.read_parquet(path+'/input/train.parquet/partition_id=5/part-0.parquet', columns=cols)\n",
    "    df7 = pl.read_parquet(path+'/input/train.parquet/partition_id=6/part-0.parquet', columns=cols)\n",
    "    df8 = pl.read_parquet(path+'/input/train.parquet/partition_id=7/part-0.parquet', columns=cols)\n",
    "    df9 = pl.read_parquet(path+'/input/train.parquet/partition_id=8/part-0.parquet', columns=cols)\n",
    "    df10 = pl.read_parquet(path+'/input/train.parquet/partition_id=9/part-0.parquet', columns=cols)\n",
    "    train_df = pl.concat([df1, df2, df3, df4, df5, df6, df7, df8, df9, df10])\n",
    "    del df1, df2, df3, df4, df5, df6, df7, df8, df9, df10\n",
    "\n",
    "lags = pl.read_parquet(path+'/input/lags.parquet/date_id=0/part-0.parquet')\n",
    "feature_tags = pl.read_csv(path+'/input/features.csv') # no one seems to use this....\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [c for c in train_df.columns if 'feature' in c]\n",
    "#features.append('weight')\n",
    "#features.append('time_id')\n",
    "#features.append('symbol_id')\n",
    "target = 'responder_6'\n",
    "print(train_df.shape)\n",
    "print(train_df.columns)\n",
    "print(train_df[features].null_count())\n",
    "# Returns the number of\n",
    "# objects it has collected\n",
    "# and deallocated\n",
    "collected = gc.collect()\n",
    "# Prints Garbage collector \n",
    "# as 0 object\n",
    "print(\"Garbage collector: collected\",\n",
    "          \"%d objects.\" % collected)\n",
    "\n",
    "# note to self, incorporate features.csv true/false tables at some point "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Filling...')\n",
    "# next two lines were for pandas not polars\n",
    "#f_mean = train[features].mean() \n",
    "# print(train.weight.gt(0).sum() == train.shape[0]) # check if weights>0 is true for entire dataframe\n",
    "print(train_df.shape)\n",
    "train_df = train_df.fill_null(0)\n",
    "#train.dropna(inplace=True)\n",
    "print(train_df.null_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# for the purged group time series split, code is copied from somewhere\n",
    "# TODO: make GitHub GIST\n",
    "# TODO: add as dataset\n",
    "# TODO: add logging with verbose\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection._split import _BaseKFold, indexable, _num_samples\n",
    "from sklearn.utils.validation import _deprecate_positional_args\n",
    "\n",
    "# modified code for group gaps; source\n",
    "# https://github.com/getgaurav2/scikit-learn/blob/d4a3af5cc9da3a76f0266932644b884c99724c57/sklearn/model_selection/_split.py#L2243\n",
    "class PurgedGroupTimeSeriesSplit(_BaseKFold):\n",
    "    \"\"\"Time Series cross-validator variant with non-overlapping groups.|\n",
    "    Allows for a gap in groups to avoid potentially leaking info from\n",
    "    train into test if the model has windowed or lag features.\n",
    "    Provides train/test indices to split time series data samples\n",
    "    that are observed at fixed time intervals according to a\n",
    "    third-party provided group.\n",
    "    In each split, test indices must be higher than before, and thus shuffling\n",
    "    in cross validator is inappropriate.\n",
    "    This cross-validation object is a variation of :class:`KFold`.\n",
    "    In the kth split, it returns first k folds as train set and the\n",
    "    (k+1)th fold as test set.\n",
    "    The same group will not appear in two different folds (the number of\n",
    "    distinct groups has to be at least equal to the number of folds).\n",
    "    Note that unlike standard cross-validation methods, successive\n",
    "    training sets are supersets of those that come before them.\n",
    "    Read more in the :ref:`User Guide <cross_validation>`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_splits : int, default=5\n",
    "        Number of splits. Must be at least 2.\n",
    "    max_train_group_size : int, default=Inf\n",
    "        Maximum group size for a single training set.\n",
    "    group_gap : int, default=None\n",
    "        Gap between train and test\n",
    "    max_test_group_size : int, default=Inf\n",
    "        We discard this number of groups from the end of each train split\n",
    "    \"\"\"\n",
    "\n",
    "    @_deprecate_positional_args\n",
    "    def __init__(self,\n",
    "                 n_splits=5,\n",
    "                 *,\n",
    "                 max_train_group_size=np.inf,\n",
    "                 max_test_group_size=np.inf,\n",
    "                 group_gap=None,\n",
    "                 verbose=False\n",
    "                 ):\n",
    "        super().__init__(n_splits, shuffle=False, random_state=None)\n",
    "        self.max_train_group_size = max_train_group_size\n",
    "        self.group_gap = group_gap\n",
    "        self.max_test_group_size = max_test_group_size\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        \"\"\"Generate indices to split data into training and test set.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Training data, where n_samples is the number of samples\n",
    "            and n_features is the number of features.\n",
    "        y : array-like of shape (n_samples,)\n",
    "            Always ignored, exists for compatibility.\n",
    "        groups : array-like of shape (n_samples,)\n",
    "            Group labels for the samples used while splitting the dataset into\n",
    "            train/test set.\n",
    "        Yields\n",
    "        ------\n",
    "        train : ndarray\n",
    "            The training set indices for that split.\n",
    "        test : ndarray\n",
    "            The testing set indices for that split.\n",
    "        \"\"\"\n",
    "        if groups is None:\n",
    "            raise ValueError(\n",
    "                \"The 'groups' parameter should not be None\")\n",
    "        X, y, groups = indexable(X, y, groups)\n",
    "        n_samples = _num_samples(X)\n",
    "        n_splits = self.n_splits\n",
    "        group_gap = self.group_gap\n",
    "        max_test_group_size = self.max_test_group_size\n",
    "        max_train_group_size = self.max_train_group_size\n",
    "        n_folds = n_splits + 1\n",
    "        group_dict = {}\n",
    "        u, ind = np.unique(groups, return_index=True)\n",
    "        unique_groups = u[np.argsort(ind)]\n",
    "        n_samples = _num_samples(X)\n",
    "        n_groups = _num_samples(unique_groups)\n",
    "        for idx in np.arange(n_samples):\n",
    "            if (groups[idx] in group_dict):\n",
    "                group_dict[groups[idx]].append(idx)\n",
    "            else:\n",
    "                group_dict[groups[idx]] = [idx]\n",
    "        if n_folds > n_groups:\n",
    "            raise ValueError(\n",
    "                (\"Cannot have number of folds={0} greater than\"\n",
    "                 \" the number of groups={1}\").format(n_folds,\n",
    "                                                     n_groups))\n",
    "\n",
    "        group_test_size = min(n_groups // n_folds, max_test_group_size)\n",
    "        group_test_starts = range(n_groups - n_splits * group_test_size,\n",
    "                                  n_groups, group_test_size)\n",
    "        for group_test_start in group_test_starts:\n",
    "            train_array = []\n",
    "            test_array = []\n",
    "\n",
    "            group_st = max(0, group_test_start - group_gap - max_train_group_size)\n",
    "            for train_group_idx in unique_groups[group_st:(group_test_start - group_gap)]:\n",
    "                train_array_tmp = group_dict[train_group_idx]\n",
    "                \n",
    "                train_array = np.sort(np.unique(\n",
    "                                      np.concatenate((train_array,\n",
    "                                                      train_array_tmp)),\n",
    "                                      axis=None), axis=None)\n",
    "\n",
    "            train_end = train_array.size\n",
    " \n",
    "            for test_group_idx in unique_groups[group_test_start:\n",
    "                                                group_test_start +\n",
    "                                                group_test_size]:\n",
    "                test_array_tmp = group_dict[test_group_idx]\n",
    "                test_array = np.sort(np.unique(\n",
    "                                              np.concatenate((test_array,\n",
    "                                                              test_array_tmp)),\n",
    "                                     axis=None), axis=None)\n",
    "\n",
    "            test_array  = test_array[group_gap:]\n",
    "            \n",
    "            \n",
    "            if self.verbose > 0:\n",
    "                    pass\n",
    "                    \n",
    "            yield [int(i) for i in train_array], [int(i) for i in test_array]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# from matplotlib.colors import ListedColormap\n",
    "    \n",
    "# # this is code slightly modified from the sklearn docs here:\n",
    "# # https://scikit-learn.org/stable/auto_examples/model_selection/plot_cv_indices.html#sphx-glr-auto-examples-model-selection-plot-cv-indices-py\n",
    "# def plot_cv_indices(cv, X, y, group, ax, n_splits, lw=10):\n",
    "#     \"\"\"Create a sample plot for indices of a cross-validation object.\"\"\"\n",
    "    \n",
    "#     cmap_cv = plt.cm.coolwarm\n",
    "\n",
    "#     jet = plt.cm.get_cmap('jet', 256)\n",
    "#     seq = np.linspace(0, 1, 256)\n",
    "#     _ = np.random.shuffle(seq)   # inplace\n",
    "#     cmap_data = ListedColormap(jet(seq))\n",
    "\n",
    "#     # Generate the training/testing visualizations for each CV split\n",
    "#     for ii, (tr, tt) in enumerate(cv.split(X=X, y=y, groups=group)):\n",
    "#         # Fill in indices with the training/test groups\n",
    "#         indices = np.array([np.nan] * len(X))\n",
    "#         indices[tt] = 1\n",
    "#         indices[tr] = 0\n",
    "\n",
    "#         # Visualize the results\n",
    "#         ax.scatter(range(len(indices)), [ii + .5] * len(indices),\n",
    "#                    c=indices, marker='_', lw=lw, cmap=cmap_cv,\n",
    "#                    vmin=-.2, vmax=1.2)\n",
    "\n",
    "#     # Plot the data classes and groups at the end\n",
    "#     ax.scatter(range(len(X)), [ii + 1.5] * len(X),\n",
    "#                c=y, marker='_', lw=lw, cmap=plt.cm.Set3)\n",
    "\n",
    "#     ax.scatter(range(len(X)), [ii + 2.5] * len(X),\n",
    "#                c=group, marker='_', lw=lw, cmap=cmap_data)\n",
    "\n",
    "#     # Formatting\n",
    "#     yticklabels = list(range(n_splits)) + ['target', 'day']\n",
    "#     ax.set(yticks=np.arange(n_splits+2) + .5, yticklabels=yticklabels,\n",
    "#            xlabel='Sample index', ylabel=\"CV iteration\",\n",
    "#            ylim=[n_splits+2.2, -.2], xlim=[0, len(y)])\n",
    "#     ax.set_title('{}'.format(type(cv).__name__), fontsize=15)\n",
    "#     return ax\n",
    "\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# #plot_cv_indices(cv, X_train, y_train, groups, ax, 5, lw=20)\n",
    "# plot_cv_indices(\n",
    "#     cv,\n",
    "#     train[features].values,\n",
    "#     train['responder_6'].values,\n",
    "#     train['date_id'].values,\n",
    "#     ax,\n",
    "#     5,\n",
    "#     lw=20\n",
    "# )\n",
    "# rubbish=gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING=True\n",
    "num_valid_dates = 100\n",
    "skip_dates = 0\n",
    "N_fold = 5\n",
    "\n",
    "# filter df to skip specified num of dates\n",
    "train_df = train_df.filter(pl.col('date_id') > skip_dates)\n",
    "\n",
    "# get unique dates as np list\n",
    "dates = train_df['date_id'].unique().to_numpy()\n",
    "\n",
    "# specify train and validation dates\n",
    "valid_dates = dates[-num_valid_dates:]\n",
    "train_dates = dates[:-num_valid_dates]\n",
    "\n",
    "# split train_dates into N_fold sub-arrays (not necessarily equal size)\n",
    "train_dates = np.array_split(train_dates, N_fold)\n",
    "#print(train_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract needed parameters from dataframe as np arrays, will later turn into tensors\n",
    "X_vl = train_df.filter(pl.col('date_id') >= valid_dates.min())[features].to_numpy()\n",
    "y_vl = train_df.filter(pl.col('date_id') >= valid_dates.min())['responder_6'].to_numpy()\n",
    "w_vl = train_df.filter(pl.col('date_id') >= valid_dates.min())['weight'].to_numpy()\n",
    "\n",
    "# neat hack to ensure the original dataframe is gone\n",
    "# del train\n",
    "# gc.collect()\n",
    "# train = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_vl.shape)\n",
    "print(w_vl.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize list to store trained models\n",
    "models = []\n",
    "\n",
    "model_path = './models'\n",
    "# Function to train a model or load a pre-trained model\n",
    "def train(model_dict, model_name='lgb'):\n",
    "\n",
    "    if TRAINING:\n",
    "        # Select dates for training based on fold number\n",
    "        selected_dates = train_dates[i] # i exists outside this function but in the cell\n",
    "    \n",
    "        # Get the model from the dictionary\n",
    "        model = model_dict[model_name]\n",
    "    \n",
    "        # Extract features, target, weights for selected training dates\n",
    "        X_tr = train_df.filter(pl.col('date_id').is_between(selected_dates[0], selected_dates[-1]))[features].to_numpy()\n",
    "        y_tr = train_df.filter(pl.col('date_id').is_between(selected_dates[0], selected_dates[-1]))['responder_6'].to_numpy()\n",
    "        w_tr = train_df.filter(pl.col('date_id').is_between(selected_dates[0], selected_dates[-1]))['weight'].to_numpy()\n",
    "    \n",
    "        # Train the model based on the type\n",
    "        if model_name == 'lgb':\n",
    "            # Train LightGBM model with early stopping and evaluation logging\n",
    "            model.fit(X_tr, y_tr, w_tr, \n",
    "                        eval_metric=[r2_lgb],\n",
    "                        eval_set = [(X_vl, y_vl, w_vl)],\n",
    "                        callbacks = [\n",
    "                            lgb.early_stopping(100),\n",
    "                            lgb.log_evaluation(10)\n",
    "                        ])\n",
    "        elif model_name == 'xgb':\n",
    "            # Train XGBoost model with early stopping and verbose logging\n",
    "            model.fit(X_tr, y_tr, sample_weight=w_tr,\n",
    "                        eval_set=[(X_vl, y_vl)],\n",
    "                        sample_weight_eval_set=[w_vl],\n",
    "                        verbose=10,\n",
    "                        early_stopping_rounds=100)\n",
    "    \n",
    "        # Append the trained model to the list\n",
    "        models.append(model)\n",
    "    \n",
    "        # Save the trained model to a file\n",
    "        joblib.dump(model, f'{model_path}/{model_name}_{i}.model')\n",
    "    \n",
    "        # Delte training data to free up memory\n",
    "        del X_tr\n",
    "        del y_tr\n",
    "        del w_tr\n",
    "        gc.collect()\n",
    "\n",
    "    else :\n",
    "        models.append(joblib.load(f'{model_path}/{model_name}_{i}.model'))\n",
    "    return    \n",
    "\n",
    "# Custom R2 metric for XGBoost\n",
    "def r2_xgb(y_true, y_pred, sample_weight):\n",
    "    r2 = 1 - np.average((y_pred - y_true)**2, weights=sample_weight) / (np.average(y_true**2, weights=sample_weight) + 1e-38)\n",
    "    return -r2\n",
    "\n",
    "# Custom R2 metric for LightGBM\n",
    "def r2_lgb(y_true, y_pred, sample_weight):\n",
    "    r2 = 1 - np.average((y_pred - y_true)**2, weights=sample_weight) / (np.average(y_true**2, weights=sample_weight) + 1e-38)\n",
    "    return 'r2', r2, True\n",
    "\n",
    "# Dict to store different models and their configurations\n",
    "model_dict = {\n",
    "    'lgb': lgb.LGBMRegressor(n_estimators=500, device='gpu', gpu_use_dp=True, objective='l2'),\n",
    "    'xgb': xgb.XGBRegressor(n_estimators=2000, learning_rate=0.1, max_depth=6, tree_method='hist', device='cuda', objective='reg:squarederror', eval_metric=r2_xgb, disable_default_eval_metric=True),\n",
    "}\n",
    "\n",
    "# Train models for each fold\n",
    "for i in range(N_fold):\n",
    "    #train(model_dict, 'lgb')\n",
    "    train(model_dict, 'xgb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lags_ : pl.DataFrame | None = None\n",
    "\n",
    "# Replace this function with your inference code.\n",
    "# You can return either a Pandas or Polars dataframe, though Polars is recommended.\n",
    "# Each batch of predictions (except the very first) must be returned within 10 minutes of the batch features being provided.\n",
    "def predict(test: pl.DataFrame, lags: pl.DataFrame | None) -> pl.DataFrame | pd.DataFrame:\n",
    "    \"\"\"Make a prediction.\"\"\"\n",
    "    # All the responders from the previous day are passed in at time_id == 0. We save them in a global variable for access at every time_id.\n",
    "    # Use them as extra features, if you like.\n",
    "    global lags_\n",
    "    if lags is not None:\n",
    "        lags_ = lags\n",
    "\n",
    "    predictions = test.select(\n",
    "        'row_id',\n",
    "        pl.lit(0.0).alias('responder_6'),\n",
    "    )\n",
    "    \n",
    "    feat = test[features].to_numpy()\n",
    "    \n",
    "    pred = [model.predict(feat) for model in models]\n",
    "    pred = np.mean(pred, axis=0)\n",
    "    \n",
    "    predictions = predictions.with_columns(pl.Series('responder_6', pred.ravel()))\n",
    "\n",
    "    # The predict function must return a DataFrame\n",
    "    assert isinstance(predictions, pl.DataFrame | pd.DataFrame)\n",
    "    # with columns 'row_id', 'responer_6'\n",
    "    assert list(predictions.columns) == ['row_id', 'responder_6']\n",
    "    # and as many rows as the test data.\n",
    "    assert len(predictions) == len(test)\n",
    "\n",
    "    return predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "inference_server = js_server.JSInferenceServer(predict)\n",
    "\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    inference_server.serve()\n",
    "else:\n",
    "    inference_server.run_local_gateway(\n",
    "        (\n",
    "            './input/test.parquet',\n",
    "            './input/lags.parquet',\n",
    "        )\n",
    "    )\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
